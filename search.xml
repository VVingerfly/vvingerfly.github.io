<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[有向面积&有向体积]]></title>
    <url>%2F2021%2F09-30-CG-SignedArea%26Volume%2F</url>
    <content type="text"><![CDATA[有向面积&amp;有向体积平面三角形有向面积对于2D平面上的一个三角形，其有向面积的计算公式如下 A = \frac{1}{2} \left|\begin{matrix} x_{0} & y_{0} & 1 \\ x_{1} & y_{1} & 1 \\ x_{2} & y_{2} & 1 \end{matrix}\right|C++代码如下：123456double signedAreaOfTriangle(const Eigen::Vector2d &amp;p0, const Eigen::Vector2d &amp;p1, const Eigen::Vector2d &amp;p2)&#123; double v12 = (p1(0)-p0(0)) * (p2(1)-p0(1)); double v21 = (p2(0)-p0(0)) * (p1(1)-p0(1)); return 0.5 * (v12 - v21);&#125; 如果三角形的点按照逆时针排布，其面积为正，如果按照顺时针排布，其面积为负。 空间四面体有向体积对于3D空间中的一个四面体，其有向体积的计算公式如下 V = \frac{1}{6} \left|\begin{matrix} x_{0} & y_{0} & z_{0} & 1 \\ x_{1} & y_{1} & z_{1} & 1 \\ x_{2} & y_{2} & z_{2} & 1 \\ x_{3} & y_{3} & z_{3} & 1 \end{matrix}\right|C++代码如下：12345678910111213double signedVolumeOfTetrahedron(const Eigen::Vector3d &amp;p0, const Eigen::Vector3d &amp;p1, const Eigen::Vector3d &amp;p2, const Eigen::Vector3d &amp;p3) &#123; Eigen::Vector3d p01 = p1 - p0; Eigen::Vector3d p02 = p2 - p0; Eigen::Vector3d p03 = p3 - p0; double v123 = p01(0)*p02(1)*p03(2); double v231 = p02(0)*p03(1)*p01(2); double v312 = p03(0)*p01(1)*p02(2); double v321 = p03(0)*p02(1)*p01(2); double v213 = p02(0)*p01(1)*p03(2); double v132 = p01(0)*p03(1)*p02(2); return (1.0/6.0)*(v123 - v231 - v312 - v321 + v213 + v132);&#125; 应用平面多边形面积计算对于二维平面上的一个封闭多边形，其面积可以通过计算多边形的每条边与原点构成的三角形的有向面积之和得到，注意多边形的顶点必须有序，即为顺时针或者逆时针。123456789101112131415double areaOfPolygon(const vector&lt;Eigen::Vector2d&gt; &amp;points)&#123; // points must be oriented, clock-wise or anti-clock-wise double areas = 0.0; int num_pts = points.size(); if (num_pts &lt; 3) return areas; Eigen::Vector2d o(0.0, 0.0); // origin in 2d plane for (int i = 0; i &lt; num_pts-1; ++i) &#123; areas += signedAreaOfTriangle(o, points[i], points[i+1]); &#125; areas += signedAreaOfTriangle(o, points[num_pts-1], points[0]); return abs(areas);&#125; 平面多边形顶点顺序判断平面多边形的有序顶点是顺时针还是逆时针，可以通过计算其有向面积得到，如果面积为正即为逆时针，如面积为负即为顺时针。 空间网格的体积计算对于三维空间中的一个封闭的三角网格，通过计算网格的每个三角面片与原点构成的四面体的有向体积之和，便可以得到该三角网格的有向体积。1234567891011121314151617double volumeOfMesh(const vector&lt;Eigen::Vector3d&gt; &amp;points, const vector&lt;Eigen::Vector3i&gt; &amp;triangles) &#123; // the normals of the mesh must be coninsistent double vols = 0.0; if (points.size() &lt; 4) return vols; Eigen::Vector3d o(0.0, 0.0, 0.0); // origin in 3d space Eigen::Vector3d p1, p2, p3; for (const auto tri&amp; : triangles) &#123; p1 = points[tri(0)]; p2 = points[tri(1)]; p3 = points[tri(2)]; vols += signedVolumeOfTetrahedron(o, p1, p2, p3); &#125; return abs(vols);&#125; 参考 How to determine if a list of polygon points are in clockwise order? Efficient feature extraction for 2D/3D objects in mesh representation How calculate a volume of this mesh 3D? how-do-i-calculate-the-signed-area-of-a-triangle-in-3d-space calculate-volume-of-any-tetrahedron-given-4-points volume-of-tetrahedron-using-cross-and-dot-product]]></content>
      <categories>
        <category>algorithm</category>
        <category>math</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rotations with Quaternions]]></title>
    <url>%2F2021%2F06-01-CG-Quaternion%2F</url>
    <content type="text"><![CDATA[A quaternion is a 4 dimensional complex-like number, it has four components, three of which are the “imaginary” part. q = a + b\textrm{i} + c\textrm{j} + d\textrm{k} \\ q = (b,c,d, a) \\ \textrm{i}^{2}=\textrm{j}^{2}=\textrm{k}^{2}=\textrm{i}\textrm{j}\textrm{k}=−1We represent a quaternion with this data structure: 四元数长度 \| q \|^2 = \sqrt{x^2 + y^2 + z^2 + w^2}四元数单位化单位四元数表示三维空间中的一个旋转。 恒等四元数四元数缩放四元数乘法 q_1 = a + b \textrm{i} + c \textrm{j} + d \textrm{k} \\ q_2 = e + f \textrm{i} + g \textrm{j} + h \textrm{k} \\ q_{1}\cdot q_{2} = (ae-bf-cg-dh) + (af+be+ch-dg)\textrm{i} + \\ (ag-bh+ce+df)\textrm{j}+(ah+bg-cf+de)\textrm{k}四元数到旋转矩阵 q = (x, y, z, w) M_{\textit{rotate}} = \begin{bmatrix} 1-2yy-2zz && 2xy-2zw && 2xz+2yw && 0 \\ 2xy+2zw && 1-2xx-2zz && 2yz-2xw && 0 \\ 2xz-2yw && 2yz+2xw && 1-2xx-2yy && 0 \\ 0 && 0 && 0 && 1 \end{bmatrix}四元数共轭 q^* = a - b\textrm{i} - c\textrm{j} - d\textrm{k}四元数取逆 q^{-1} = \frac{q^*}{\| q \|^2}四元数差 q_3=q_1^{−1} \cdot q_2四元数 Exp &amp; Log \exp(q) = \exp(w) \begin{pmatrix} \frac{v_{x}}{||v||}\sin(||v||) \\ \frac{v_{y}}{||v||}\sin(||v||) \\ \frac{v_{z}}{||v||}\sin(||v||) \\ \cos(||v||) \end{pmatrix} \log(q) = \begin{pmatrix} \frac{v_{x}}{||v||}\arccos(\frac{w}{||q||}) \\ \frac{v_{y}}{||v||}\arccos(\frac{w}{||q||}) \\ \frac{v_{z}}{||v||}\arccos(\frac{w}{||q||}) \\ \log(||q||) \end{pmatrix}四元数指数 q^n = \exp(n \log(q))四元数球面插值 \textrm{slerp}(q_{1}, q_{2}, t) = q_{1}(q_{1}^{-1}q_{2})^{t}参考 Rotations with quaternions]]></content>
      <categories>
        <category>cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for Conda]]></title>
    <url>%2F2019%2F07-31-Tips4Conda%2F</url>
    <content type="text"><![CDATA[管理环境创建环境 基于 python3.6 创建一个名为test_py3的环境 1conda create -n test_py3 python=3.6 基于 python2.7 创建一个名为test_py2的环境 1conda create -n test_py2 python=2.7 删除环境直接删除相应文件夹，或者使用如下命令1conda remove -n test_py2 --all 激活环境如果要激活test_py2环境，执行下面命令：12activate test_py2 # windowssource activate test_py2 # linux/mac 此时提示符应该会变成如下内容：1(test_py2)$ 要退出该环境，执行下面命令：12(test_py2)$ deactivate # windows(test_py2)$ source deactivate # linux/mac 重命名环境conda没有重命名的命令，实现重命名是通过clone命令完成的，分为两步：123# 首先记得通过deactivate退出当前已经激活的环境conda create -n new_name --clone old_name # 拷贝旧环境conda remove -n old_name --all # 删除旧环境 包管理搜索包1conda search tensorflow 安装包1conda install tensorflow=1.13 删除包1conda uninstall tensorflow 升级包1conda update tensorflow 升级python从python3.6升级到python3.7 1conda install -c anaconda python=3.7 升级condaconda将conda、python等都视为package，因此，完全可以使用conda来管理conda和python的版本 12conda update conda # 更新condaconda update anaconda # 更新anaconda 更改配置查看配置查看当前配置1conda info 或者在C:\Users\username\.condarc或/home/username/.condarc文件中看到源情况。 更改下载源添加源使用科大源，参考Anaconda 源使用帮助 123conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 使用清华源，参考Anaconda 镜像使用帮助 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 删除源1conda config --remove channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/]]></content>
      <categories>
        <category>tips</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tips for CUDA]]></title>
    <url>%2F2019%2F06-04-Tips4CUDA%2F</url>
    <content type="text"><![CDATA[注意点 __synchthreads() 只能同步 block 中的线程，CUDA 中没有函数能够同步不同 block 中的线程。 Device 上定义的函数只能被 Device 上的函数或 kernel 调用，不能被 Host 上的函数调用。Device 上定义的函数不能调用 Host 上定义的函数。 __constant__ 变量在 kernel 中不能被改变，在 kernel 外部可以通过 cudaMemcpyToSymbol() 修改其值。 Device 上的函数不能引用 Host 上的指针（指向 host 内存空间的指针），Device 上的代码不能访问 Host 内存空间。 使用 shared memory 时，注意前面代码是否有线程 return。 shared memory 在定义时不会被初始化。 页锁定内存 (pinned memory) 在分配(cudaHostAlloc())和释放(cudaFreeHost()) 内存时消耗比较大，但是在大内存传输时有着更高的传输吞吐量，速度更快。 Ref1: Choosing Between Pinned and Non-Pinned Memory Ref2: How to Optimize Data Transfers in CUDA C/C++ 一般 32 个线程为一个 warp ，这 32 个线程上的指令在硬件上已经同步。 参考 在cuda的核函数中访问类的成员函数 Ref1: Accessing class data members from within cuda kernel - how to design proper host/device interaction?]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo搭建个人博客网站]]></title>
    <url>%2F2019%2F02-18-Mis-HexoBlog%2F</url>
    <content type="text"><![CDATA[准备工作首先下载nodejs,一路next安装即可。验证是否安装成功：12node -v # 输出 v10.15.1npm -v # 输出 6.8.0 接下来更改npm的安装源，这能大大加快安装包的速度。123npm get registry # 输出默认源 https://registry.npmjs.org/npm config set registry http://registry.npm.taobao.orgnpm get registry # 输出 http://registry.npm.taobao.org/，说明已更改为淘宝源 再运行npm install hexo-cli -g。 如果出现permission error，不要急着用sudo npm install hexo-cli -g，运行命令sudo chown -R &#39;whoami&#39; /usr/local/lib/node_modules，&#39;whoami&#39;即你的用户名，通过whoami命令查看。然后再运行npm install hexo-cli -g。 验证是否安装成功，命令行输入node -v，输出hexo-cli等众多包即对应的版本则OK。 创建本地博文服务在本地新建你的博客文件夹如blog_hexo，进入该文件夹执行下列命令：123hexo initnpm installhexo server 此时访问http://localhost:4000就能打开博文了，Ctrl+C关闭服务器。 添加新页面如要添加tags页面，首先运行下面命令1hexo new page tags 这时会在sources/tags里面生成index.md的文件，打开这个文件编辑，添加type: tags，即12345---title: tagsdate: 2016-11-11 21:40:58type: tags--- 最后再在主题配置文件中，在menu项下，把tags页打开。1234menu: home: / || home archives: /archives/ || archive tags: /tags/ || tags emoji支持Hexo默认是采用hexo-renderer-marked，这个渲染器不支持插件扩展，还有一个支持插件扩展的是 hexo-renderer-markdown-it，可以使用这个渲染引擎来支持emoji表情，具体实现过程如下：首先进入blog跟目录，执行如下命令12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --save 再安装emoji插件，执行如下命令1npm install markdown-it-emoji --save 最后再编辑站点配置文件，就是编辑根目录的_config.yml文件，添加如下内容1234567891011121314151617181920212223# Markdown-it config## Docs: https://github.com/celsomiranda/hexo-renderer-markdown-it/wikimarkdown: render: html: true xhtmlOut: false breaks: true linkify: true typographer: true quotes: '“”‘’' plugins: - markdown-it-abbr - markdown-it-footnote - markdown-it-ins - markdown-it-sub - markdown-it-sup - markdown-it-emoji # add emoji anchors: level: 2 collisionSuffix: 'v' permalink: true permalinkClass: header-anchor permalinkSymbol: ¶ 关联github首先在GitHub中新建username.github.io仓库，其中username为自己的用户名。再进入blog_hexo根目录，安装npm install hexo-deployer-git --save，然后打开根目录下的_config.yml，拉到最下面, 修改Deployment设置：123456# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/username/username.github.io branch: master 再进入根目录执行如下命令123hexo cleanhexo generatehexo deploy 第一次上传可能会让你输入git的用户名和密码。如果成功，就可以打开http://username.github.io,username替换成自己用户名。 Hexo命令常用命令123hexo n '文章名' == hexo new '文章名' #新建文章hexo g == hexo generate #生成静态页hexo d == hexo deploy #部署发布 #可与hexo g合并为hexo d -g 服务器1234hexo s == hexo server # Hexo会监视文件变动并自动更新，您无须重启服务器。hexo s -s #静态模式hexo s -p 5000 #更改端口hexo s -i 192.168.1.1 #自定义 ip 模板123hexo n '文章名' #新建文章hexo n page '页面名' #新建页面hexo n photo '页面名' #新建photo页面 参考 Hexo awesome-hexo Hexo部署github博客 亲测Hexo+Github个人博客搭建 HEXO+Github,搭建属于自己的博客 搭建Hexo博客进阶篇—API和一些小部件（四） 使用hexo+github搭建免费个人博客详细教程]]></content>
      <categories>
        <category>tutorials</category>
        <category>miscellaneous</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for Server]]></title>
    <url>%2F2018%2F10-23-Tips4Server%2F</url>
    <content type="text"><![CDATA[安装软件安装Anaconda123bash Anaconda3-5.2.0-Linux-x86_64.sh# 安装到指定目录 /opt/anaconda3/bash /opt/Anaconda3-5.2.0-Linux-x86_64.sh -p /opt/anaconda3/ -u 安装TensorFlow1234$ conda create -n tensorflow python=3.6$ source activate tensroflow(tensroflow)$ conda install tensorflow==1.8.0(tensroflow)$ conda install tensorflow-gpu==1.8.0 1tensorboard --logdir=xxx --port=6007 文件操作移动文件移动 /home/user/data_src/ 文件夹中的所有文件到 /home/user/data_dst/ 目录下，但是如果原文件夹中文件数量太多，会报Argument list too long的错误。 1mv /home/user/data_src/*.* /home/user/data_dst/ 拷贝 /home/user/data_src/ 文件夹下的 png 文件到 /home/user/data_dst/ 文件夹下 1find /home/user/data_src/ -name "*.png" | xargs -i cp &#123;&#125; /home/user/data_dst/ 删除文件将 /home/user/data/ 文件夹中的所有文件删除 1find /home/user/data/ -name "*.*" | xargs -i rm &#123;&#125; 删除指定扩展名的文件1find /home/user/data/ -name "*.png" | xargs rm -rf 拷贝文件本地到本地拷贝cp 是 copy 的缩写，即拷贝命令 12cp src/dir/file.txt dst/dir/file.txt # 拷贝文件cp -r src/dir/ dst/dir/ # 拷贝文件夹 本地到远程拷贝scp 是 secure copy 的简写，是linux系统下基于ssh登陆进行安全的远程文件或目录拷贝的命令，命令格式为 scp [参数] [原路径] [目标路径]。 12scp /home/user1/file.txt user2@172.11.11.11:/home/user2/file.txt # 拷贝文件scp /home/user1/data/ liwei@172.11.11.11:/home/user2/data/ # 拷贝文件夹 此时会出现 user1@ip&#39;s password: user2@ip&#39;s password: 提示输入密码，注意先输入 user2 的密码，按回车，再输入 user1 的密码，按回车，OK！[1] 1scp -r /home/user/data user@172.11.11.11:/D:/tmp/ 本地Win10到远程Linux12scp -P port_x Desktop/test.sh root@172.11.11.11:/home/xxx/# 将win桌面上的test.sh 传到端口号为 port_x 的开发机/home/xxx/目录下 远程Linux到本地Win1012scp -P port_x root@172.11.11.11:/home/xxx/test.sh Desktop/# 将端口号为 port_x 的开发机/home/xxx/目录下的test.sh文件传到win桌面下 文件权限ls -l your_file 中显示的内容如下： -rwxrw-r--. 1 root root 1213 Dec 17 02:53 your_file 10个字符确定不同用户能对文件干什么 第一个字符代表文件(-)、目录(d)，链接(l) 其余字符每3个一组(rwx)，读(r)、写(w)、执行(x)也可用数字表示为：r=4，w=2，x=1 因此rwx=4+2+1=7 第一组rwx：文件所有者的权限是读、写和执行 第二组rw-：与文件所有者同一组的用户的权限是读、写但不能执行 第三组r—：不与文件所有者同组的其他用户的权限是读不能写和执行 1 表示连接的文件数 root 表示用户 root 表示用户所在的组 1213 表示文件大小（字节） Dec 17 02:53 表示最后修改日期 your_file 表示文件名 修改文件权限有时候执行一个脚本或者运行一个可执行文件时，如执行脚本 ./foo.sh，会报错 -bash: ./foo.sh: Permission denied，你会再试 sudo ./foo.sh，发现继续报错 sudo: foo.sh: command not found，这时候可能是因为该文件没有执行权限，可以通过 ls -l foo.sh 查看文件信息，如果确实没有，可以为文件增加执行权限 1chmod +x foo.sh 这个时候就可以运行了。 统计文件信息du -sh filename（其实我们经常用 du -sh *，显示当前目录下所有的文件及其大小，如果要排序再在后面加上| sort -n） 1234du -sm * | sort -n # 统计当前目录大小，并按大小排序du -sk * | sort -ndu -sk * | grep filename # 查看一个文件夹的大小du -m | cut -d "/" -f 2 # 看第二个 / 字符前的文字 其他操作查看GPU信息 显示GPU使用情况 nvidia-smi 命令nvidia-smi即 NVIDIA System Management Interface. 周期性显示GPU显示情况watch -n 3 nvidia-smi // 每隔3秒输出一次 修改密码输入 passwd root 或者 passwd your_user_name 软件&amp;工具推荐软件 WinSCP: Windows 系统下进行本地与远程文件传输的 GUI 工具。 MobaXTerm: Windows 系统下的终端工具。 PyCharm: 功能强大的 Python IDE，可以同步本地与远程代码，在本地连接远程服务器允许代码。 工具 tmux: 终端工具，本地关机不影响远程服务器的程序运行。 htop: 监控工具, htop 详解 rsync: 同步工具, rsync 用法教程 命令1df -h 用来显示内存的使用情况，使用权限是所有用户。 1free [-b-k-m] [-o] [-s delay] [-t] [-V] 参数： －b －k －m：分别以字节（KB、MB）为单位显示内存使用情况。 －s delay：显示每隔多少秒数来显示一次内存使用情况。 －t：显示内存总和列。 －o：不显示缓冲区调节列。 删除软链接 rm -rf cuda 新建软连接 ln -s /usr/local/cuda-9.0 cuda 查看 ls -l cuda 参考 Rishav Sharan@StackOverflow scp 跨机远程拷贝 Linux中“Argument list too long”解决方法]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for TMUX]]></title>
    <url>%2F2018%2F10-23-Tips4TMUX%2F</url>
    <content type="text"><![CDATA[常用命令123456789101112131415161718tmux ls # 显示后台session列表tmux new -t [name] # 新建sessiontmux a -t [name] # 进入sessiontmux kill-session -t [name] # 关闭sessionprefix + $ # 重命名sessionprefix + s # 从当前session查看所有sessionprefix + d # 断开当前session连接prefix + c # 新建windowprefix + &amp; # 关闭当前windowprefix + , # 重命名当前windowprefix + " # 当前pane下侧新建paneprefix + % # 当前pane右侧新建paneprefix + x # 关闭当前paneprefix + 方向键 # 根据方向键切换panepredix + z # 进入或退出当前pane最大化 基本概念tmux采用C/S模型构建，输入tmux命令就相当于开启了一个服务器，此时默认将新建一个会话，然后会话中默认新建一个窗口，窗口中默认新建一个面板。会话、窗口、面板之间的联系如下： 一个tmux session（会话）可以包含多个window（窗口），窗口默认充满会话界面，因此这些窗口中可以运行相关性不大的任务。 一个window又可以包含多个pane（面板），窗口下的面板，都处于同一界面下，这些面板适合运行相关性高的任务，以便同时观察到它们的运行情况。 会话新建会话新建一个tmux session非常简单，语法为tmux new -s session-name，也可以简写为tmux，为了方便管理，建议指定会话名称，如下。 12tmux # 新建一个无名称的会话tmux new -s demo # 新建一个名称为demo的会话 断开当前会话会话中操作了一段时间，我希望断开会话同时下次还能接着用，怎么做？此时可以使用detach命令。 1tmux detach # 断开当前会话，会话在后台运行 也许你觉得这个太麻烦了，是的，tmux的会话中，我们已经可以使用tmux快捷键了。使用快捷键组合Ctrl+b + d，三次按键就可以断开当前会话。 进入之前的会话断开会话后，想要接着上次留下的现场继续工作，就要使用到tmux的attach命令了，语法为tmux attach-session -t session-name，可简写为tmux a -t session-name 或 tmux a。通常我们使用如下两种方式之一即可： 12tmux a # 默认进入第一个会话tmux a -t demo # 进入到名称为demo的会话 关闭会话会话的使命完成后，一定是要关闭的。我们可以使用tmux的kill命令，kill命令有kill-pane、kill-server、kill-session 和 kill-window共四种，其中kill-session的语法为tmux kill-session -t session-name。如下： 12tmux kill-session -t demo # 关闭demo会话tmux kill-server # 关闭服务器，所有的会话都将关闭 查看所有的会话管理会话的第一步就是要查看所有的会话，我们可以使用如下命令： 12tmux list-session # 查看所有会话tmux ls # 查看所有会话，提倡使用简写形式 如果刚好处于会话中怎么办？别担心，我们可以使用对应的tmux快捷键Ctrl+b + s，此时tmux将打开一个会话列表，按上下键(⬆︎⬇︎)或者鼠标滚轮，可选中目标会话，按左右键（⬅︎➜）可收起或展开会话的窗口，选中目标会话或窗口后，按回车键即可完成切换。 快捷指令关于快捷指令，首先要认识到的是：tmux的所有指令，都包含同一个前缀，默认为Ctrl+b，输入完前缀过后，控制台激活，命令按键才能生效。前面tmux会话相关的操作中，我们共用到了两个快捷键Ctrl+b + d、Ctrl+b + s，但这仅仅是冰山一角，欲窥tmux庞大的快捷键体系，请看下表。 系统指令 前缀 指令 描述 Ctrl+b ? 显示快捷键帮助文档 Ctrl+b d 断开当前会话 Ctrl+b D 选择要断开的会话 Ctrl+b Ctrl+z 挂起当前会话 Ctrl+b r 强制重载当前会话 Ctrl+b s 显示会话列表用于选择并切换 Ctrl+b : 进入命令行模式，此时可直接输入ls等命令 Ctrl+b [ 进入复制模式，按q退出 Ctrl+b ] 粘贴复制模式中复制的文本 Ctrl+b ~ 列出提示信息缓存 窗口（window）指令 前缀 指令 描述 Ctrl+b c 新建窗口 Ctrl+b &amp; 关闭当前窗口（关闭前需输入y or n确认） Ctrl+b 0~9 切换到指定窗口 Ctrl+b p 切换到上一窗口 Ctrl+b n 切换到下一窗口 Ctrl+b w 打开窗口列表，用于且切换窗口 Ctrl+b , 重命名当前窗口 Ctrl+b . 修改当前窗口编号（适用于窗口重新排序） Ctrl+b f 快速定位到窗口（输入关键字匹配窗口名称） 面板（pane）指令 前缀 指令 描述 Ctrl+b &quot; 当前面板上下一分为二，下侧新建面板 Ctrl+b % 当前面板左右一分为二，右侧新建面板 Ctrl+b x 关闭当前面板（关闭前需输入y or n确认） Ctrl+b z 最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增） Ctrl+b ! 将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效） Ctrl+b ; 切换到最后一次使用的面板 Ctrl+b q 显示面板编号，在编号消失前输入对应的数字可切换到相应的面板 Ctrl+b { 向前置换当前面板 Ctrl+b } 向后置换当前面板 Ctrl+b Ctrl+o 顺时针旋转当前窗口中的所有面板 Ctrl+b 方向键 移动光标切换面板 Ctrl+b o 选择下一面板 Ctrl+b 空格键 在自带的面板布局中循环切换 Ctrl+b Alt+方向键 以5个单元格为单位调整当前面板边缘 Ctrl+b Ctrl+方向键 以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖） Ctrl+b t 显示时钟 参考 Tmux使用手册 Tmux 快捷键 &amp; 速查表 tmux shortcuts &amp; cheatsheet]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for Notepad]]></title>
    <url>%2F2018%2F10-21-Tips4Notepad%2F</url>
    <content type="text"><![CDATA[查找和替换空行/空格/换行首先 Ctrl + F 打开查找窗口，在 查找目标 中写入要查找的对象，如空格，就写 \s，然后在下面的 查找模式 中勾选 正则表达式，点击右侧的查找下一个，便可以找到下一个空格。 如果要把所有的空格替换为换行，则点击 替换 选项，在 替换为 中写 \r\n 或者 \r\，点击 全部替换 即可。 空格：\s 回车: \r 换行: \r\n Tab: \t 正则表达式查找/.+? 注意问号后面一个空格，上面的正则表达式的意思是搜索字符串中以 / 开始，后面有一个或多个字符 (. 代表除了 \n 之外的其他字符，+ 代表匹配一次或多次，? 代表非贪婪，即从当前字符开始向后一个一个字符的匹配)，最后以空格字符结尾。 删除空行 Find what: ^\r\nReplace with: keep this emptySearch Mode: Regular expressionWrap around: selected Ref: https://stackoverflow.com/questions/3866034/removing-empty-lines-in-notepad 参考 Notepad++如何删除空行和空白字符]]></content>
      <categories>
        <category>tips</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tips for CMake]]></title>
    <url>%2F2018%2F09-19-Tips4CMake%2F</url>
    <content type="text"><![CDATA[变量与文件变量设置系统环境变量，如设置vcpkg的根目录： 1set(ENV&#123;VCPKG_ROOT&#125; D:/vcpkg) 参考：stackoverflow 列表列表追加 1list(APPEND CMAKE_PREFIX_PATH "C:/Program Files/glew") 文件保存某文件夹下的所有特定文件，如列出src文件夹下的所有.cpp文件保存在变量source中： 12file(GLOB sources $&#123;CMAKE_SOURCE_DIR&#125;/src/*.cpp)add_executable(myapp $&#123;sources&#125;) 属性设置与定义组织管理相关设置添加VS项目筛选器将项目放在一个Visual Studio筛选器下面，如将所有的测试项目放在tests下面：123set_property(GLOBAL PROPERTY USE_FOLDERS ON) # 首先打开这个属性set_property(TARGET test_app_1 PROPERTY FOLDER "tests")set_property(TARGET test_app_2 PROPERTY FOLDER "tests") 添加VS文件夹将文件组织在Visual Studio的一个文件夹下，如将CUDA的核函数放在一个单独的叫做CUDA kernels文件夹中1source_group("CUDA kernels" FILES $&#123;KERNELS&#125;) 添加宏定义1234set(DATAPATH $&#123;PROJECT_SOURCE_DIR&#125;/data) # 定义路径变量add_definitions("-DDATA_PATH=\"$&#123;DATAPATH&#125;/\"") # 设置宏DATA_PATHadd_definitions(-DHAVE_OPENCV=1) # 设置宏HAVE_OPENCV且其值为1 添加预处理定义为特定目标添加预处理定义 12target_compile_definitions(mylib PRIVATE MYLIB_DEF1) # PRIVATE属性指MYLIB_DEF1只针对mylib有效，且不能传递给依赖mylib的目标target_compile_definitions(mylib PUBLIC MYLIB_DEF_2) # PUBLIC指MYLIB_DEF2可以传递给依赖mylib的目标 添加链接库123target_link_libraries(mylib PRIVATE mylib_dep1) # mylib_dep1依赖项只在mylib的实现中使用，而没有在其头文件中使用target_link_libraries(mylib PUBLIC mylib_dep2) # mylib_dep2依赖项在头文件和实现中都有使用target_link_libraries(mylib INTERFACE mylib_dep3) # mylib_dep3依赖项只在头文件中有使用 通常，如果仅在库的实现中使用了依赖项，而在头文件中没有使用，则应使用带有PRIVATE关键字的target_link_libraries()来指定依赖关系。如果在库的头文件中也使用了依赖项，则应将其指定为PUBLIC依赖项。如果依赖性没有在库的实现使用，只在库的头文件中使用，应该将其指定为INTERFACE依赖项。 添加Debug库后缀为生成的Debug库添加后缀 12345if(WIN32) set(CMAKE_DEBUG_POSTFIX "d")endif()add_library(mylib $&#123;SOURCE_FILES&#125;)set_target_properties(mylib PROPERTIES DEBUG_POSTFIX $&#123;CMAKE_DEBUG_POSTFIX&#125;) 添加64位库后缀如果要区分生成的32位与64位库，可以通过下面的方式实现1234567if(CMAKE_SIZEOF_VOID_P EQUAL 4) set(ARCH_POSTFIX "")else() set(ARCH_POSTFIX 64)endif()add_library(my_lib$&#123;ARCH_POSTFIX&#125; $&#123;SOURCE_FILES&#125;) 配置相关设置链接静态库根据Debug或者Release模式自动链接静态库，target_link_libraries()提供了debug和optimized参数，可以根据当前具体配置来链接相应的库。==注意==：debug或optimized后面只能紧跟着一个库名称。 1target_link_libraries(myexe debug $&#123;DBG_LIB&#125; optimized $&#123;REL_LIB&#125; ) 参考：cmake: target_link_libraries &amp; stackoverflow 拷贝动态库Windows平台根据Debug或者Release模式自动拷贝动态库到可执行文件目录下 123456789101112131415# Copy DLL to binary path in windowsmacro(copy_dll target_name dll_path dll_name binary_path) if (WIN32) message("copy_dll() called: target_name = $&#123;target_name&#125;; dll_file = $&#123;dll_path&#125;/$&lt;CONFIG&gt;/$&#123;dll_name&#125;; binary_path = $&#123;binary_path&#125;/$&lt;CONFIG&gt;/") set(dll_dbg $&#123;dll_path&#125;/Debug/$&#123;dll_name&#125;d.dll) set(dll_rel $&#123;dll_path&#125;/Release/$&#123;dll_name&#125;.dll) #message("dll_dbg = $&#123;dll_dbg&#125;") #message("dll_rel = $&#123;dll_rel&#125;") add_custom_command(TARGET $&#123;target_name&#125; POST_BUILD COMMAND $&#123;CMAKE_COMMAND&#125; -E echo "copy $&lt;CONFIG&gt; dll $&lt;IF:$&lt;CONFIG:Debug&gt;,$&#123;dll_dbg&#125;,$&#123;dll_rel&#125;&gt; to $&#123;binary_path&#125;/$&lt;CONFIG&gt;/" COMMAND $&#123;CMAKE_COMMAND&#125; -E copy_if_different $&lt;IF:$&lt;CONFIG:Debug&gt;,$&#123;dll_dbg&#125;,$&#123;dll_rel&#125;&gt; $&#123;binary_path&#125;/$&lt;CONFIG&gt;/ COMMENT "copy dll for target $&#123;target_name&#125; done" ) endif (WIN32)endmacro() 参考：cmakecommands &amp; tool-cmake 输出输出包含路径如果想要打印出项目的所有包含路径，使用命令 1234get_property(dirs DIRECTORY $&#123;CMAKE_CURRENT_SOURCE_DIR&#125; PROPERTY INCLUDE_DIRECTORIES)foreach(dir $&#123;dirs&#125;) message(STATUS "dir='$&#123;dir&#125;'")endforeach() 注意：上面方法只能追踪该命令调用前CMakeLists.txt中通过include_directories命令包含的路径。 命令编译器指定编译器1cmake -DCMAKE_CXX_COMPILER=g++-8 .. CMake-GUI指定编译器如果需要用CMake-GUI生成VS2017的项目，但是使用VS2015的编译器，在下方填写1v140 对应的cmake命令是cmake -G &quot;Visual Studio 15 2017&quot; -T v140。另外VS2017对应v141，VS2019对应v142。 VS CodeBugs FixLNK2026 LNK2026: XXX模块对于SAFESEH映像是不安全的解决方法：在add_executable或add_library之前加 123set(CMAKE_EXE_LINKER_FLAGS "$&#123;CMAKE_EXE_LINKER_FLAGS&#125; /SAFESEH:NO")set(CMAKE_SHARED_LINKER_FLAGS "$&#123;CMAKE_SHARED_LINKER_FLAGS&#125; /SAFESEH:NO")set(CMAKE_MODULE_LINKER_FLAGS "$&#123;CMAKE_MODULE_LINKER_FLAGS&#125; /SAFESEH:NO") error C1128 error C1128: 字节数超过对象文件格式限制: 请使用 /bigobj 进行编译解决方法：在CMakeLists.txt文件中加上 1add_compile_options(-bigobj)]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的数据标准化]]></title>
    <url>%2F2018%2F08-18-ML-DataNormalization%2F</url>
    <content type="text"><![CDATA[标准化方法Z-score NormalizationZ-score normalization又叫 standardization(规范化)，将特征进行缩放使得其具有均值为0，方差为1的标准正态分布的特性。 z = \frac{x-\mu}{\sigma}其中均值$\mu = \frac{1}{N} \sum_{i=1}^N (x_i)$，方差$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2}$。 标准化特征使其具有0均值1方差不仅对于比较单位不同的测量有用，而且还是众多机器学习算法的普遍要求。如果特征在不同的尺度下，不同的权重更新的速率会差异很大，不利于算法的收敛。 Mix-max Normalization另一种常用的标准化叫Mix-max normalization，通常被简称为normalization(标准化)。这种方法将数据放缩到一个固定的区间，通常是0到1之间。 z = \frac{x-\min{(x)}}{\max{(x)}-\min{(x)}}Constant Normalization将数据除以一个固定的常数，比如最大值： z = \frac{x}{\max{(|x|)}}Binary Encoding类别值(Categorical values)例如性别可以编码为0和1，如男性为1，女性为0，也可以编码为-1和1. Manhattan Encoding如果是不能二进制编码的类别值，可以使用曼哈顿编码Manhattan encoding，使用0或1来表示特征是否被包含： 例如：在宗教类别中，可以将穆斯林(Muslim)、印度教(Hindu)和基督教(Christian)分别编码为 [1 0 0]、[0 1 0] 和 [0 0 1]。 示例代码代码参考自About Feature Scaling and Normalization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import pandas as pdimport numpy as npdf = pd.io.parsers.read_csv( 'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', header=None, usecols=[0,1,2] )df.columns=['Class label', 'Alcohol', 'Malic acid']df.head()from sklearn import preprocessing# Compute the mean and std to be used for later scaling.std_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']]) # Perform standardization by centering and scalingdf_std = std_scale.transform(df[['Alcohol', 'Malic acid']]) minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])print('Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_std[:,0].mean(), df_std[:,1].mean()))print('\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_std[:,0].std(), df_std[:,1].std()))print('\nMin-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_minmax[:,0].min(), df_minmax[:,1].min()))print('Max-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_minmax[:,0].max(), df_minmax[:,1].max()))from sys import platform as _platformif _platform == "darwin": import matplotlib matplotlib.use('TkAgg')from matplotlib import pyplot as pltdef plot(): plt.figure(figsize=(8,6)) plt.scatter(df['Alcohol'], df['Malic acid'], color='green', label='input scale', alpha=0.5) plt.scatter(df_std[:,0], df_std[:,1], color='red', label='Standardized [$N (\mu=0, \; \sigma=1)$]', alpha=0.3) plt.scatter(df_minmax[:,0], df_minmax[:,1], color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3) plt.title('Alcohol and Malic Acid content of the wine dataset') plt.xlabel('Alcohol') plt.ylabel('Malic Acid') plt.legend(loc='upper left') plt.grid() plt.tight_layout()plot()plt.show() 结果 Reference Preprocessing data A Dummies Guide to Data Normalization About Feature Scaling and Normalization]]></content>
      <categories>
        <category>algorithm</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convolutional Neural Network in TensorFlow]]></title>
    <url>%2F2018%2F07-19-ML-TensorFlowConv%2F</url>
    <content type="text"><![CDATA[翻译自Build a Convolutional Neural Network using Estimators TensorFlow的layer模块提供了一个轻松构建神经网络的高端API，它提供了创建稠密（全连接）层和卷积层，添加激活函数，应用dropout regularization的方法。本教程将介绍如何使用layer来构建卷积神经网络来识别MNIST数据集中的手写数字。 MNIST数据集由60,000训练样例和10,000测试样例组成，全部都是0-9的手写数字，每个样例由28x28大小的图片构成。 Getting Started首先来搭建TensorFlow程序的骨架，创建一个叫cnn_mnist.py的文件，并在其中添加下面代码： 1234567891011121314from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_function# Importsimport numpy as npimport tensorflow as tftf.logging.set_verbosity(tf.logging.INFO)# Our application logic will be added hereif __name__ == "__main__": tf.app.run() 下面的教程将指导如何在该文件中添加代码来构建、训练、评价卷积神经网络。最终完成代码可以在这里下载。 Intro to Convolutional Neural Networks卷积神经网络（Convolutional Neural Networks, CNNs）是图像分类任务的主流架构，CNNs通过对图像的原始像素数据作用一系列的滤波来提取并学习高阶特征，然后模型使用该特征来进行分类。CNNs包含三个部分： 卷积层（Convolutional layers）：卷积层在图像上应用一定数目的卷积滤波。对于图像的每一个子区域，该层使用一些数学运算来产生输出特征图中的一个值。然后卷积层一般会继续对输出结果使用ReLU激活函数以在模型中引入非线性性。 池化层（Pooling layers）：池化层对卷积层提取的图像数据进行下采样，来减小特征图的维度以减少处理时间。一个广泛使用的池化算法是最大池化，最大池化提取特征图的一个子区域（如2x2的子像素块），只保留其最大值。 全连接层（Dense (fully connected) layers）：全连接层在通过卷积层和池化层处理后得到的特征中进行分类，在全连接层中，该层的每一个节点与下一层的每一个节点都有连接。 通常，一个卷积神经网络由一堆进行特征提取的卷积模块组成，每一个模块由一个卷积层，紧接着一个池化层组成，最后一个卷积模块后面接着一个或多个全连接层来进行分类。最后一个全连接层的每一个节点对应模型目标类别中的每一个分类，并借助一个softmax激活函数来为每一个节点产生一个0到1之间的值（所有节点值的和为1），可以借助softmax得到的值来解释目标图像落在每个类别中的相对概率。 Note：斯坦福大学的Convolutional Neural Networks for Visual Recognition课程资料有关于CNN架构的更详细的介绍。 Building the CNN MNIST Classifier接下来使用下面的CNN架构来构建一个模型对MNIST数据集中的图像进行分类： Convolutional Layer #1: 应用32个5x5的滤波（提取5x5的像素块），并使用ReLU激活函数 Pooling Layer #1: 使用最大池化，滤波大小为2x2，stride为2（使得被池化的区域不会重叠） Convolutional Layer #2: 应用64个5x5的滤波，并使用ReLU激活函数 Pooling Layer #2: 同样，使用最大池化，滤波大小为2x2，stride为2 Dense Layer #1: 1024个神经元，dropout regularization rate为0.4（在训练的过程中每个元素有0.4的概率被丢弃） Dense Layer #2 (Logits Layer): 10个神经元，每个代表数字的类别（0到9） tf.layers模块提供了创建这三个神经网络层的方法： conv2d(). 创建一个2维的卷积层，参数包括滤波个数，滤波核大小，padding，激活函数。 max_pooling2d(). 使用最大池化算法构建一个2维的池化层。参数包括池化滤波大小和strides。 dense(). 构建一个全连接层，参数包括神经元的个数和激活函数。 这些方法的输入都是一个张量（Tensor），输出是一个变换的张量，这也使得层与层之间的连接变得简单，即只需将一层的输出当作下一层的输入。 打开cnn_mnist.py文件并添加下面符合TensorFlow的Estimator API接口的cnn_model_fn函数。cnn_mnist.py将MINIST特征数据、标记和模型模式（TRAIN，EVAL，PREDICT）作为参数，配置CNN，然后返回预测、损失和一个训练操作： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def cnn_model_fn(features, labels, mode): """Model function for CNN.""" # Input Layer input_layer = tf.reshape(features["x"], [-1, 28, 28, 1]) # Convolutional Layer #1 conv1 = tf.layers.conv2d( inputs=input_layer, filters=32, kernel_size=[5, 5], padding="same", activation=tf.nn.relu) # Pooling Layer #1 pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2) # Convolutional Layer #2 and Pooling Layer #2 conv2 = tf.layers.conv2d( inputs=pool1, filters=64, kernel_size=[5, 5], padding="same", activation=tf.nn.relu) pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2) # Dense Layer pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu) dropout = tf.layers.dropout( inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN) # Logits Layer logits = tf.layers.dense(inputs=dropout, units=10) predictions = &#123; # Generate predictions (for PREDICT and EVAL mode) "classes": tf.argmax(input=logits, axis=1), # Add `softmax_tensor` to the graph. It is used for PREDICT and by the # `logging_hook`. "probabilities": tf.nn.softmax(logits, name="softmax_tensor") &#125; if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) # Calculate Loss (for both TRAIN and EVAL modes) loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits) # Configure the Training Op (for TRAIN mode) if mode == tf.estimator.ModeKeys.TRAIN: optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) train_op = optimizer.minimize( loss=loss, global_step=tf.train.get_global_step()) return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op) # Add evaluation metrics (for EVAL mode) eval_metric_ops = &#123; "accuracy": tf.metrics.accuracy( labels=labels, predictions=predictions["classes"])&#125; return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops=eval_metric_ops) 下面的章节将更加详细的介绍创建每一层的tf.layers代码，还有怎样计算损失，配置训练操作，生成预测。熟悉CNN的可以直接跳到Training and Evaluating the CNN MNIST Classifier章节. Input Layer在layers模块中创建二维图像数据的卷积和池化层的方法期望输入张量默认的形状是[batch_size, image_height, image_width, channels]，这个行为可以通过使用data_format参数来改变， batch_size.在训练中进行梯度下降时使用的样例子集的大小。 image_height.样例图像的高度。 image_width. 样例图像的宽度。 channels. 样例图像的颜色通道数。对于彩色图像，通道数是3(red, green, blue)，对于黑白图像monochrome images，通道数是1(black)。 data_format. 一个字符串，channels_last和channels_first中的一个值，默认是channels_last，其中channels_last对应输入形状(batch, ..., channels)，channels_first对应输入形状(batch, channels, ...)。 在这个例子中，MNIST数据集由28x28的黑白图像组成，所以输入层的目标形状是[batch_size, 28, 28, 1]。为了将输入的特征图转化为这个形状，可以使用下面的reshape操作：1input_layer = tf.reshape(features["x"], [-1, 28, 28, 1]) 注意这里将batch size指定为-1，表示这个维度将根据features[&quot;x&quot;]中输入值的个数来动态计算，其他维度的大小均设置为常量。这样可以将batch_size当作一个可以调节的超参数。例如，如果按照5个batches传递样例，features[&quot;x&quot;]将包含3,920个值，输入层的形状是[5, 28, 28, 1]。同样，如果传递100个batches的样例，features[&quot;x&quot;]将包含78,400个值，输入层的形状是[100, 28, 28, 1]。 Convolutional Layer #1在第一个卷积层，这里对输入层使用32个5x5的滤波，并使用ReLU激活函数，可以使用layers模块中的conv2d()方法来创建该层：123456conv1 = tf.layers.conv2d( inputs=input_layer, filters=32, kernel_size=[5, 5], padding="same", activation=tf.nn.relu) 参数inputs指定输入张量，形状必须是[batch_size, image_height, image_width, channels]。这里将第一个卷积层与形状为[batch_size, 28, 28, 1]的输入层input_layer连接。注意：当传入的参数data_format=channels_first时，conv2d()的输入张量形状必须是[batch_size, channels, image_height, image_width]。 参数filters指定使用滤波的数目，kernel_size通过[height, width]的形式指定滤波的维度，如果滤波的height和width的值相同，可以直接使用一个整数来设置kernel_size参数，如kernel_size=5。 参数padding指定两个枚举变量中的一个，valid或same，大小写不敏感，默认值为valid。如果需要输出张量与输入张量有相同的height和width值，设置padding=same，TensorFlow将在输入张量的边缘添加0值，以确保输出张量的height和width为28.（如果不设置为padding，在28x28的张量上进行5x5的卷积操作将产生一个24x24的张量，因为在28x28的格子上只有24x24个位置能够提取5x5的小块。） 参数activation指定作用在卷积输出张量上的激活函数，这里借助tf.nn.relu来指定ReLU激活函数。 这里通过conv2d()生成的输出张量的形状为[batch_size, 28, 28, 32]：跟输入有着同样的height和width维度，但是现在有32个通道，其来自于32个滤波。 Pooling Layer #1下面将第一个池化层连接到刚刚创建的卷积层，这里使用layers中的max_pooling2d()方法来创建一层来进行滤波大小为2x2，stride为2的最大池化：1pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2) 同样，inputs指定输入张量，形状是[batch_size, image_height, image_width, channels]，这里输入的张量是conv1，即第一个卷积层的输出，其形状为[batch_size, 28, 28, 32]。 注意：跟conv2d()一样，如果传入参数data_format=channels_first，max_pooling2d()也必须接受形状为[batch_size, channels, image_height, image_width]的张量。 参数pool_size指定最大池化滤波大小[height, width]，如果两个维度的值一样，可以直接用一个整数值代替，如pool_size=2。 参数strides指定stride的大小。这里将stride设为2，表示滤波提取的子区域在height和width方向应该相差2个像素间隔（对于2x2滤波，这意味着被提取的区域都不会有交叠）。如果想为height和width设置不同的stride值，可以通过指定一个元组或列表如stride=[3, 6]来实现。 由max_pooling2d()产生的输出张量pool1的形状为[batch_size, 14, 14, 32]：这里2x2的滤波将height和width分别减少了50%。 Convolutional Layer #2 and Pooling Layer #2和之前一样，可以通过conv2d()和max_pooling2d()将第二个卷积层和池化层连接到已有的CNN上。对于第二个卷积层，这里使用64个5x5的滤波，并同样使用ReLU激活函数。对于第二个池化层，这里使用和第一个池化层同样的配置，即2x2的最大池化，stride为2。 12345678conv2 = tf.layers.conv2d( inputs=pool1, filters=64, kernel_size=[5, 5], padding="same", activation=tf.nn.relu)pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2) 注意第二个卷积层将第一个池化层的输出张量作为输入，并输出张量conv2，conv2的形状为[batch_size, 14, 14, 64]，和pool1具有同样的height和width（由于padding=&quot;same&quot;），64个通道是由于有64个滤波作用。第二个池化层将conv2作为输入，输出pool2，其形状为[batch_size, 7, 7, 64]。 Dense Layer接下来，在当前CNN上添加一个全连接层（包括1,024个神经元，使用ReLU激活函数），以在前面卷积层和池化层提取的特征上做分类。在连接该层之前，需要将特征图pool2展开成[batch_size, features]的形状，这样该张量便只有两个维度：1pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) 在上面的reshape()运算中，-1表示batch_size维度将根据输入数据的样例个数来动态计算，每个样例有7 (pool2 height) * 7 (pool2 width) * 64 (pool2 channels)个特征，因此特征的维度是7764（总共3136）。输出张量pool2_flat的形状为[batch_size, 3136]。 现在可以通过layers中的dense()方法来连接全连接层：1dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu) 参数inputs指定输入张量：展开的特征图pool2_flat。参数units指定全连接层中的神经元数目。参数activation指定激活函数，这里同样使用tf.nn.relu来添加ReLU激活函数。 为了改进模型的结果，这里对全连接层使用dropout正则化，使用layers中的dropout方法：12dropout = tf.layers.dropout( inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN) 同样，inputs表示输入张量，这里是全连接层的输出。参数rate指定dropout rate，这里使用0.4，表示在训练过程中有40%的元素会被随机丢弃。参数training由一个布尔值指定当前是否是训练模式。dropout只在training是True的情况下使用。这里检查传入模型函数cnn_model_fn的模式是否是TRAIN模式。 输出的张量dropout的形状是[batch_size, 1024]。 Logits Layer神经网络的最后一层是logits layer，该层将返回预测的原始值。这里创建一个有10个神经元（每个神经元表示0-9的目标类别）的全连接层，使用默认的线性激活函数：1logits = tf.layers.dense(inputs=dropout, units=10) CNN最终的输出张量logits的形状为[batch_size, 10]。 Generate Predictions模型的logits layer返回一个形状为[batch_size, 10]的张量作为预测的原始值，接下来将这些原始值转换为2种不同的格式使得模型函数能够返回： 每个样例预测的类别：0-9的一个数字 一个样例属于每个类别的概率，如某个样例是0的概率、1的概率等等。 给定一个类别，所预测的类别是logits张量所对应行中的最大值，可以通过tf.argmax函数找到该最大值的索引：1tf.argmax(input=logits, axis=1) 参数input指定需要提取最大值的张量，即logits。参数axis指定需要寻找最大值的输入张量的轴，这里需要沿着索引为1的维度，即对应我们的预测结果来找最大值（注意张量logits的形状为[batch_size, 10]）。 通过使用softmax函数tf.nn.softmax从logits layer得到概率：1tf.nn.softmax(logits, name="softmax_tensor") 注意：这里使用参数name来显示地命名这个运算为softmax_tensor，这样后面可以引用它。（后面要为softmax值设置记录（logging）） 这里将预测结果编制进一个词典，返回一个EstimatorSpec对象：123456predictions = &#123; "classes": tf.argmax(input=logits, axis=1), "probabilities": tf.nn.softmax(logits, name="softmax_tensor")&#125;if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) Calculate Loss对于训练和评价，都需要定义一个损失函数来估计模型预测的值与实际的目标类别的接近程度。对于多目标分类问题如MNIST，一般用交叉熵来作为损失度量。下面的代码计算模型在TRAIN或EVAL模式下的交叉熵：1loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits) 张量labels包含样例预测索引的列表，如[1, 9, …]。logits包含最后一层的线性输出。函数tf.losses.sparse_softmax_cross_entropy从这两个输入以高效、数值稳定的方式计算softmax crossentropy，也叫categorical crossentropy或negative log-likelihood。 Configure the Training Op 在前面的小节中，已经将CNN的损失定义为logits层和已知labels的softmax cross-entropy，下面配置模型在训练过程中去优化该损失函数，这里将使用0.001的学习率、随机梯度下降法作为优化算法：123456if mode == tf.estimator.ModeKeys.TRAIN: optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) train_op = optimizer.minimize( loss=loss, global_step=tf.train.get_global_step()) return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op) 注意：需要更深入地了解为Estimator模型函数配置训练运算，参考Defining the training op for the model. Add evaluation metrics为了在模型中增加准确度量，这里在EVAL模式下定义一个eval_metric_ops字典：12345eval_metric_ops = &#123; "accuracy": tf.metrics.accuracy( labels=labels, predictions=predictions["classes"])&#125;return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops=eval_metric_ops) Training and Evaluating the CNN MNIST ClassifierMNIST CNN模型的函数已经完成，下面准备训练并评价该模型。 Load Training and Test Data首先载入训练和测试数据，在cnn_mnist.py文件中添加一个main()函数：1234567def main(unused_argv): # Load training and eval data mnist = tf.contrib.learn.datasets.load_dataset("mnist") train_data = mnist.train.images # Returns np.array train_labels = np.asarray(mnist.train.labels, dtype=np.int32) eval_data = mnist.test.images # Returns np.array eval_labels = np.asarray(mnist.test.labels, dtype=np.int32) 这里将训练特征数据（55,000张手写数字图像的原始像素值）和训练标签（每张图像对应的从0到9的值）作为numpy arrays的形式分别存储在train_data和train_labels中。同样，评价特征数据（10,000张图像）和评价标签被分别存储在eval_data和eval_labels中。 Create the Estimator接下来为模型创建一个Estimator（一个进行高端模型训练、评价和推断的TensorFlow类）。在 main()中添加如下代码：123# Create the Estimatormnist_classifier = tf.estimator.Estimator( model_fn=cnn_model_fn, model_dir="/tmp/mnist_convnet_model") 参数model_fn指定用于训练、评价和预测的模型函数，这里传入前面创建的cnn_model_fn函数。参数model_dir指定模型数据(checkpoints)存储的路径。 Set Up a Logging Hook由于CNNs需要花费一定的时间去训练，这里设置一些记录以能够追踪训练的过程。可以使用TensorFlow的tf.train.SessionRunHook创建一个tf.train.LoggingTensorHook来记录softmax layer得到的概率值。在main()中添加如下代码：1234# Set up logging for predictionstensors_to_log = &#123;"probabilities": "softmax_tensor"&#125;logging_hook = tf.train.LoggingTensorHook( tensors=tensors_to_log, every_n_iter=50) 这里在tensors_to_log中存储想要记录的张量的字典，每个key是记录输出中要打印的标签，对应的标签是张量在TensorFlow图中的名字，这里probabilities可以在softmax_tensor中找到，前面在cnn_model_fn中生成概率时在softmax运算中指定的名字。 注意：如果不显示地通过name参数来给一个运算命名，TensorFlow会指定一个默认的名字。通过TensorBoard可视化运算图或者打开TensorFlow Debugger (tfdbg)可以发现每个运算的名字。 接下来创建LoggingTensorHook，并将tensors_to_log传入tensors参数，这里设置every_n_iter=50表示在训练中每50步输出一次probabilities。 Train the Model接下来创建train_input_fn函数并在mnist_classifier中调用train()来准备训练模型。在main()中添加下面代码：1234567891011# Train the modeltrain_input_fn = tf.estimator.inputs.numpy_input_fn( x=&#123;"x": train_data&#125;, y=train_labels, batch_size=100, num_epochs=None, shuffle=True)mnist_classifier.train( input_fn=train_input_fn, steps=20000, hooks=[logging_hook]) 在numpy_input_fn的调用中，训练特征数据和标签分别被传入x（作为一个dict）和y。batch_size被设置为100，表示在每一步中模型训练100个样例批次。num_epochs=None表示模型会一直训练直到达到给定的步数。shuffle=True表示随机改组训练数据。在train的调用中，设置steps=20000表示模型将会训练20,000步。将logging_hook传入参数hooks使得在训练的过程中其可以被触发。 Evaluate the Model一旦训练完成，便可以计算其在MNIST测试集上的准确率来评价模型，这里调用evaluate方法来评价在model_fn的eval_metric_ops参数中指定的度量，在main()中添加如下代码：12345678# Evaluate the model and print resultseval_input_fn = tf.estimator.inputs.numpy_input_fn( x=&#123;"x": eval_data&#125;, y=eval_labels, num_epochs=1, shuffle=False)eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)print(eval_results) 为了创建eval_input_fn，设置num_epochs=1，这样模型可以在每个epoch评价度量并返回结果，这里设置shuffle=False以在数据中依次迭代。 Run the Model到目前为止，CNN模型函数、Estimator、训练/评价逻辑都编码完成，接下来看看结果。运行cnn_mnist.py。 注意：训练CNNs非常耗时。cnn_mnist.py的估计完成时间取决于处理器，但在CPU上一般需要1小时以上。为了更快速地训练，可以减小传入train()的步数，但这会影响准确率。 随着模型训练，可以看到如下输出记录：12345678910111213INFO:tensorflow:loss = 2.36026, step = 1INFO:tensorflow:probabilities = [[ 0.07722801 0.08618255 0.09256398, ...]]...INFO:tensorflow:loss = 2.13119, step = 101INFO:tensorflow:global_step/sec: 5.44132...INFO:tensorflow:Loss for final step: 0.553216.INFO:tensorflow:Restored model from /tmp/mnist_convnet_modelINFO:tensorflow:Eval steps [0,inf) for training step 20000.INFO:tensorflow:Input iterator is exhausted.INFO:tensorflow:Saving evaluation summary for step 20000: accuracy = 0.9733, loss = 0.0902271&#123;'loss': 0.090227105, 'global_step': 20000, 'accuracy': 0.97329998&#125; 可以看到最终在测试集上可以达到97.3%的准确率。 Additional ResourcesTo learn more about TensorFlow Estimators and CNNs in TensorFlow, see the following resources: Creating Estimators in tf.estimator provides an introduction to the TensorFlow Estimator API. It walks through configuring an Estimator, writing a model function, calculating loss, and defining a training op. Advanced Convolutional Neural Networks walks through how to build a MNIST CNN classification model without estimators using lower-level TensorFlow operations. 上次更新日期：七月 19, 2018 Reference Build a Convolutional Neural Network using Estimators]]></content>
      <categories>
        <category>translation</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bvh骨骼动画文件格式]]></title>
    <url>%2F2018%2F07-19-CG-bvh%2F</url>
    <content type="text"><![CDATA[简介BVH是BioVision等设备对人体运动进行捕获后产生文件格式的文件扩展名。 BVH文件包含角色的骨骼和肢体关节旋转数据。BVH 是一种通用的人体特征动画文件格式，广泛地被当今流行的各种动画制作软件支持，如3DMax。 文件格式文件主要部分骨架信息 和 数据块 骨架信息：按照层次关系，定义了如root、hip、leg等位置和旋转分量，从而形成一个完整的骨架 数据块：对应上面的骨架各部位标出每帧的数据信息 一个BVH文件包含两部分，头部部分和数据部分，头部部分描述了骨架的层次关系和初始姿势，数据部分包含了动作(motion)的数据. 文件示例可以参考文件示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116HIERARCHYROOT Hips&#123; OFFSET 0.00 0.00 0.00 CHANNELS 6 Xposition Yposition Zposition Zrotation Xrotation Yrotation JOINT Chest &#123; OFFSET 0.00 5.21 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT Neck &#123; OFFSET 0.00 18.65 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT Head &#123; OFFSET 0.00 5.45 0.00 CHANNELS 3 Zrotation Xrotation Yrotation End Site &#123; OFFSET 0.00 3.87 0.00 &#125; &#125; &#125; JOINT LeftCollar &#123; OFFSET 1.12 16.23 1.87 CHANNELS 3 Zrotation Xrotation Yrotation JOINT LeftUpArm &#123; OFFSET 5.54 0.00 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT LeftLowArm &#123; OFFSET 0.00 -11.96 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT LeftHand &#123; OFFSET 0.00 -9.93 0.00 CHANNELS 3 Zrotation Xrotation Yrotation End Site &#123; OFFSET 0.00 -7.00 0.00 &#125; &#125; &#125; &#125; &#125; JOINT RightCollar &#123; OFFSET -1.12 16.23 1.87 CHANNELS 3 Zrotation Xrotation Yrotation JOINT RightUpArm &#123; OFFSET -6.07 0.00 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT RightLowArm &#123; OFFSET 0.00 -11.82 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT RightHand &#123; OFFSET 0.00 -10.65 0.00 CHANNELS 3 Zrotation Xrotation Yrotation End Site &#123; OFFSET 0.00 -7.00 0.00 &#125; &#125; &#125; &#125; &#125; &#125; JOINT LeftUpLeg &#123; OFFSET 3.91 0.00 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT LeftLowLeg &#123; OFFSET 0.00 -18.34 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT LeftFoot &#123; OFFSET 0.00 -17.37 0.00 CHANNELS 3 Zrotation Xrotation Yrotation End Site &#123; OFFSET 0.00 -3.46 0.00 &#125; &#125; &#125; &#125; JOINT RightUpLeg &#123; OFFSET -3.91 0.00 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT RightLowLeg &#123; OFFSET 0.00 -17.63 0.00 CHANNELS 3 Zrotation Xrotation Yrotation JOINT RightFoot &#123; OFFSET 0.00 -17.14 0.00 CHANNELS 3 Zrotation Xrotation Yrotation End Site &#123; OFFSET 0.00 -3.75 0.00 &#125; &#125; &#125; &#125;&#125;MOTIONFrames： 2Frame Time： 0.033333 8.03 35.01 88.36 -3.41 14.78 -164.35 13.09 40.30 -24.60 7.88 43.80 0.00 -3.61 -41.45 5.82 10.08 0.00 10.21 97.95 -23.53 -2.14 -101.86 -80.77 -98.91 0.69 0.03 0.00 -14.04 0.00 -10.50 -85.52 -13.72 -102.93 61.91 -61.18 65.18 -1.57 0.69 0.02 15.00 22.78 -5.92 14.93 49.99 6.60 0.00 -1.14 0.00 -16.58 -10.51 -3.11 15.38 52.66 -21.80 0.00 -23.95 0.00 7.81 35.10 86.47 -3.78 12.94 -166.97 12.64 42.57 -22.34 7.67 43.61 0.00 -4.23 -41.41 4.89 19.10 0.00 4.16 93.12 -9.69 -9.43 132.67 -81.86 136.80 0.70 0.37 0.00 -8.62 0.00 -21.82 -87.31 -27.57 -100.09 56.17 -61.56 58.72 -1.63 0.95 0.03 13.16 15.44 -3.56 7.97 59.29 4.97 0.00 1.64 0.00 -17.18 -10.02 -3.08 13.56 53.38 -18.07 0.00 -25.93 0.00 头部部分 头部部分开头包含”HIERARCHY“关键词 接着一行开头为”ROOT”关键词，后面跟着root的名字.当然，一个root段(segment)之后，允许再定义另一个段，也可以定义为”ROOT”.原理上，BVH文件能够包含任何数量的骨架段. BVH文件的格式为递归的格式，层次结构的每一个segment包含了一些相关数据，该segment要递归定义它的子segment. 关键词”ROOT“后以大括号开头，下行以tab填充，能够增加可读性. segment的第一个信息是该segment相对于父segment的偏移量(OFFSET)， 如果是根ROOT segment则偏移量OFFSET则通常为0，OFFSET指定了X，Y，Z方向上与父segment的偏移量，偏移量也指示用于绘制父segment的长度和方向. BVH文件中没有明确的信息能够描述一个segment该怎样画，通常通过从第一个segment的偏移量来定义父segment，典型的，只有ROOT和上部主体的segment将有多个子segment. 接下来一是”CHANNEL“头部信息，后面跟着的数字表明channels的数目，后面的标签表明每一个channel的类型. 一个BVH文件解析器必须跟踪channel的数目和channel的类型，在后面MOTION信息被解析的时候，这个顺序在解析每一行的MOTION数据的时候需要用到.这种格式规定很明显具有灵活性，能够允许segment拥有任何数目的channel，且能够按照任何顺序. channel旋转的次序是：Z X Y， 接下来能看到”JOINT“和”END SITE“关键词，一个JOINT的定义和ROOT的定义是相同的，除了CHANNEL的数目不同，JOINT正是递归开始的地方，剩下部分对JOINT的解析和ROOT一样的. END SITE结束了递归，它的定义还包含了一些数据，它提供了前segment的长度，就像前面子segment提供了offset用于绘制父segment的长度和方向 JOINT的结尾以右括号结束，BVH中，空间被定义成了右旋坐标系与Y轴作为世界矢量. 数据部分 数据部分以MOTION开头，后面的数字表示了帧率，下一行Frame Time定义了采样频率 文件剩余的部分包含了motion的数据，每一行是一个motion数据的样本， The numbers appear in the order of the channel specifications as the skeleton hierarchy was parsed. 关键词 HIERARCHY ： 开头 ROOT ： 根关节 JOINT ： 根关节下的关节 OFFSET ： 子关节相对父关节的偏移，也可以表示对应父关节的长度和方向，当子关节不止一个时，采用第一个子关节的数据. CHANNELS ： 给出了关于channel的个数和名称(ROOT总是拥有6个channels，而一般JOINT只有3个，较之ROOT缺少了XYZ的position信息，因为子关节只需要根据它相对于父关节的偏移就可以算出它在坐标系中的具体位置了) 注意 rotation channel的顺序是：Zrotation Xrotation Yrotation BVH格式的运动采取的旋转方式比较特别-End Site 表示终结递归，该关节的定义到此为止，可看作一个终端效应器 数据块以 MOTION 关键字开始-Frames： 定义帧数，Frame Time：定义数据采样速率－每帧的时间长度，如0.033333则表示BVH文件的一般采样速率，每秒30帧 接下来的数据就是实际的运动数据，对应骨架信息的层次结构 对于子关节来说，平移信息存储在骨架信息的OFFSET中，旋转信息则来自于MOTION部分； 对于根结点来说，平移量是OFFSET和Motion section中定义的平移量之和。 BVH不考虑Scale变换。 数据计算segment的位置，需要从局部的平移和旋转信息创建一个变换矩阵.对于任何joint，平移的信息将会是层次部分中定义的offset，旋转信息则从motion部分得到 对于根ROOT，平移数据为offset的总和，而旋转的消息来自motion部分，BVH不需要考虑比例因子的计算. 创建变换矩阵的直接方法是创建3个单独的旋转矩阵，每个矩阵对应每根轴 vR = vYXZ 解析器可以参考：BVH java解析器 参考 Biovision BVH BVH文件格式解析 PyMO-A library for using motion capture data for machine learning 文章转载自 BVH文件格式解析]]></content>
      <categories>
        <category>repost</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Funstions for Tensor Operations]]></title>
    <url>%2F2018%2F07-19-TF-Functions%2F</url>
    <content type="text"><![CDATA[形状相关 Tensor Shapetf.reshape12345tf.reshape( tensor, shape, name=None) 给定一个tensor，该函数返回一个与输入tensor具有相同值且形状为shape的张量。 如果shape中有一个特殊值-1，则该维度的大小通过整个tensor的尺寸计算得到，特殊情况下shape为[-1]时则将tensor摊平成1-D。shape最多有一个值为-1。 如果shape是一维或更高维度，该函数返回的形状为shape的张量中的值由tensor的值填充，在这种情况下，由shape决定的元素个数必须与输入tensor中的元素个数一致。 Ref tf.reshape Args: tensor: 一个Tensor张量 。 shape: 一个Tensor张量，必须是int32或int64类型，决定了输出张量的形状。 name: 操作的名字，可选参数。 Returns:一个 Tensor张量，和输入tensor具有相同的数据类型。 tf.expand_dims123456tf.expand_dims( input, axis=None, name=None, dim=None) 给定一个张量input，该函数在input的形状中维度索引为axis的位置插入一个维度为1的维度，维度索引axis从0开始，如果你为axis指定一个负数，则其从后往前数。 这个操作在你想要在某个元素上加入一个batch维度时非常有用。例如有一个形状为[height, width, channels]的图像，通过expand_dims(image, 0)可以使其成为一个只有一张图片的batch，其形状为[1, height, width, channels]。 Ref tf.expand_dims Args: input: 一个张量. axis: 0-D (标量)，指定扩展张量input形状的维度索引，必须位于[-rank(input) - 1, rank(input)]之间。 name: 输出张量的名字。 dim: 0-D (标量)，等价于 axis，将会被废弃。 Returns: 和输入张量input具有相同数据的张量，但其形状多了一个大小为1的维度。 tf.squeeze1234tf.squeeze(input, axis=None, name=None, squeeze_dims=None) 给定一个张量input，该函数返回一个在移除所有大小为1的维度后，与输入张量具有相同数据类型的张量。如果你不想移除所有大小为1的维度，可以通过指定axis来移除部分大小为1的维度。 Ref tf.squeeze Args: input: 一个张量。A Tensor. The input to squeeze. axis: 一个可选的ints列表，默认为[]。如果指定，则只压缩列表中的维度，维度索引从0开始。当压缩大小不是1的维度时会产生错误。必须在 [-rank(input), rank(input))之间。 name: 操作的名字，可选参数。 squeeze_dims: 废弃的参数，现在改为axis。 Returns: 和输入张量input具有相同类型、相同数据的张量，但是有一个或多个大小为1的维度被移除。 tf.transpose123456tf.transpose( a, perm=None, name='transpose', conjugate=False) 对张量a进行转秩操作，根据参数perm重新排列各个维度。 返回的张量的第i个维度对应输入张量的第perm[i]个维度，如果perm参数没有指定，其默认会被设为 (n-1…0)，其中n是输入张量的秩。所以默认情况下，该操作进行一个常规的矩阵转秩操作。如果参数conjugate设置为True，a.dtype 是 complex64 或者 complex128 ，则返回输入张量的共轭转秩。 Ref tf.transpose Args: a: A Tensor. perm: A permutation of the dimensions of a. name: A name for the operation (optional). conjugate: Optional bool. Setting it to True is mathematically equivalent to tf.conj(tf.transpose(input)). Returns:A transposed Tensor. Numpy Compatibility In numpy transposes are memory-efficient constant time operations as they simply return a new view of the same data with adjusted strides. TensorFlow does not support strides, so transpose returns a new tensor with the items permuted. tf.concattf.stacktf.tile特殊矩阵tf.one_hottf.eye运算 Arithmetic Operatorstf.addtf.subtracttf.tensordottf.matmultf.multiply12345tf.multiply( x, y, name=None) Returns x * y element-wise. Args: x: A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128. y: A Tensor. Must have the same type as x. name: A name for the operation (optional). Returns:A Tensor. Has the same type as x. Ref tf.multiply tf.scalar_multf.divtf.dividetf.truedivtf.floordivtf.realdivtf.truncatedivtf.floor_divtf.truncatemodtf.floormodtf.modtf.crosstf.einsum1tf.einsum(equation, *inputs) Ref tf.einsum 图像 Imagestf.imageRef Upgrade to TensorFlow 1.0 Arithmetic Operators]]></content>
      <categories>
        <category>coding</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS10.13.2配置TensorFlow]]></title>
    <url>%2F2018%2F07-08-Mac-ConfigureTensorFlow%2F</url>
    <content type="text"><![CDATA[安装Pythonpython3是通过Homebrew安装的，默认安装的是3.7.0版本，但TensorFlow目前只能在3.4，3.5，3.6上安装，因此首先需要安装3.6版本的python。 首先运行命令1brew unlink python3 然后通过https://github.com/Homebrew/homebrew-core/commits/master/Formula/python.rb找到要需要安装的python版本，我这里选择python:3.6.5_1 bottle，再用如下命令安装1brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb 此时输入1python3 --version 发现得到1Python 3.6.5 则安装成功，/usr/local/Cellar/python中有3.6.5_1和3.7.0两个文件夹。 安装VirtualenvTensorFlow的官方教程推荐使用Virtualenv的方式安装TensorFlow。Virtualenv是一个与其他Python开发相互隔离的虚拟Python环境，它无法干扰同一计算机上的其他Python程序，也不会受其影响。要开始使用TensorFlow，只需要“激活”虚拟环境。总而言之，Virtualenv提供一种安全可靠的机制来安装和运行TensorFlow。 Virtualenvwrapper则是对Virtualenv提供了简易的命令行封装，可以更方便地管理虚拟环境。 首先安装Virtualenv和Virtualenvwrapper：12pip3 install virtualenvpip3 install virtualenvwrapper 然后查找virtualenvwrapper.sh的位置：1which virtualenvwrapper.sh 得到1/usr/local/bin/virtualenvwrapper.sh 安装完成后先设置WORKON_HOME，即环境的存储路径，并且运行source /usr/local/bin/virtualenvwrapper.sh，注意，如果你的virtualenv和virtualenvwrapper安装在Homebrew安装的Python3中，还需要设置VIRTUALENVWRAPPER_PYTHON路径，否则会报No module named &#39;virtualenvwrapper&#39;的错误。123export WORKON_HOME=~/myLibs/pyenvsexport VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3source /usr/local/bin/virtualenvwrapper.sh 把export命令和source命令加入到~/.bash_profile文件中，每次打开终端就无需初始化了。 一些常用的命令12345workon 显示所有的环境名称workon 环境名 进入/切换到该环境deactivate 返回到系统环境mkvirtualenv 环境名 新建环境rmvirtualenv 移除环境 安装TensorFlow安装接下来安装TensorFlow，参考官方安装教程。首先输入下面的命令创建Virtualenv环境：1virtualenv --system-site-packages -p python3 ~/myLibs/pyenvs/tensorflow 然后输入下面命令激活Virtualenv环境：12cd ~/myLibs/pyenvs/tensorflowsource ./bin/activate # If using bash, sh, ksh, or zsh 执行上述source命令后，提示符应该会变成如下内容：1(tensorflow)$ 接下来将TensorFlow及其所需的所有软件包安装到活动Virtualenv环境中：1(tensorflow)$ pip3 install --upgrade tensorflow 稍等片刻，TensorFlow就会安装完毕。 注意：每次在新的shell中使用TensorFlow时，都必须激活Virtualenv环境。 如果Virtualenv环境当前未处于活动状态（即提示符不是(tensorflow) $），需调用以下命令：12cd ~/myLibs/pyenvs/tensorflowsource ./bin/activate 或者，找到activate文件所在的目录即~/myLibs/pyenvs/tensorflow/bin，在当前工作目录下直接执行下面命令也可快速进入虚拟环境，更方便快捷： 1source ~/myLibs/pyenvs/tensorflow/bin/activate 进入虚拟环境后命令提示符将变成如下所示，则表示的tensorflow环境已处于活动状态： 1(tensorflow) $ 当Virtualenv环境处于活动状态时，就可以从该shell运行TensorFlow程序了。 用完TensorFlow后，可以通过发出以下命令来停用此环境：1(tensorflow)$ deactivate 提示符将恢复为默认提示符。 测试下面再运行一个简短的TensorFlow程序来测试其是否正确安装。首先激活从shell中调用Python，如下所示：1$ python3 在Python交互式shell中输入以下几行简短的程序代码：12345&gt;&gt;&gt; # python&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello)) 如果系统输出以下内容，则说明TensorFlow已加被正确安装：1Hello, TensorFlow! 如果发生错误，你就需要继续折腾了。 用PyCharm测试新建PyCharm项目，一般需要做一番配置才可运行TensorFlow项目。 打开Preference，按照如下步骤操作即可。 卸载如果需要卸载TensorFlow，只需移除之前创建的~/myLibs/pyenvs/tensorflow文件夹即可。 参考 在 macOS 上安装 TensorFlow Mac下安装Python虚拟环境Virtualenv No module named ‘virtualenvwrapper’ 为什么pycharm中无法import tensorflow？]]></content>
      <categories>
        <category>tutorials</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3D变换]]></title>
    <url>%2F2018%2F06-10-CG-Transformation3D%2F</url>
    <content type="text"><![CDATA[三维矩阵变换你可以使用一个4×4的矩阵将任何点变换到另一个点。下面的例子中，我们用一个矩阵对点$(x, y, z)$进行变化，产生了一个新的点$(x’, y’, z’)$： \begin{bmatrix} x' & y' & z' & 1 \end{bmatrix}= \begin{bmatrix} x & y & z & 1 \end{bmatrix} \begin{bmatrix} M_{11} & M_{12} & M_{13} & M_{14} \\ M_{21} & M_{22} & M_{23} & M_{24} \\ M_{31} & M_{32} & M_{33} & M_{34} \\ M_{41} & M_{42} & M_{43} & M_{44} \end{bmatrix}最常用的变换包括：平移（translation），旋转（rotation）和缩放（scaling）。你可以将这些变换合并起来，组成一个矩阵，同时进行几种变换。 平移将一个点$(x, y, z)$平移到另一个点$(x’, y’, z’)$： \begin{bmatrix} x' & y' & z' & 1 \end{bmatrix}= \begin{bmatrix} x & y & z & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ T_x & T_y & T_z & 1 \end{bmatrix}旋转将一个点$(x, y, z)$沿$x-$轴进行旋转，得到了一个新的点$(x’, y’, z’)$： \begin{bmatrix} x' & y' & z' & 1 \end{bmatrix}= \begin{bmatrix} x & y & z & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & \cos\theta & \sin\theta & 0 \\ 0 & -\sin\theta & \cos\theta & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}沿$y-$轴进行旋转： \begin{bmatrix} x' & y' & z' & 1 \end{bmatrix}= \begin{bmatrix} x & y & z & 1 \end{bmatrix} \begin{bmatrix} \cos\theta & 0 & \sin\theta & 0 \\ 0 & 1 & 0 & 0 \\ -\sin\theta & 0 & \cos\theta & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}沿$z-$轴进行旋转： \begin{bmatrix} x' & y' & z' & 1 \end{bmatrix}= \begin{bmatrix} x & y & z & 1 \end{bmatrix} \begin{bmatrix} \cos\theta & \sin\theta & 0 & 0 \\ -\sin\theta & \cos\theta & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}缩放 \begin{bmatrix} x' & y' & z' & 1 \end{bmatrix}= \begin{bmatrix} x & y & z & 1 \end{bmatrix} \begin{bmatrix} s_x & 0 & 0 & 0 \\ 0 & s_y & 0 & 0 \\ 0 & 0 & s_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}矩阵旋转欧拉角旋转偏航角：yaw偏航角yaw是绕$z$轴逆时针旋转$\alpha$，对应的旋转矩阵是 R_z(\alpha)= \begin{bmatrix} \cos\theta & -\sin\theta & 0\\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}俯仰角: pitch俯仰角pitch是绕$y$轴逆时针旋转$\beta$，对应的旋转矩阵是 R_y(\beta)= \begin{bmatrix} \cos\theta & 0 & \sin\theta \\ 0 & 1 & 0 \\ -\sin\theta & 0 & \cos\theta \end{bmatrix}滚转角: roll滚转角roll是绕$x$轴逆时针旋转$\gamma$，对应的旋转矩阵是 R_x(\gamma)= \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta \end{bmatrix}刚体变换这三个旋转矩阵合起来便可以表达一个刚体变换[[3]][Yaw, pitch, and roll rotations] R(\alpha,\beta,\gamma)=R_z(\alpha)R_y(\beta)R_x(\gamma)四元数旋转四元数、欧拉角、旋转矩阵的优点和缺点 矩阵旋转 优点： 旋转轴可以是任意向量； 缺点： 旋转其实只需要知道一个向量+一个角度，一共4个值的信息，但矩阵法却使用了16个元素； 而且在做乘法操作时也会增加计算量，造成了空间和时间上的一些浪费； 欧拉旋转 优点： 很容易理解，形象直观； 表示更方便，只需要3个值（分别对应x、y、z轴的旋转角度）；但按我的理解，它还是转换到了3个3*3的矩阵做变换，效率不如四元数； 缺点： 之前提到过这种方法是要按照一个固定的坐标轴的顺序旋转的，因此不同的顺序会造成不同的结果； 会造成万向节锁（Gimbal Lock）的现象。这种现象的发生就是由于上述固定坐标轴旋转顺序造成的。理论上，欧拉旋转可以靠这种顺序让一个物体指到任何一个想要的方向，但如果在旋转中不幸让某些坐标轴重合了就会发生万向节锁，这时就会丢失一个方向上的旋转能力，也就是说在这种状态下我们无论怎么旋转（当然还是要原先的顺序）都不可能得到某些想要的旋转效果，除非我们打破原先的旋转顺序或者同时旋转3个坐标轴。这里有个视频可以直观的理解下； 由于万向节锁的存在，欧拉旋转无法实现球面平滑插值。 四元数旋转 优点： 可以避免万向节锁现象； 只需要一个4维的四元数就可以执行绕任意过原点的向量的旋转，方便快捷，在某些实现下比旋转矩阵效率更高； 可以提供平滑插值。 缺点： 比欧拉旋转稍微复杂了一点点，因为多了一个维度； 理解更困难，不直观； 参考 矩阵与变换 图形学复习要点 Yaw, pitch, and roll rotations Creating a rotation matrix with pitch, yaw, roll using Eigen]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS项目属性配置简介]]></title>
    <url>%2F2018%2F06-09-VS-ProjectConfiguration%2F</url>
    <content type="text"><![CDATA[转载自 Visual Studio项目属性的一些配置项的总结 VS项目的文件组织方式VC6.0之后的VC (VS)系列使用解决方案(Solution)来替代原来的工作空间，用于组织和管理多个相关的项目(Project)。VS中的每个管理器(解决方案或项目)都会对应一个总的文件夹，这个管理器文件夹下存放本管理器的配置文件以及子管理器。以C#项目为例，解决方案管理器总文件夹下包含解决方案配置文件*.sln和项目子管理器文件夹，而项目子管理器文件夹下包含C#源文件*.cs、项目配置文件*.csproj、Properties属性文件夹、obj文件夹和bin文件夹。其中obj和bin文件夹下各包含debug和release两个文件夹。obj文件夹下存放中间编译结果，bin文件夹下存放最终生成的exe或dll文件。 常用项目属性和系统默认配置变量通常程序开发步骤包括编辑程序、编译程序、装配链接程序、程序调试测试、安装部署。表1给出了程序开发过程中常用的系统变量名和意义： 系统变量 变量含义 $(ConfigurationName) 配置名，通常是debug或release $(IntDir) 编译器使用的中间目录，产出obj文件 $(OutDir) 链接器使用的输出目录 $(ProjectDir) 项目目录 $(ProjectName) 项目名 $(SolutionDir) 解决方案目录 $(TargetDir) 目标输出文件所在的目录，产生exe文件 $(TargetExt) 目标输出的扩展名 $(TargetFileName) 目标输出文件名，包括扩展名 $(TargetName) 目标输出名，不包括扩展名 $(TargetPath) 目标输出文件的全路径名 常规—&gt;输出目录项目属性的“常规”栏目中“输出目录(OutDir)”的作用是给$(OutDir)系统变量赋值，其默认属性值为$(SolutionDir)$(ConfigurationName)，$(SolutionDir)表示解决方案目录，$(ConfigurationName)的值为debug或release。启动编译后会在解决方案文件夹下建立debug文件夹。 也就是说默认情况下的输出目录是在解决方案目录下的debug或release文件夹下，当然这是针对C++型项目而言，C#型项目不一样。 常规—&gt;中间目录项目属性的“常规”栏目中，“中间目录(IntDir)”的作用是存储链接器所需的输入文件，默认属性为(ProjectDir)(ConfigurationName)，编译后会在MyProject项目文件夹下建立一个debug文件夹，并在该文件夹下生成MyProject.obj二进制文件。 接器—&gt;常规—&gt;输出文件项目属性的“链接器”栏目下，“常规”选项下，“输出文件”默认属性为$(OutDir)$(ProjectName).exe，其中$(OutDir)指的是输出目录，启动链接后，在输出目录下生成MyProject.exe文件。$(TargetDir)的值是由“输出文件”指定的目录决定的。也就是链接器最后生成的*.exe文件所在位置。 “输出目录”和“输出文件”两个属性对应的目录默认情况下是一样的，这样用着方便。如果两个不一样，则链接器所需的*.ilk和*.pdb等中间文件在“输出目录”，而最终生成的exe文件在“输出文件”属性设置的目录中。 调试—&gt;命令项目的“输出目录”属性值决定着系统变量$(OutDir)的值，而项目的“输出文件”的属性值决定着$(TargetDir)和$(TargetPath)的值。程序调试时，系统变量$(OutDir)的值是最先确定的，而$(TargetDir)和$(TargetPath)的值是在链接器生成exe文件后才确定的。 “调试”栏目中的“命令(Command)”属性项，这个属性表示启动调试器时执行的exe文件“全路径名+文件名”，默认为链接器生成的$(TargetPath)目录，当然你也可以手动更改“命令”属性的值。 单击调试按钮(VS中的那个小三角形按钮)，VS会起动图中所示目录下的exe文件。一般来说“链接器”—&gt;“输出文件”与“调试”—&gt;“命令”中的文件位置、名称是相同，以表示链接器生成的文件和调试时使用的文件一样。一言以蔽之，①&lt;“调试”—&gt;“命令”&gt;、②TargetPath、③输出文件，④输出目录(OutDir) 默认情况下是处于同一个目录，并呈现出前一个紧密依赖于后一个的关系。 调试—&gt;工作目录工作目录(WorkingDirection )与执行目录(Command)可以不同，它是程序工作运行过程中默认读取的目录，调试时是将工作目录下的文件作为附加参数添加到执行目录的exe文件中去调试执行。“调试”栏目中的“工作目录”项，默认属性值为$(ProjectDir)，即工程配置文件MyProject.vcproj所在目录，调试过程中它会随着OpenFileDialog、SaveFileDialog等对象所确定的目录而改变。对于静态链接的lib和dll库文件可以放入exe所在的执行目录，而动态加载的dll一般放在工作目录，比如插件就放在工作目录。此外，程序运行过程中生成一个txt文本文件或读取一些配置文件，如果在创建或读取过程中未指定绝对路径，只指定其文件名，那么默认的路径就是工作目录。 VS中工作目录是用于调试过程，只有在调试时，VS才会把项目配置属性中的工作目录设置为执行进程的工作目录，然后再启动对应的exe程序。如果用户选择直接双击一个exe程度启动新进程，VS会自动把exe文件所在的目录设置为新进程的工作目录。因此，在软件部署发布的时候，需把工作目录内的文件拷贝到exe所在的执行目录内，否则就会运行出错。 链接器—&gt;输入—&gt;附加依赖项“链接器”栏目下，“输入”选项下，“附加依赖项”属性。此项是设置程序链接时使用的静态库的名称。相当于链接已经编译好了的“代码”。由此我们可以简单的认为这些库就相当于我们自己写的源文件，只不过这些库是编译好了的源文件而已。 参考 Visual Studio项目属性的一些配置项的总结]]></content>
      <categories>
        <category>tools</category>
        <category>IDEs</category>
      </categories>
      <tags>
        <tag>cpp</tag>
        <tag>vs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Triangulation of Simple Polygons]]></title>
    <url>%2F2018%2F06-09-CG-PolygonsTriangulation%2F</url>
    <content type="text"><![CDATA[Triangulation of Simple Polygons Ben Discoe, notes from 2001.02.11, updated through 2009.01 I needed some code for tessellating polygons, which could be integrated into the VTP libraries, with the following desirable traits: portable no dependencies on large external libraries free (no restrictive license) in C/C++ easy to use preferably, handles ‘holes’ in the polygon Here are each of the options i found. OpenGL contains functionality (in the glu library) which is capable of tessellation problem: requires a complicated system of registering 6 callback functions problem: not easy to use, no example code in Red Book problem: has large external dependency (OpenGL, with a valid context) Extracting the gluTessellate functionality from the SGI OpenGL® Sample Implementation one developer has done this, and says “It was very easy”, which seems rather surprising! Valéry Moya wrote in July 2009: “I wrote my own glu tesselator. Grabbing the tessellation from the callbacks isn’t all that hard and doesn’t require too much code to get working. (In fact, I got it working with just 2 of the callbacks registered: GLU_TESS_BEGIN and GLU_TESS_VERTEX.) In my trial, I was able to get tessellations from lists verts that make up the contour of the polygon I wanted to tessellate (with no overlapping edged, but allowing holes defined as counterclockwise list of verts). The “hard” part is that you need to handle getting the triangulated verts as triangles, triangle fans and triangle strip order but this not as bad as it sounds to keep track of. “ Val’s code: glu_tesselation.c Fast Polygon Triangulation based on Seidel’s Algorithm (1995) has C code available problem: small Unix dependency (), calls nonstandard stdlib function “log2” problem: in the function add_segment(), a variable is used without being initialized - i.e. it’s questionable code problem? the README written in 1995 says “for non-commercial use only” the author Atul Narkhede, who went from CMU to SGI, refers to the code on a more current website as specifically “Public Domain”, so the latest status is apparently non-restrictive 2008, received an email from another guy trying the code, and he found the code was too buggy Efficient Polygon Triangulation, by John W. Ratcliff on flipcode C++ code, very small and easy to use! Uses STL, but this dependency was easy to remove Tested it on my own data, it worked very well. Add it to vtdata. Only one problem: doesn’t handle holes (and doesn’t claim to) GPC does clipping and boolean operations in addition to triangulation problem: reportedly “uses a simple trapezoidal decomposition that introduces lots of t-junctions” problem: prohibitive license restricts usage “Triangle” by Jonathan Shewchuk Can produce Delaunay triangulations, which apparently have desirable traits for some purposes, but are a bit overkill for the general case. For our simpler case it can produce “Constrained Delaunay”, which means no extraneous triangles. Supports holes! Source is free and portable, only one source file (triangle.c) which makes it easy to integrate with. One small drawback: You can’t just tell it which segments are the edges of your hole. Instead, you must supply some point that lies within the hole. Triangle triangulate the hole, then does some kind of “flood fill” to empty it. It’s inefficient, but what’s worse is it requires the caller to compute a point-in-polygon for the hole. This is a non-trivial algorithm for an arbitrary complex polygon. In 2008.01, i adapted the Triangle code with a wrapper to call it from vtlib. It works quite well, for a very large number of polygons. However, there are some (rare) cases of degenerate geometry where it will crash. In particular: It does not like duplicate vertices or duplicate edges. ‘Duplicate’ in this case is relative to numeric precision: For a building 10-100 meters in size, two vertices within 8cm of each other, defining a very short edge, can cause Triangle to crash. It does not like it when a hole (inner ring) in the polygon has a vertex in the same location as one in the outer ring (crash). TerraGear a Free library which contains a cleaned-up version of the Narkhede implementation of the Seidel algorithm (above)? No. Perhaps it did back in 2001, but as of 2007, it contains code to call “Triangle”. It is actually useful as good example code of how to call Triangle. Panda3d A huge, free software stack used by Disney’s VR group, which includes triangulation adapted from “Narkhede A. and Manocha D., Fast polygon triangulation algorithm based on Seidel’s Algorithm” The triangulation is buried deep in the C++ part of its code, underneath a massive mess of python, tcl, cross-platform abstraction, and custom build tools used to build custom build tools! On 2008.01.30, i lifted the ‘Triangulate’ module out of Panda, made it standalone, and ran the provided test (test_tri.cxx). It crashed, with a negative array index. I also spent 3 hours trying to build Panda itself (with most dependencies including Python disabled.) No luck, it is just too complex. There were some implications that the Panda version of Narkhede-Manocha might be cleaned up or fixed. However, since it crashes (for me) on a simple test outside of Panda, this is not encouraging. From: Sébastien Berthet [mailto:sbrt@yahoo.fr] Sent: Friday, February 01, 2008 12:56 AM I assume that you used the last version on CVS (commited 3 weeks ago), right ? http://panda3d.cvs.sourceforge.net/panda3d/panda/src/mathutil/triangulator.cxx?revision=1.5&amp;view=markup The thread on the forum mentions a few fixes… Poly2Tri (Liang / Kittelman, 2005) “Subdivision using monotone polygons and a sweep line algorithm, O(n log n) time and O(n) storage” Supports holes, makes a lot of very skinny triangles. It compares the speed of Poly2Tri vs. some other triangulation implementations, and it claims to be faster, although the triangles produced are very skinny slivers. Another open source implementation is available at PolygonTriangulator.cxx (google it), by Thomas Kittelmann, which says it is adapted from the implementation by Wu Liang (2005), which appears to be part of a project called “Atlas LXR” poly2tri (Mason Green?, 2009-) Says it is “Based on the paper Sweep-line algorithm for constrained Delaunay triangulation by V. Domiter and and B. Zalik” Handles holes. BSD license. Lives on google-code. I evaluated it on 2011-07-19. It managed to tessellate some input which Shewchuk’s Triangle couldn’t not. However, it still crashed on some input, like the polygon to the right. Notice how, once again, one of the polygon’s holes (inner rings) exactly touches the outer ring. The algorithm seems to regard this as degenerate (and does not even detect it to avoid crashing) but it is unfortunately a commonly encountered shape. JTS The Java Topology Suite (JTS) as of 2011 will apparently do reasonable quality triangulation and support holes, see Polygon Triangulation via Ear-Clipping with Delaunay Refinement. JTS is a very large package (as is its C++ sibling, GEOS) but if you are already using it, then this might be an option. OthersTriangulating a Simple Polygon in Linear Time (Chazelle 1991)M. Held (2001): “FIST: Fast Industrial-Strength Triangulation of Polygons”. FIST is not open source nor allows unrestricted use. Reference Triangulation of Simple Polygons]]></content>
      <categories>
        <category>repost</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>triangulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提取HTML中的文本]]></title>
    <url>%2F2018%2F06-08-Py-ExtractTextFromHTML%2F</url>
    <content type="text"><![CDATA[提出HTML中的文本使用NTLK，参考自Shatu的代码如下:1234567import nltkfrom urllib import urlopenurl = "http://news.bbc.co.uk/2/hi/health/2284783.stm"html = urlopen(url).read()raw = nltk.clean_html(html)print(raw) 将HTML文件转化为Markdown参考aaronsw/html2text/html2text.py 参考 Extracting text from HTML file using Python aaronsw/html2text]]></content>
      <categories>
        <category>tools</category>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>python</tag>
        <tag>html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[径向基函数]]></title>
    <url>%2F2018%2F05-31-Alg-RBF%2F</url>
    <content type="text"><![CDATA[径向基函数简介径向基(Radial Basis Function, RBF)函数是一个取值仅仅依赖于离原点距离的实值函数，也就是$\phi(x) = \phi(||x||)$,或者还可以是到任意一点$c$的距离，$c$点即为中心点，也就是$\phi(x,c) = \phi(||x-c||)$。任意一个满足$\phi(x) = \phi(||x||)$特性的函数$\phi$都叫做径向量函数，标准的一般使用欧氏距离，尽管其他距离函数也是可以的。 常用RBF函数常用径向基函数包括（$r=||x-x_i||$）： Gaussian: \phi (r)=e^{-(\varepsilon r)^{2}} Multiquadric: \phi(r) = \sqrt{(1+(\epsilon r)^2)} Inverse Quadraic: \phi(r) = \frac{1}{1+(\epsilon r)^2} Inverse Multiquadric: \phi(r) = \frac{1}{\sqrt{(1+(\epsilon r)^2)}} Polyharmonic spline: \phi (r)=r^{k},\;k=1,3,5,\dots \phi (r)=r^{k}\ln{(r)},\;k=2,4,6,\dots Thin plate spline (a special polyharmonic spline): \phi (r)=r^{2}\ln(r)函数逼近RBF函数一般用来逼近其他复杂的函数，逼近函数$y(\mathbf x)$用$N$个RBF的加权和来表示，每个RBF有不同的中心$\mathbf x_i$，其一般形式如下： y(\mathbf x) = \sum_{i=1}^N w_i \phi( || \mathbf x - \mathbf x_i || )理论上来说，只要$N$足够大，$y(\mathbf x)$能够以任意的精度逼近在某个区间的任意连续函数。 RBF网络函数 y(\mathbf x) = \sum_{i=1}^N w_i \phi( || \mathbf x - \mathbf x_i || )还可以通过一个简单的单层人工神经网络来表达，即RBF网络，径向基函数作为网络的激活函数。 因为$y(\mathbf x)$相对于$w_i$是可微的，所有这些权重可以通过神经网络中标准的迭代法来学习得到。 通过这种方式使用径向基函数在收敛后可以在拟合集中得到一个合理的结果，但是在拟合集外面表现会很差，除非添加一个垂直于径向基函数的多项式项。 Reference WIKIPEDIA-Radial basis function 径向基函数(RBF)]]></content>
      <categories>
        <category>algorithm</category>
        <category>math</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[颜色插值]]></title>
    <url>%2F2018%2F05-10-ColorInterpolate%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556Eigen::Vector3f colorMapping_B2R(float value, float min, float max)&#123; // from blue to red float mid1 = (max + 3 * min) / 4.0; float mid2 = (max + min) / 2.0; float mid3 = (3 * max + min) / 4.0; Eigen::Vector3f color; //Eigen::Vector3f blue = Eigen::Vector3f(0.0, 0.0, 1.0); //Eigen::Vector3f cyan = Eigen::Vector3f(0.0, 1.0, 1.0); //Eigen::Vector3f green = Eigen::Vector3f(0.0, 1.0, 0.0); //Eigen::Vector3f yellow = Eigen::Vector3f(1.0, 1.0, 0.0); //Eigen::Vector3f red = Eigen::Vector3f(1.0, 0.0, 0.0); // blue to cyan if (value &gt;= min &amp;&amp; value &lt; mid1) &#123; float temp = (value - min) / (mid1 - min); color = Eigen::Vector3f(0.0, temp, 1.0); &#125; // cyan to green else if (value &gt;= mid1 &amp;&amp; value &lt; mid2) &#123; float temp = (value - mid1) / (mid2 - mid1); color = Eigen::Vector3f(0.0, 1.0, 1.0 - temp); &#125; // green to yellow else if (value &gt;= mid2 &amp;&amp; value &lt; mid3) &#123; float temp = (value - mid2) / (mid3 - mid2); color = Eigen::Vector3f(temp, 1.0, 0.0); &#125; // yellow to red else if (value &gt;= mid3 &amp;&amp; value &lt;= max) &#123; float temp = (value - mid3) / (max - mid3); color = Eigen::Vector3f(1.0, 1.0 - temp, 0.0); &#125; // handle color beyond red else if (value &gt; max) &#123; color = Eigen::Vector3f(1.0, 0.0, 0.0); &#125; // handle color beyond blue else if (value &lt; min) &#123; color = Eigen::Vector3f(0.0, 0.0, 1.0); &#125; return color;&#125; Reference Grayscale to Red-Green-Blue (MATLAB Jet) color scale]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for vcpkg]]></title>
    <url>%2F2018%2F05-08-Tips4vcpkg%2F</url>
    <content type="text"><![CDATA[概述vcpkg是微软开发的在Windows, Linux和MacOS平台管理C/C++库的开源工具。 快速开始要求使用vcpkg需满足如下条件： Windows 10, 8.1, 7, Linux, or MacOS Visual Studio 2017 or Visual Studio 2015 Update 3 (on Windows) Git CMake 3.10.2 (optional) 安装vcpkg12345&gt; git clone https://github.com/Microsoft/vcpkg&gt; cd vcpkgPS&gt; .\bootstrap-vcpkg.batUbuntu:~/$ ./bootstrap-vcpkg.sh 为了让计算机的所有用户都可以使用vcpkg，运行如下命令（首次运行需管理员权限）：12PS&gt; .\vcpkg integrate installUbuntu:~/$ ./vcpkg integrate install 安装库通过如下命令便可以安装库：12PS&gt; .\vcpkg install sdl2 curlUbuntu:~/$ ./vcpkg install sdl2 curl 如果你在安装库时下载速度非常慢甚至下载失败，可以拷贝下载链接自行下载好库的压缩包，然后放在downloads文件夹，这样vcpkg便直接使用下载好的库来编译安装。 安装不同配置的库123.\vcpkg install ceres:x32-windows.\vcpkg install ceres:x64-windows.\vcpkg install ceres:x64-windows-static 选择库的模块安装对于有些库，默认可能不是所有的依赖都安装，如ceres-solver，默认不支持suitesparse，cxsparse，此时可以通过下面命令重新安装 1.\vcpkg install ceres[suitesparse,cxsparse]:x64-windows --recurse 其中--recurse表示可以卸载之前的库。更多install参数可以通过命令.\vcpkg help install查看。 再比如支持cuda的opencv版本，可以通过下面命令来安装。1.\vcpkg install opencv[cuda]:x64-windows 卸载vcpkg直接删除vcpkg的文件夹即可。 vcpkg命令 vcpkg help： 帮助 vcpkg search： 搜索库 vcpkg install： 安装库 对于Windows平台 .\vcpkg install openmesh:x86-windows：安装32位库 .\vcpkg install openmesh:x64-windows：安装64位库 .\vcpkg install openmesh:x64-windows-static 安装64位静态库 vcpkg list：列出所有已经安装的库 vcpkg upgrade：列出所有可以升级的库，如果需要升级，需额外添加--no-dry-run命令 vcpkg update vcpkg remove：移除某个已经安装的库，如果需要移除依赖该库的其他库，添加--recurse命令 vcpkg文件夹构成 buildtrees — contains subfolders of sources from which each library is built docs — documentation and examples downloads — cached copies of any downloaded tools or sources. vcpkg searches here first when you run the install command installed— Contains the headers and binaries for each installed library. When you integrate with Visual Studio, you are essentially telling it add this folder to its search paths packages — Internal folder for staging between installs ports — Files that describe each library in the catalog, its version, and where to download it. You can add your own ports if needed scripts — Scripts (cmake, powershell) used by vcpkg toolsrc — C++ source code for vcpkg and related components triplets — Contains the settings for each supported target platform (for example, x86-windows or x64-uwp) vcpkg更新在vcpkg根目录下的ports文件夹中可以看到当前版本包含的所有库，但由于vcpkg项目正在活跃开发中，有时候有些库在你当前的版本中并没有加入，这时可以考虑更新vcpkg。首先拉取vcpkg的远程仓库，更新本地仓库：1234git fetch origin master:temp // 从远程的origin仓库的master分支下载到本地并新建一个分支tempgit diff temp // 比较本地的仓库和远程仓库的区别git merge temp // 合并temp分支到master分支git branch -d temp // 如果不想要temp分支了，可以删除此分支 然后重新编译生成vcpkg.exe工具12PS&gt; .\bootstrap-vcpkg.batLinux:~/$ ./bootstrap-vcpkg.sh 然后可以通过命令.\vcpkg update .\vcpkg upgrade更新已经安装好的库。再通过install命令安装新的库。 vcpkg更改设置默认安装库类型对于Windows平台，vcpkg默认安装32位库，如果想要设置为默认安装64位库，在环境变量中加上VCPKG_DEFAULT_TRIPLET=x64-windows即可。 默认使用库类型如果需要指定使用64位静态库，可以在cmake命令中加入-DVCPKG_TARGET_TRIPLET=x64-windows-static 实现，或者在CMakeLists.txt文件的project命令之前加入 1set(VCPKG_TARGET_TRIPLET "x64-windows-static") vcpkg使用CMake在CMake中使用通过vcpkg安装的库的最佳方式是通过工具链文件 (toolchain file) scripts/buildsystems/vcpkg.cmake，让安装的库通过find_package()被发现。 要使用这个文件，只需通过命令-DCMAKE_TOOLCHAIN_FILE=[vcpkg root]/scripts/buildsystems/vcpkg.cmake将其加入CMake命令行中即可。例如12cmake .. -DCMAKE_TOOLCHAIN_FILE=vcpkg/scripts/buildsystems/vcpkg.cmake (Linux/MacOS)cmake .. -DCMAKE_TOOLCHAIN_FILE=vcpkg\scripts\buildsystems\vcpkg.cmake (Windows) 再比如，如果要用VS2017编译器，输入下面命令即可：1cmake .. -DCMAKE_TOOLCHAIN_FILE=D:\vcpkg\scripts\buildsystems\vcpkg.cmake -G "Visual Studio 15 2017 Win64" 还有一种方法，直接在CMakeLists.txt文件中指定CMAKE_TOOLCHAIN_FILE，即12set(CMAKE_TOOLCHAIN_FILE "D:\vcpkg\scripts\buildsystems\vcpkg.cmake")project(PROJECT_NAME) 这里需要注意的是，设置CMAKE_TOOLCHAIN_FILE要在project()命令之前。另外多说一句，类似CMAKE_TOOLCHAIN_FILE, CMAKE_SYSTEM_NAME, CMAKE_C_COMPILER等这些变量都要在project()命令之前设定，不然CMake仍然会按照默认的设置来。 VisualStudio在VS中，所有已经安装的库都被VS项目自动包含（通过前面提到的vcpkg integrate install命令实现），无需配置便可直接使用。 CLion在CLion中的配置如下File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; CMake，在CMake Options中添加-DCMAKE_TOOLCHAIN_FILE=[vcpkg root]/scripts/buildsystems/vcpkg.cmake 参考 vcpkg-github vcpkg-docs Vcpkg: a tool to acquire and build C++ open source libraries on Windows CMAKE_TOOLCHAIN_FILE only recognized on command line]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码的多平台支持]]></title>
    <url>%2F2018%2F05-03-Cpp-CrossPlatform%2F</url>
    <content type="text"><![CDATA[c++123456789101112131415161718192021222324252627#ifdef _WIN32 //define something for Windows (32-bit and 64-bit, this part is common) #ifdef _WIN64 //define something for Windows (64-bit only) #else //define something for Windows (32-bit only) #endif#elif __APPLE__ #include "TargetConditionals.h" #if TARGET_IPHONE_SIMULATOR // iOS Simulator #elif TARGET_OS_IPHONE // iOS device #elif TARGET_OS_MAC // Other kinds of Mac OS #else # error "Unknown Apple platform" #endif#elif __linux__ // linux#elif __unix__ // all unices not caught above // Unix#elif defined(_POSIX_VERSION) // POSIX#else# error "Unknown compiler"#endif python判断系统信息的代码 123import platformprint(platform.system())print(platform.release()) 123456789101112from sys import platform as _platformif _platform == "linux" or _platform == "linux2": # linuxelif _platform == "darwin": # MAC OS Xelif _platform == "win32": # Windowselif _platform == "win64": # Windows 64-bitelse # others cmake1234567if(WIN32) aux_source_directory(os/win SOURCES)elseif(APPLE) aux_source_directory(os/mac SOURCES)else(UNIX) aux_source_directory(os/linux SOURCES)endif(WIN32) qmake123456789macx &#123;# mac only&#125;unix:!macx&#123;# linux only&#125;win32&#123;&#125; Reference How to detect reliably Mac OS X, iOS, Linux, Windows in C preprocessor? 编译系统对跨平台代码的支持 Adding Libraries to Projects Python: What OS am I running on?]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake语言]]></title>
    <url>%2F2018%2F04-15-CMakeLanguage%2F</url>
    <content type="text"><![CDATA[翻译自cmake-language. 注意：翻译不一定准确，内容仅供参考! 组织(Organization)CMake输入文件是用CMake Language写的源文件，包括名叫CMakeLists.txt的文件和以.cmake作为扩展名的文件。 一个项目中的CMake源文件被组织成三部分： Directories (CMakeLists.txt), Scripts (&lt;script&gt;.cmake), Modules (&lt;module&gt;.cmake). 目录(Directories)当CMake处理一个项目的源代码树时，入口点是最上层源目录里的CMakeLists.txt文件，这个文件可能包含整个构建规范(build specification)，或者通过使用add_subdirectory()命令来添加需要构建的子目录，每个通过该命令添加的子目录中也必须含有一个CMakeLists.txt文件来作为该路径的入口点。对于CMakeLists.txt文件处理的每一个源目录都会在构建树(build tree)中生成一个对应的目录，来作为默认的工作和输出目录。 脚本(Scripts)一个单独的&lt;script&gt;.cmake源文件可以在script模式下通过使用cmake(1))命令行工具中的-P选项来被处理，Script模式只是简单地运行该源文件中用CMake Language写的命令，不会生成一个构建系统。它不允许包含有定义构建目标或行为的CMake命令。 模块(Modules)在Directories或者Scripts中的CMake Language代码可以在包含环境中使用include()命令来载入一个&lt;script&gt;.cmake源文件，可以通过查看cmake-modules(7))来了解CMake包含的所有模块。项目源代码树可能也提供它们自己的模块，并在CMAKE_MODULE_PATH变量中指定它们的位置。 语法(Syntax)编码(Encoding)为了最大化在所有支持平台的可移植性，一个CMake语言的源文件由7-bit ASCII文本写成，换行可以被编码成\n或\r\n，但在读入输入文件时会被转化为\n。 注意实现是8-bit clean，所以源文件在系统API支持UTF-8编码的平台上可能会使用UTF-8编码。此外，CMake版本在 3.2及以上支持在Windows上使用UTF-8编码源文件（使用UTF-16调用系统API）。CMake版本在3.0及以上允许在源文件中使用UTF-8自己顺序标记(UTF-8 Byte-Order Mark)。 源文件(Source Files)一个CMake语言的源文件由零个或更多的由换行和可选的空格分隔开的命令调用(Command Invocations)和注释(Comments)组成：123456file ::= file_element*file_element ::= command_invocation line_ending | (bracket_comment|space)* line_endingline_ending ::= line_comment? newlinespace ::= &lt;match '[ \t]+'&gt;newline ::= &lt;match '\n'&gt; Note that any source file line not inside Command Arguments or a Bracket Comment can end in a Line Comment. 命令调用(Command Invocations)一个命令调用是一个名字后面紧跟封闭括号中的被空格分开的多个参数：123456command_invocation ::= space* identifier space* '(' arguments ')'identifier ::= &lt;match '[A-Za-z_][A-Za-z0-9_]*'&gt;arguments ::= argument? separated_arguments*separated_arguments ::= separation+ argument? | separation* '(' arguments ')'separation ::= space | line_ending 例如：1add_executable(hello world.c) 命令的名字对大小写不敏感。参数中嵌套的圆括号必须对称，每个(或者)作为一个字面上未引用的参数(literal Unquoted Argument)传递给命令调用。这可以在调用if()命令时包围条件使用。例如：1if(FALSE AND (FALSE OR TRUE)) # evaluates to FALSE 注意：CMake版本在3.0之前的命令名至少要是2个字符。CMake版本在2.8.12之前接受在一个未引用的或引用的参数后面紧跟着一个引用的参数Quoted Argument，并不能被任何空格分开。为了兼容，CMake版本高于2.8.12接受这样的代码，但会产生一个警告。 命令参数(Command Arguments)命令调用中有三种形式的参数：1argument ::= bracket_argument | quoted_argument | unquoted_argument Bracket Argument括号参数，受启发于Lua的长括号语法，将内容装入拥有同样长度的开始和结束括号：12345bracket_argument ::= bracket_open bracket_content bracket_closebracket_open ::= '[' '='* '['bracket_content ::= &lt;any text not containing a bracket_close with the same number of '=' as the bracket_open&gt;bracket_close ::= ']' '='* ']' 开始括号首先是一个[，后面跟着0个或多个=，最后再跟着]，与此对应的结束括号也以[开头，后面跟着同样数量的=，然后再跟着]。括号不嵌套，Brackets do not nest. A unique length may always be chosen for the opening and closing brackets to contain closing brackets of other lengths. 括号参数内容包括开闭括号之间的所有文本，除了一个新行紧跟着一个开括号，如果有会被忽略。括号内的内容不会被进行任何计算，包括转义序列或者变量引用。一个括号参数总是作为一个参数传递给命令调用。例如：1234567message([=[This is the first line in a bracket argument with bracket length 1.No \-escape sequences or $&#123;variable&#125; references are evaluated.This is always one argument even though it contains a ; character.The text does not end on a closing bracket of length 0 like ]].It does end in a closing bracket of length 1.]=]) 注意：CMake版本在3.0之前不支持括号参数，它会将开括号当作一个Unquoted Argument的开始。 引号参数(Quoted Argument)一个引号参数包含开闭双引号之间的内容12345quoted_argument ::= '"' quoted_element* '"'quoted_element ::= &lt;any character except '\' or '"'&gt; | escape_sequence | quoted_continuationquoted_continuation ::= '\' newline 引号参数内容包含开闭引号之间的所有内容，转义序列和变量引用都会被计算。一个引号参数总是作为一个参数传递给命令调用。例如：123456message("This is a quoted argument containing multiple lines.This is always one argument even though it contains a ; character.Both \\-escape sequences and $&#123;variable&#125; references are evaluated.The text does not end on an escaped double-quote like \".It does end in an unescaped double quote.") 每一行的最后以奇数个反斜线\结尾会被当作一个续行，后面紧跟的换行符会被忽略：12345message("\This is the first line of a quoted argument. \In fact it is the only line but since it is long \the source code uses line continuation.\") 主要：CMake版本在3.0之前不支持用\续行，会报告一个在引号参数中包含某些行以奇数个\字符结束的错误。 非引号参数(Unquoted Argument)一个非引号参数不会被任何引号语法包围，不能包含任何空格、(、)、#、&quot;或者\，除非被反斜线转义：1234unquoted_argument ::= unquoted_element+ | unquoted_legacyunquoted_element ::= &lt;any character except whitespace or one of '()#"\'&gt; | escape_sequenceunquoted_legacy ::= &lt;see note in text&gt; 非引号参数内容由所有连续或转义的字符组成，转义序列和变量引用会被计算。得到的值会像Lists划分元素一样划分，每一个非空的元素作为一个参数传递给命令调用。所以非引用参数可以作为0个或多个参数传递给命令调用。例如：12345678foreach(arg NoSpace Escaped\ Space This;Divides;Into;Five;Arguments Escaped\;Semicolon ) message("$&#123;arg&#125;")endforeach() 注意：为了支持传统的CMake代码，非引用参数也可能包含双引号包围的字符串(&quot;...&quot;，可能包含水平空格)和make-style变量引用(${MAKEVAR})。非转义的双引号必须平衡，可能不会在非引号参数的开始出现，此时将作为内容的一部分对待。例如，非引用参数-Da=&quot;b c&quot;, -Da=$(v)和 a&quot; &quot;b&quot;c&quot;d都按字面值解析，可以作为引号参数分别写作&quot;-Da=\&quot;b c\&quot;&quot;, &quot;-Da=$(v)&quot;, and &quot;a\&quot; \&quot;b\&quot;c\&quot;d&quot;。Make-style引用被当作内容的一部分，不会经过变量展开。它们被当作一个参数的一部分(而不是作为独立的$、(、MAKEVAR和)参数)。上面的unquoted_legacy表示这样的参数。现在不推荐使用传统的非引用参数，应该用引用参数或者括号参数来表示内容。 转义序列(Escape Sequences)一个转义序列是\后面跟着一个字符：1234escape_sequence ::= escape_identity | escape_encoded | escape_semicolonescape_identity ::= '\' &lt;match '[^A-Za-z0-9;]'&gt;escape_encoded ::= '\t' | '\r' | '\n'escape_semicolon ::= '\;' 一个\跟着一个非字母数字字符简单地编码了语义字符而不将其解释为语法。\t、 \r或则\n分别编码了制表符、回车符和换行符。变量引用外面的\;编码了其本身，但可能被用于非引用参数来编码为;且不划分其参数值。变量引用中的\;编码了;字符。 变量引用(Variable References)一个变量引用形如${variable_name}，在一个引用参数或非引用参数中被计算。一个变量引用会被替换为变量的值，或者如果变量未定义则表示一个空字符串。变量引用可以嵌套，从内到外被计算，如 ${outer_${inner_variable}_variable}。变量引用可以包含字母数字字符、/_.+-字符、转义序列。嵌套的引用可以用于计算任何名字的变量。环境变量引用形如$ENV{VAR}，跟普通变量引用一样在上下文中被求值。 注释(Comments)注释从一个#字符开始，但#不能位于括号参数、引号参数或非引号参数中或者作为非引号参数的一部分的转义字符\中。注释有两种类型：括号注释和行注释。 括号注释(Bracket Comment)如果#后面紧跟着一个括号参数，则括号中包含的内容构成一个括号注释：1bracket_comment ::= '#' bracket_argument 例如：123#[[This is a bracket comment.It runs until the close bracket.]]message("First Argument\n" #[[Bracket Comment]] "Second Argument") 注意CMake版本在3.0之前不支持括号注释，这时将#解释为一个行注释的开头。 行注释(Line Comment)如果#后面没有紧跟着一个括号参数，则构成一个行注释，直到行尾结束：12line_comment ::= '#' &lt;any text not starting in a bracket_argument and not containing a newline&gt; 例如：123# This is a line comment.message("First Argument\n" # This is a line comment :) "Second Argument") # This is a line comment. 控制结构(Control Structures)条件块(Conditional Blocks)if()/elseif()/else()/endif()命令使得代码块有条件地执行。 循环(Loops)foreach()/endforeach() and while()/endwhile()命令界定代码块循环执行。在每个这样的快中，break()命令可以终止循环，continue()命令可以让下一次迭代马上进行。 命令定义(Command Definitions)macro()/endmacro()和function()/endfunction()命令界定代码块可以在后面作为命令被调用。 变量(Variables)变量是CMake语言中存储的基本单元，它们的值永远是==字符串类型==，虽然有些命令会将字符串解释为其他类型的值。set()和unset()命令显式地设置或取消设置一个变量，但是其他命令同样也有修改变量的语义。变量名是大小写敏感的，几乎可以由任意文本组成，但是一般推荐使用只包含字母数字外加_和-的变量名。 变量有动态的作用域，每个变量“set”或者“unset”在当前的作用域创建一个绑定： 函数作用域(Function Scope) 通过function()命令创建的命令定义生成一个命令，当被调用时，在新的变量绑定域内处理记录的命令，一个在该域内“set”或者“unset”的变量，只对当前函数和其内部嵌套的调用可见，直到函数返回。 路径作用域(Directory Scope)源文件树中的每一个路径都有自己的变量绑定，在处理一个路径中的CMakeLists.txt文件之前，CMake拷贝在父目录中定义的所有变量绑定，来初始化当前的路径作用域。CMake脚本文件，当被cmake -P命令处理后，会在路径作用域中被绑定。一个变量不在函数内“set”或者“unset”会在当前路径作用域中绑定。 持久缓存(Persistent Cache)CMake存储了一些独立的缓存变量或者缓存条目集合，它们的值在一个项目构建树的多次运行中一直保持存在。缓存条目有独立的绑定域，只有通过显式请求才能修改，如通过set()和unset()命令的CACHE选项。 当计算变量引用的值时，CMake首先搜索函数调用栈以获取绑定，然后回退到当前目录作用域中的绑定。如果找到set绑定，则使用其值。 如果找到unset绑定，或者未找到绑定，则CMake将搜索缓存条目，如果找到缓存条目，则使用其值。 否则，变量引用将计算为空字符串。$CACHE{VAR}语法可用于执行直接查找缓存条目。cmake-variables(7))手册记录了CMake提供的，或者在项目代码设置时对CMake有意义的许多变量。 环境变量(Environment Variables)环境变量就像普通变量一样，但是有如下不同： 作用域(Scope) 环境变量在CMake处理过程中拥有全局作用域，它们永远不会被缓存。 引用(References) 变量引用形式为 $ENV{&lt;variable&gt;}。 初始化(Initialization) CMake环境变量的初始值是调用进程的初始值。 可以使用set()和unset()命令更改值。 这些命令仅影响正在运行的CMake进程，而不影响整个系统环境。 更改的值不会写回调用进程，后续构建或测试进程也不会看到它们。 cmake-env-variables(7)) 手册记录了对CMake具有特殊意义的环境变量。 列表(Lists)尽管CMake中的所有值都存储为字符串，但在某些上下文中可以将字符串视为列表，例如在评估非引号参数期间。 在这种情况下，通过拆分将字符串分成列表元素，不紧跟着不等数量的[和]且不紧跟在\前面的;字符作为拆分符。 序列\;不拆分值但被替换为在结果元素中替换为;。 元素列表通过连接由;分隔的元素并被表示为字符串。 例如，set()命令将多个值作为列表存储到目标变量中：1set(srcs a.c b.c c.c) # sets "srcs" to "a.c;b.c;c.c" 列表适用于简单的用例，例如源文件列表，不应用于复杂的数据处理任务。 大多数列表元素在构造列表的命令时都不会转义;字符，因此展平嵌套的列表：1set(x a "b;c") # sets "x" to "a;b;c", not "a;b\;c" Reference cmake-language]]></content>
      <categories>
        <category>translation</category>
        <category>cmake</category>
      </categories>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake构建系统]]></title>
    <url>%2F2018%2F04-15-CMakeBuildSystem%2F</url>
    <content type="text"><![CDATA[翻译自cmake-buildsystem. 注意：翻译不一定准确，内容仅供参考! 介绍(Introduction)一个基于CMake的构建系统由一套高级的逻辑目标(high-level logical targets)组成，每个目标对应一个可执行文件或库，或者作为一个包含自定义命令的自定义目标。目标之间的依赖关系在构建系统中表示，以确定构建顺序和响应变化的再生成规则。 二进制目标(Binary Targets)可执行文件和库通过使用add_executable()和add_library()命令来定义，产生的二进制文件有适合当前目标平台的前缀(prefixes)、后缀(suffixes)和扩展名(extensions)。二进制目标之间的依赖关系通过命令target_link_libraries()来表示：123add_library(archive archive.cpp zip.cpp lzma.cpp)add_executable(zipapp zipapp.cpp)target_link_libraries(zipapp archive) archive被定义为一个静态库，包含由archive.cpp、zip.cpp和lzma.cpp编译而来的目标。zipapp是一个可执行文件，由编译和链接zipapp.cpp构成。当链接zipapp可执行文件时，archive静态库即被链接。 二进制可执行文件(Binary Executables)命令add_executable()定义一个可执行目标：1add_executable(mytool mytool.cpp) 有些命令，如在构建时生成运行时规则的add_custom_command()命令，可以显式地使用一个EXECUTABLE target作为COMMAND executable。构建系统的规则会保证在尝试运行命令前构建好可执行文件。 二进制库类型(Binary Library Types)普通库(Normal Libraries)默认情况下，add_library()命令定义一个静态库，除非有具体类型指明。类型可以通过下面的命令指定：1add_library(archive SHARED archive.cpp zip.cpp lzma.cpp) 1add_library(archive STATIC archive.cpp zip.cpp lzma.cpp) 通过开启BUILD_SHARED_LIBS变量，能够改变add_library()的行为，使得默认构建共享库。 在构建系统定义的环境中，库是SHARED或者STATIC并没有什么关系，命令依赖和其他的API的工作方式一样，和库的类型无关。MODULE库不一样，一般它不会被链接进去，它不会在target_link_libraries()命令的右侧使用。MODULE库通过运行时技术当作一个插件载入。如果一个库不导出任何非托管的符号(unmanaged symbols)（如Windows resource DLL, C++/CLI DLL），要求这个库不是SHARED库，因为CMake期待SHARED库至少导出一个符号。1add_library(archive MODULE 7z.cpp) Apple框架(Apple Frameworks)一个共享库可能会被标记为FRAMEWORK目标属性来创建一个OS X或者iOS的框架捆绑包(Framework Bundle)，MACOSX_FRAMEWORK_IDENTIFIER设置CFBundleIdentifier键值来唯一地识别一个捆绑包。123456add_library(MyFramework SHARED MyFramework.cpp)set_target_properties(MyFramework PROPERTIES FRAMEWORK TRUE FRAMEWORK_VERSION A MACOSX_FRAMEWORK_IDENTIFIER org.cmake.MyFramework) 目标库(Object Libraries)目标库同样不会被链接，它定义了从给定的源文件编译生成的一些未归档的目标文件(object files)。这些目标文件可以作为其他目标的源输入使用：123add_library(archive OBJECT archive.cpp zip.cpp lzma.cpp)add_library(archiveExtras STATIC $&lt;TARGET_OBJECTS:archive&gt; extras.cpp)add_executable(test_exe $&lt;TARGET_OBJECTS:archive&gt; test.cpp) OBJECT库可能不会在target_link_libraries()的右端项使用，它们也可能不作为TARGET在add_custom_command(TARGET)命令签名中使用。它们可能会被安装，并作为一个INTERFACE库导出。 虽然在调用target_link_libraries()命令时目标库不会被直接命名，但是它们能够通过使用一个Interface Library间接地被链接，但Interface Library的INTERFACE_SOURCES。 虽然在使用add_custom_command(TARGET)命令签名时目标库可能不会被当作TARGET使用，但使用$&lt;TARGET_OBJECTS:objlib&gt;可以让目标列表可以被add_custom_command(OUTPUT)或file(GENERATE)使用。 构建规范和使用要求(Build Specification and Usage Requirements)命令target_include_directories()、target_compile_definitions()和target_compile_options()指定二进制目标的构建规范和使用需求，这些命令分别填入(populate)INCLUDE_DIRECTORIES、COMPILE_DEFINITIONS和COMPILE_OPTIONS目标属性，和/或INTERFACE_INCLUDE_DIRECTORIES、INTERFACE_COMPILE_DEFINITIONS和INTERFACE_COMPILE_OPTIONS目标属性。 每个命令都有PRIVATE、PUBLIC和INTERFACE三种模式。PRIVATE模式只填入(populate)目标属性非INTERFACE_变体(variant)，INTERFACE模式只填入INTERFACE_变体，PUBLIC模式同时填入各自目标属性的变体。每个命令可以通过多次使用这些关键词来调用：1234target_compile_definitions(archive PRIVATE BUILDING_WITH_LZMA INTERFACE USING_ARCHIVE_LIB) 注意：使用要求并非旨在使下游能够方便地使用特定的COMPILE_OPTIONS或COMPILE_DEFINITIONS等。 属性的内容必须是要求，而不仅仅是建议或方便。 有关在创建重新分发包时指定使用要求，参考cmake-packages(7))的Creating Relocatable Packages章节的内容。 目标属性(Target Properties)编译二进制目标的源文件时，将正确使用INCLUDE_DIRECTORIES，COMPILE_DEFINITIONS和COMPILE_OPTIONS目标属性的内容。 INCLUDE_DIRECTORIES中的条目将使用-I或-isystem作为前缀并按照属性值出现的顺序添加到编译行。 COMPILE_DEFINITIONS中的条目以-D或/D为前缀，并以未指定的顺序添加到编译行。 DEFINE_SYMBOL目标属性也作为SHARED和MODULE库目标的特殊便利案例添加到编译定义。 COMPILE_OPTIONS中的条目将针对shell进行转义，并按属性值的出现顺序添加。有几个编译选项有特殊的单独处理，例如POSITION_INDEPENDENT_CODE。 INTERFACE_INCLUDE_DIRECTORIES，INTERFACE_COMPILE_DEFINITIONS和INTERFACE_COMPILE_OPTIONS目标属性的内容是使用要求 - 它们指定消费者必须使用的内容才能正确编译并链接到它们出现的目标。对于任何二进制目标，将使用target_link_libraries()命令中指定的每个目标上的每个INTERFACE_属性的内容：123456789101112131415set(srcs archive.cpp zip.cpp)if (LZMA_FOUND) list(APPEND srcs lzma.cpp)endif()add_library(archive SHARED $&#123;srcs&#125;)if (LZMA_FOUND) # The archive library sources are compiled with -DBUILDING_WITH_LZMA target_compile_definitions(archive PRIVATE BUILDING_WITH_LZMA)endif()target_compile_definitions(archive INTERFACE USING_ARCHIVE_LIB)add_executable(consumer)# Link consumer to archive and consume its usage requirements. The consumer# executable sources are compiled with -DUSING_ARCHIVE_LIB.target_link_libraries(consumer archive) 因为通常要求将源目录和相应的构建目录添加到INCLUDE_DIRECTORIES，所以可以启用CMAKE_INCLUDE_CURRENT_DIR变量以方便地将相应的目录添加到所有目标的INCLUDE_DIRECTORIES。 可以启用变量CMAKE_INCLUDE_CURRENT_DIR_IN_INTERFACE以将相应的目录添加到所有目标的INTERFACE_INCLUDE_DIRECTORIES。 这样可以使target_link_libraries()命令方便地在多个不同目录中使用目标。 可传递使用要求(Transitive Usage Requirements)目标的使用要求可以传递给它的依赖。 target_link_libraries()命令具有PRIVATE，INTERFACE和PUBLIC关键字来控制传播。1234567891011121314add_library(archive archive.cpp)target_compile_definitions(archive INTERFACE USING_ARCHIVE_LIB)add_library(serialization serialization.cpp)target_compile_definitions(serialization INTERFACE USING_SERIALIZATION_LIB)add_library(archiveExtras extras.cpp)target_link_libraries(archiveExtras PUBLIC archive)target_link_libraries(archiveExtras PRIVATE serialization)# archiveExtras is compiled with -DUSING_ARCHIVE_LIB# and -DUSING_SERIALIZATION_LIBadd_executable(consumer consumer.cpp)# consumer is compiled with -DUSING_ARCHIVE_LIB 由于archive是archiveExtras的PUBLIC依赖项，因此它的使用要求也会传递给consumer。由于serialization是archiveExtras的PRIVATE依赖关系，因此它的使用要求不会传递给consumer。 通常，如果仅在库的实现使用了依赖项，而在头文件中没有使用，则应使用带有PRIVATE关键字的target_link_libraries()来指定依赖关系。如果在库的头文件中也使用了依赖项（例如类的继承），则应将其指定为PUBLIC依赖项。如果依赖性没有在库的实现使用，只在库的头文件中使用，应该将其指定为INTERFACE依赖项。在调用target_link_libraries()命令时可以多次使用关键字：1234target_link_libraries(archiveExtras PUBLIC archive PRIVATE serialization) 通过从依赖项中读取目标属性的INTERFACE_变体并将值附加到操作数的非INTERFACE_变体来传播使用要求。 例如，读取依赖项的INTERFACE_INCLUDE_DIRECTORIES并将其附加到操作数的INCLUDE_DIRECTORIES。 如果依赖项的顺序有关系且需要维护时，通过target_link_libraries()的调用产生的顺序无法正确编译，则使用适当的命令直接设置属性可以更新顺序。例如，如果必须按lib1 lib2 lib3的顺序指定目标的链接库，但包含路径必须按lib3 lib1 lib2的顺序指定：123target_link_libraries(myExe lib1 lib2 lib3)target_include_directories(myExe PRIVATE $&lt;TARGET_PROPERTY:lib3,INTERFACE_INCLUDE_DIRECTORIES&gt;) 注意，在使用install(EXPORT)命令导出目标的使用要求以进行安装时必须小心，具体参阅Creating Packages。 可兼容接口属性(Compatible Interface Properties)某些目标属性需要在目标和每个依赖项的接口之间兼容。 例如，POSITION_INDEPENDENT_CODE目标属性可以指定一个布尔值来说明是否应将目标编译为位置无关代码(position-independent-code)，该代码具有特定于平台的后果。 目标还可以指定使用要求INTERFACE_POSITION_INDEPENDENT_CODE以传达consumers必须编译为位置无关代码。12345678add_executable(exe1 exe1.cpp)set_property(TARGET exe1 PROPERTY POSITION_INDEPENDENT_CODE ON)add_library(lib1 SHARED lib1.cpp)set_property(TARGET lib1 PROPERTY INTERFACE_POSITION_INDEPENDENT_CODE ON)add_executable(exe2 exe2.cpp)target_link_libraries(exe2 lib1) 这里，exe1和exe2都将被编译为与位置无关的代码。lib1也将被编译为与位置无关的代码，因为这是SHARED库的默认设置。 如果依赖关系具有冲突，不兼容的要求cmake(1))发出诊断： 123456789101112add_library(lib1 SHARED lib1.cpp)set_property(TARGET lib1 PROPERTY INTERFACE_POSITION_INDEPENDENT_CODE ON)add_library(lib2 SHARED lib2.cpp)set_property(TARGET lib2 PROPERTY INTERFACE_POSITION_INDEPENDENT_CODE OFF)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 lib1)set_property(TARGET exe1 PROPERTY POSITION_INDEPENDENT_CODE OFF)add_executable(exe2 exe2.cpp)target_link_libraries(exe2 lib1 lib2) lib1要求INTERFACE_POSITION_INDEPENDENT_CODE与exe1目标的POSITION_INDEPENDENT_CODE属性”不兼容”。该库要求将其消费者构建为与位置无关的代码，而可执行文件exe1指定不构建为与位置无关的代码，因此发出诊断。 库lib1和lib2的要求不是“兼容的”。其中一个要求其消费者构建为与位置无关的代码，而另一个要求消费者不构建为与位置无关的代码。因为exe2链接到两者并且它们存在冲突，所以会发出诊断。 要互相“兼容”，POSITION_INDEPENDENT_CODE属性如果被设置，必须与所有可传递依赖项的INTERFACE_POSITION_INDEPENDENT_CODE属性在布尔意义上相同。 通过在COMPATIBLE_INTERFACE_BOOL目标属性的内容中指定属性，可以将“兼容接口要求”的属性扩展到其他属性。每个指定的属性必须在使用目标和相应的属性之间兼容，并且每个依赖项都有一个INTERFACE_前缀：1234567891011121314add_library(lib1Version2 SHARED lib1_v2.cpp)set_property(TARGET lib1Version2 PROPERTY INTERFACE_CUSTOM_PROP ON)set_property(TARGET lib1Version2 APPEND PROPERTY COMPATIBLE_INTERFACE_BOOL CUSTOM_PROP)add_library(lib1Version3 SHARED lib1_v3.cpp)set_property(TARGET lib1Version3 PROPERTY INTERFACE_CUSTOM_PROP OFF)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 lib1Version2) # CUSTOM_PROP will be ONadd_executable(exe2 exe2.cpp)target_link_libraries(exe2 lib1Version2 lib1Version3) # Diagnostic 非布尔属性也可以参与“兼容接口”计算。 COMPATIBLE_INTERFACE_STRING属性中指定的属性必须是未指定的，或者与所有可传递指定的依赖项中的相同字符串进行比较。 这可以用于确保库的多个不兼容版本不会通过目标的传递要求一起被链接：1234567891011121314add_library(lib1Version2 SHARED lib1_v2.cpp)set_property(TARGET lib1Version2 PROPERTY INTERFACE_LIB_VERSION 2)set_property(TARGET lib1Version2 APPEND PROPERTY COMPATIBLE_INTERFACE_STRING LIB_VERSION)add_library(lib1Version3 SHARED lib1_v3.cpp)set_property(TARGET lib1Version3 PROPERTY INTERFACE_LIB_VERSION 3)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 lib1Version2) # LIB_VERSION will be "2"add_executable(exe2 exe2.cpp)target_link_libraries(exe2 lib1Version2 lib1Version3) # Diagnostic COMPATIBLE_INTERFACE_NUMBER_MAX目标属性指定将以数值方式评估内容，并计算所有指定的最大数量：12345678910111213141516add_library(lib1Version2 SHARED lib1_v2.cpp)set_property(TARGET lib1Version2 PROPERTY INTERFACE_CONTAINER_SIZE_REQUIRED 200)set_property(TARGET lib1Version2 APPEND PROPERTY COMPATIBLE_INTERFACE_NUMBER_MAX CONTAINER_SIZE_REQUIRED)add_library(lib1Version3 SHARED lib1_v3.cpp)set_property(TARGET lib1Version3 PROPERTY INTERFACE_CONTAINER_SIZE_REQUIRED 1000)add_executable(exe1 exe1.cpp)# CONTAINER_SIZE_REQUIRED will be "200"target_link_libraries(exe1 lib1Version2)add_executable(exe2 exe2.cpp)# CONTAINER_SIZE_REQUIRED will be "1000"target_link_libraries(exe2 lib1Version2 lib1Version3) 同样，COMPATIBLE_INTERFACE_NUMBER_MIN可用于从依赖项计算属性的数字最小值。每个计算的“兼容”属性值可以在生成阶段使用生成器表达式被消费者读取。请注意，对于每个依赖项，每个兼容接口属性中指定的属性集不得与任何其他属性中指定的集相交。 属性来源调试(Property Origin Debugging)因为构建规范可以由依赖性来确定，所以缺少用于创建目标和代码以设置构建规范的代码的局部性，可能使代码更难以推理。 cmake(1))提供了一个调试工具来打印可能由依赖关系确定的属性内容的来源。 可以调试的属性列在CMAKE_DEBUG_TARGET_PROPERTIES变量文档中：12345678set(CMAKE_DEBUG_TARGET_PROPERTIES INCLUDE_DIRECTORIES COMPILE_DEFINITIONS POSITION_INDEPENDENT_CODE CONTAINER_SIZE_REQUIRED LIB_VERSION)add_executable(exe1 exe1.cpp) 对于COMPATIBLE_INTERFACE_BOOL或COMPATIBLE_INTERFACE_STRING中列出的属性，调试输出显示哪个目标负责设置属性，以及哪些其他依赖项也定义了该属性。 对于COMPATIBLE_INTERFACE_NUMBER_MAX和COMPATIBLE_INTERFACE_NUMBER_MIN，调试输出显示每个依赖项的属性值，以及该值是否确定新的极值。 使用生成器表达式构建规范(Build Specification with Generator Expressions)构建规范可以使用包含内容的generator expressions)，该内容可以是有条件的或仅在生成时知道。 例如，可以使用TARGET_PROPERTY表达式读取属性计算的“兼容”值：123456789101112add_library(lib1Version2 SHARED lib1_v2.cpp)set_property(TARGET lib1Version2 PROPERTY INTERFACE_CONTAINER_SIZE_REQUIRED 200)set_property(TARGET lib1Version2 APPEND PROPERTY COMPATIBLE_INTERFACE_NUMBER_MAX CONTAINER_SIZE_REQUIRED)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 lib1Version2)target_compile_definitions(exe1 PRIVATE CONTAINER_SIZE=$&lt;TARGET_PROPERTY:CONTAINER_SIZE_REQUIRED&gt;) 在这种情况下，exe1源文件将使用-DCONTAINER_SIZE = 200进行编译。 由配置决定的构建规范可以使用CONFIG生成器表达式方便地设置。123target_compile_definitions(exe1 PRIVATE $&lt;$&lt;CONFIG:Debug&gt;:DEBUG_BUILD&gt;) CONFIG参数在与正在构建的配置进行比较时不区分大小写。 在存在IMPORTED目标的情况下，此表达式还会考虑MAP_IMPORTED_CONFIG_DEBUG的内容。 由cmake(1))生成的一些构建系统在CMAKE_BUILD_TYPE变量中具有预定的构建配置集。 诸如Visual Studio和Xcode之类的IDE的构建系统是独立于构建配置生成的，并且在构建之前不知道实际的构建配置。 因此，代码如1234string(TOLOWER $&#123;CMAKE_BUILD_TYPE&#125; _type)if (_type STREQUAL debug) target_compile_definitions(exe1 PRIVATE DEBUG_BUILD)endif() 可能适用于Makefile生成器和Ninja生成器，但不能移植到IDE生成器。 此外，IMPORTED配置映射不会像这样的代码一起考虑，因此应该避免。 一元TARGET_PROPERTY生成器表达式和TARGET_POLICY生成器表达式在使用它的目标上下文中被计算，这意味着可以根据消费者不同地评估使用要求规范：1234567891011121314add_library(lib1 lib1.cpp)target_compile_definitions(lib1 INTERFACE $&lt;$&lt;STREQUAL:$&lt;TARGET_PROPERTY:TYPE&gt;,EXECUTABLE&gt;:LIB1_WITH_EXE&gt; $&lt;$&lt;STREQUAL:$&lt;TARGET_PROPERTY:TYPE&gt;,SHARED_LIBRARY&gt;:LIB1_WITH_SHARED_LIB&gt; $&lt;$&lt;TARGET_POLICY:CMP0041&gt;:CONSUMER_CMP0041_NEW&gt;)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 lib1)cmake_policy(SET CMP0041 NEW)add_library(shared_lib shared_lib.cpp)target_link_libraries(shared_lib lib1) exe1可执行文件将使用-DLIB1_WITH_EXE进行编译，而shared_lib共享库将使用-DLIB1_WITH_SHARED_LIB和-DCONSUMER_CMP0041_NEW进行编译，因为策略CMP0041在创建shared_lib目标时为NEW。 BUILD_INTERFACE表达式包含仅被同一构建系统中的目标使用时的需求，或者使用export()命令导出到构建目录的目标使用时的需求。 INSTALL_INTERFACE表达式包含仅在使用install(EXPORT)命令安装和导出目标时才使用的需求：123456789101112add_library(ClimbingStats climbingstats.cpp)target_compile_definitions(ClimbingStats INTERFACE $&lt;BUILD_INTERFACE:ClimbingStats_FROM_BUILD_LOCATION&gt; $&lt;INSTALL_INTERFACE:ClimbingStats_FROM_INSTALLED_LOCATION&gt;)install(TARGETS ClimbingStats EXPORT libExport $&#123;InstallArgs&#125;)install(EXPORT libExport NAMESPACE Upstream:: DESTINATION lib/cmake/ClimbingStats)export(EXPORT libExport NAMESPACE Upstream::)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 ClimbingStats) 在上面的情况下，exe1可执行文件将使用-DClimbingStats_FROM_BUILD_LOCATION进行编译。 导出命令生成IMPORTED目标，省略INSTALL_INTERFACE或BUILD_INTERFACE，并删除* _INTERFACE标记。 使用ClimbingStats包的单独项目将包含：1234find_package(ClimbingStats REQUIRED)add_executable(Downstream main.cpp)target_link_libraries(Downstream Upstream::ClimbingStats) 根据是从构建位置还是安装位置使用ClimbingStats包，可以使用-DClimbingStats_FROM_BUILD_LOCATION或-DClimbingStats_FROM_INSTALL_LOCATION编译Downstream目标。 有关包和导出的更多信息，请参阅cmake-packages(7))手册。 包含目录和使用要求(Include Directories and Usage Requirements)当包含目录指定为使用要求以及与生成器表达式一起使用时，包含目录需要特别考虑。 target_include_directories()命令接受相对和绝对包含目录：12345add_library(lib1 lib1.cpp)target_include_directories(lib1 PRIVATE /absolute/path relative/path) 相对路径相对于命令出现的源目录进行解释。IMPORTED目标的INTERFACE_INCLUDE_DIRECTORIES中不允许相对路径。 在使用非平凡的生成器表达式的情况下，INSTALL_PREFIX表达式可以在INSTALL_INTERFACE表达式的参数内使用。 它是一个替换标记，在使用项目导入时会扩展为安装前缀。 包含目录使用要求通常在构建树和安装树之间有所不同。 BUILD_INTERFACE和INSTALL_INTERFACE生成器表达式可用于根据使用位置描述单独的使用要求。 INSTALL_INTERFACE表达式中允许使用相对路径，并相对于安装前缀进行解释。 例如：1234567add_library(ClimbingStats climbingstats.cpp)target_include_directories(ClimbingStats INTERFACE $&lt;BUILD_INTERFACE:$&#123;CMAKE_CURRENT_BINARY_DIR&#125;/generated&gt; $&lt;INSTALL_INTERFACE:/absolute/path&gt; $&lt;INSTALL_INTERFACE:relative/path&gt; $&lt;INSTALL_INTERFACE:$&lt;INSTALL_PREFIX&gt;/$&lt;CONFIG&gt;/generated&gt;) 有两个与包含目录使用要求相关的便利API。 可以启用CMAKE_INCLUDE_CURRENT_DIR_IN_INTERFACE变量，对于每个影响的目标，其效果与下面等价：123set_property(TARGET tgt APPEND PROPERTY INTERFACE_INCLUDE_DIRECTORIES $&lt;BUILD_INTERFACE:$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;;$&#123;CMAKE_CURRENT_BINARY_DIR&#125;&gt;) 安装目标的便利性是带有install(TARGETS)命令的INCLUDES DESTINATION组件：12345install(TARGETS foo bar bat EXPORT tgts $&#123;dest_args&#125; INCLUDES DESTINATION include)install(EXPORT tgts $&#123;other_args&#125;)install(FILES $&#123;headers&#125; DESTINATION include) 这相当于在install(TARGETS)生成时将${CMAKE_INSTALL_PREFIX}/include附加到每个已安装的IMPORTED目标的INTERFACE_INCLUDE_DIRECTORIES。 当使用imported target的INTERFACE_INCLUDE_DIRECTORIES时，属性中的条目将被视为SYSTEM包含目录，就好像它们列在依赖项的INTERFACE_SYSTEM_INCLUDE_DIRECTORIES中一样。 这可能导致省略在这些目录中找到的头文件的编译器警告。 可以通过在导入目标(Imported Targets)的使用者上设置NO_SYSTEM_FROM_IMPORTED目标属性来控制导入目标的此行为。 如果二进制目标与macOS FRAMEWORK传递链接，则框架的Headers目录也会被视为使用要求。 这与将framework目录作为包含目录传递具有相同的效果。 链接器和生成器表达式(Link Libraries and Generator Expressions)与构建规范一样，可以使用生成表达式条件指定链接库(link libraries)。 但是，由于使用要求的消耗是基于链接依赖关系的收集，因此链接依赖关系必须形成“有向非循环图”作为另一个限制。 也就是说，如果链接到目标依赖于目标属性的值，那么该目标属性可能不依赖于链接的依赖项：12345678910add_library(lib1 lib1.cpp)add_library(lib2 lib2.cpp)target_link_libraries(lib1 PUBLIC $&lt;$&lt;TARGET_PROPERTY:POSITION_INDEPENDENT_CODE&gt;:lib2&gt;)add_library(lib3 lib3.cpp)set_property(TARGET lib3 PROPERTY INTERFACE_POSITION_INDEPENDENT_CODE ON)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 lib1 lib3) 由于exe1目标的POSITION_INDEPENDENT_CODE属性的值取决于链接库（lib3），并且链接exe1的边缘由相同的POSITION_INDEPENDENT_CODE属性确定，因此上面的依赖关系图包含一个循环。 cmake(1))在这种情况下发出诊断。 输出工件(Output Artifacts)由add_library()和add_executable()命令创建的构建系统目标生成创建二进制输出的规则。 二进制文件的确切输出位置只能在生成时确定，因为它依赖于构建配置和链接依赖项的链接语言等。TARGET_FILE，TARGET_LINKER_FILE和相关表达式可用于访问生成的二进制文件的名称和位置 。 但是，这些表达式不适用于OBJECT库，因为这些类不会生成与表达式相关的单个文件。 目标可以构建三种输出工件(output artifacts)，如以下部分中所述。 它们的分类在DLL平台和非DLL平台之间有所不同。 包括Cygwin在内的所有基于Windows的系统都是DLL平台。 运行时输出工件(Runtime Output Artifacts)构建系统目标的运行时输出工件(runtime output artifact)可以是： 由add_executable()命令创建的可执行目标的可执行文件（例如.exe）。 在DLL平台上：由add_library()命令和SHARED选项创建的共享库目标的可执行文件（例如.dll）。RUNTIME_OUTPUT_DIRECTORY和RUNTIME_OUTPUT_NAME目标属性可用于控制构建树中的运行时输出工件位置和名称。 库输出工件(Library Output Artifacts)构建系统目标的库输出工件(library output artifact)可以是： 由add_library()命令和MODULE选项创建的模块库目标的可加载模块文件（例如.dll或.so）。 在非DLL平台上：由add_library()命令和SHARED选项创建的共享库目标的共享库文件（例如.so或.dylib）。LIBRARY_OUTPUT_DIRECTORY和LIBRARY_OUTPUT_NAME目标属性可用于控制构建树中的库输出工件位置和名称。 归档输出工件(Archive Output Artifacts)构建系统目标的归档输出工件(archive output artifact)可以是： 由add_library()命令和STATIC选项创建的静态库目标的静态库文件（例如.lib或.a）。 在DLL平台上：由add_library()命令和SHARED选项创建的共享库目标的导入库文件（例如.lib）。 该文件只在库导出至少一个非托管符号的情况下才保证存在。 在DLL平台上：当设置了ENABLE_EXPORTS目标属性时，add_executable()命令创建的可执行目标的导入库文件（例如.lib）。ARCHIVE_OUTPUT_DIRECTORY和ARCHIVE_OUTPUT_NAME目标属性可用于控制构建树中的归档输出工件位置和名称。 目录范围命令(Directory-Scoped Commands)target_include_directories()，target_compile_definitions()和target_compile_options()命令一次只对一个目标产生影响。 命令add_compile_definitions()，add_compile_options()和include_directories()具有类似的功能，但为方便起见，在目录范围而不是目标范围内运行。 伪目标(Pseudo Targets)某些目标类型不代表构建系统的输出，而只代表外部依赖项，别名或其他非构建工件等输入。 伪目标未在生成的构建系统中表示。 导入目标(Imported Targets)IMPORTED目标表示预先存在的依赖项。通常这些目标由上游包定义，应该被视为不可变的。声明IMPORTED目标后，可以使用常规命令调整其目标属性，例如target_compile_definitions()，target_include_directories()，target_compile_options()或target_link_libraries()，就像使用任何其他常规目标一样。 IMPORTED目标可能具有与二进制目标相同的使用要求属性，例如INTERFACE_INCLUDE_DIRECTORIES，INTERFACE_COMPILE_DEFINITIONS，INTERFACE_COMPILE_OPTIONS，INTERFACE_LINK_LIBRARIES和INTERFACE_POSITION_INDEPENDENT_CODE。 LOCATION也可以从IMPORTED目标中读取，但很少有理由这样做。诸如add_custom_command()之类的命令可以透明地将IMPORTED EXECUTABLE目标用作COMMAND可执行文件。 IMPORTED目标定义的范围是定义它的目录。它可以从子目录访问和使用，但不能从父目录或兄弟目录中访问和使用。范围类似于cmake变量的范围。 还可以定义GLOBAL IMPORTED目标，该目标在构建系统中可全局访问。 有关创建具有IMPORTED目标的包的更多信息，请参阅cmake-packages(7))手册。 别名目标(Alias Targets)ALIAS目标是一个名称，可以在只读上下文中与二进制目标名称互换使用。 ALIAS目标的主要用例是例如库附带的单元测试可执行文件，它可以是同一构建系统的一部分，也可以根据用户配置单独构建。 12345add_library(lib1 lib1.cpp)install(TARGETS lib1 EXPORT lib1Export $&#123;dest_args&#125;)install(EXPORT lib1Export NAMESPACE Upstream:: $&#123;other_args&#125;)add_library(Upstream::lib1 ALIAS lib1) 在另一个目录中，我们可以无条件地链接到Upstream::lib1目标，它可以是来自包的IMPORTED目标，或者如果构建为同一构建系统的一部分则是ALIAS目标。12345if (NOT TARGET Upstream::lib1) find_package(lib1 REQUIRED)endif()add_executable(exe1 exe1.cpp)target_link_libraries(exe1 Upstream::lib1) ALIAS目标不具有可变性，可安装性或可导出性。 它们完全是构建系统描述的本地。 可以通过从中读取ALIASED_TARGET属性来测试名称是否为ALIAS名称：1234get_target_property(_aliased Upstream::lib1 ALIASED_TARGET)if(_aliased) message(STATUS "The name Upstream::lib1 is an ALIAS for $&#123;_aliased&#125;.")endif() 接口库(Interface Libraries)INTERFACE目标没有LOCATION并且是可变的，但在其他方面类似于IMPORTED目标。 它可以指定使用要求，例如INTERFACE_INCLUDE_DIRECTORIES，INTERFACE_COMPILE_DEFINITIONS，INTERFACE_COMPILE_OPTIONS，INTERFACE_LINK_LIBRARIES，INTERFACE_SOURCES和INTERFACE_POSITION_INDEPENDENT_CODE。 只有target_include_directories()，target_compile_definitions()，target_compile_options()，target_sources()和target_link_libraries()命令的INTERFACE模式可以与INTERFACE库一起使用。 INTERFACE库的主要用例是只含头文件的库(header-only libraries)。12345678add_library(Eigen INTERFACE)target_include_directories(Eigen INTERFACE $&lt;BUILD_INTERFACE:$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/src&gt; $&lt;INSTALL_INTERFACE:include/Eigen&gt;)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 Eigen) 这里，Eigen目标的使用要求在编译时被消耗和使用，但它对链接没有影响。 另一个用例是针对使用要求采用完全以目标为中心的设计：123456789101112add_library(pic_on INTERFACE)set_property(TARGET pic_on PROPERTY INTERFACE_POSITION_INDEPENDENT_CODE ON)add_library(pic_off INTERFACE)set_property(TARGET pic_off PROPERTY INTERFACE_POSITION_INDEPENDENT_CODE OFF)add_library(enable_rtti INTERFACE)target_compile_options(enable_rtti INTERFACE $&lt;$&lt;OR:$&lt;COMPILER_ID:GNU&gt;,$&lt;COMPILER_ID:Clang&gt;&gt;:-rtti&gt;)add_executable(exe1 exe1.cpp)target_link_libraries(exe1 pic_on enable_rtti) 这样，exe1的构建规范完全表示为链接目标，编译器特定标志的复杂性封装在INTERFACE库目标中。 允许在INTERFACE库中设置或读取的属性包括： 与INTERFACE_*匹配的属性 与COMPATIBLE_INTERFACE_*匹配的内置属性 EXPORT_NAME EXPORT_PROPERTIES IMPORTED MANUALLY_ADDED_DEPENDENCIES NAME 与IMPORTED_LIBNAME_*匹配的属性 与MAP_IMPORTED_CONFIG_*匹配的属性INTERFACE库可以安装和导出。 他们引用的任何内容必须单独安装：12345678910111213141516add_library(Eigen INTERFACE)target_include_directories(Eigen INTERFACE $&lt;BUILD_INTERFACE:$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/src&gt; $&lt;INSTALL_INTERFACE:include/Eigen&gt;)install(TARGETS Eigen EXPORT eigenExport)install(EXPORT eigenExport NAMESPACE Upstream:: DESTINATION lib/cmake/Eigen)install(FILES $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/src/eigen.h $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/src/vector.h $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/src/matrix.h DESTINATION include/Eigen) 参考(Reference) cmake-buildsystem]]></content>
      <categories>
        <category>translation</category>
        <category>cmake</category>
      </categories>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++函数参数传递详解]]></title>
    <url>%2F2018%2F04-14-Cpp-FunctionParameters%2F</url>
    <content type="text"><![CDATA[转载自 C/C++中函数参数传递详解 参数传递C++中函数的参数传递方式包括：值传递、指针传递、引用传递这三种方法。下面的代码使用这三种方式，来实现a和b值的交换。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;using namespace std; //值传递void swap1(int p,int q)&#123; int temp; temp=p; p=q; q=temp;&#125;//指针传递，函数体内只有指针值的变化void swap2(int *p,int *q)&#123; int temp; temp=*p; *p=*q; *q=temp;&#125;//指针传递，函数体内只有指针的变化void swap3(int *p,int *q)&#123; int *temp; temp=p; p=q; q=temp;&#125;//引用传递void swap4(int &amp;p,int &amp;q)&#123; int temp; temp=p; p=q; q=temp;&#125;int main()&#123; int a=1,b=2; swap1(a,b); //swap2(&amp;a,&amp;b); //swap3(&amp;a,&amp;b); //swap4(a,b); cout &lt;&lt; a &lt;&lt; " " &lt;&lt; b&lt;&lt; endl; return 0;&#125; 代码中一共有四个函数，其中有两个是指针传递，但函数体内的实现不一样。下面具体分析： 值传递swap1函数实现的值传递，值传递传递的是实际参数的一个副本，形参p与q的地址和实参a与b的地址不一样，参数传递时只是把a与b的值拷贝过去了，在swap1中对p和q操作只是对临时分配的栈中内容进行操作，函数执行完后形参就消失了，对原来的a和b不产生任何影响。所以swap1不能完成交换a和b值的功能。 指针传递swap2和swap3都是指针传递，swap2函数体内交换了p和q指向地址的值，swap3函数体内交换了p和q指向的地址。 先说swap2，形参指针p和q指向的是a和b的地址，而不是像值传递那样将实参的值拷贝到另外分配的地址中，运行到函数尾时，指针p和q指向的地址没变，但地址中的值变了，也即a和b地址中的变了，就是a和b的值成功交换。 再来看swap3，swap3运行到函数尾时，p和q交换了地址，但最后函数执行完后，a和b的值并未交换。swap3中，形参p和q会保存在栈中，p指向a的地址，q指向b的地址，使用temp指针完成了p和q的地址交换，即p指向b的地址，q指向了a的地址，但a和b地址中的值并未发生变化，这与swap2不同，swap2中是p指向的地址中的值(就是a)与q指向的地址中的值(b)交换，所以swap2执行完后a和b的值是交换了的。 引用传递引用传递时，对形参的操作等同于对实参的操作，即传递的不会是实参的副本，而就是实参。最后会交换a和b的值。 总结到此，完了。当然函数参数也可以是指向指针的指针，这也是很常见的，但通常用在需要动态分配内存的地方以避免内存泄露。在使用cuda时调用cudaMalloc其参数就是这样，指向指针的指针。而malloc、CPLMolloc、new这些是通过返回值传递分配的动态内存的，自然是不会出现内存泄露的，这个后面再说。 参考 C/C++中函数参数传递详解 C++中函数调用时的三种参数传递方式详解]]></content>
      <categories>
        <category>repost</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++内存分配]]></title>
    <url>%2F2018%2F04-14-Cpp-MemoryAllocation%2F</url>
    <content type="text"><![CDATA[C++存储区 栈区：由编译器自动分配释放，存放函数的参数值，局部变量的值等。函数结束后，该变量的存储单元自动释放，效率高，分配的空间有限。 堆区：一般由程序员通过new分配的动态内存单元，并由delete释放，若程序员不释放，程序结束时可能由OS（操作系统）回收。 自由存储区：那些由malloc等分配的内存块，他和堆是十分相似的，不过它是用free来结束自己的生命的。 全局/静态存储区：全局变量和静态变量被分配到同一块内存中，在以前的C语言中，全局变量又分为初始化的和未初始化的，在C++里面没有这个区分了，他们共同占用同一块内存区。 常量存储区：这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改 堆与栈的区别参考 What and where are the stack and heap? C++内存分配方式详解(堆、栈、自由存储区、全局/静态存储区和常量存储区) C++五种内存分配、堆与栈区别 C++内存分配机制]]></content>
      <categories>
        <category>knowledge</category>
        <category>coding</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++指针作为函数参数]]></title>
    <url>%2F2018%2F04-14-Cpp-PointerWithinFunctions%2F</url>
    <content type="text"><![CDATA[指针作为函数参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;using namespace std;int* test1(int* i)&#123; i = new int; *i = 1; return i;&#125;void test2(int* i)&#123; *i = 2;&#125;void test3(int* i)&#123; i = new int; *i = 3;&#125;void test4(int** i)&#123; *i = new int; **i = 4;&#125;void test5(int*&amp; i)&#123; i = new int; *i = 5;&#125;int main()&#123; int *a; a = test1(a); cout &lt;&lt; "test1: " &lt;&lt; *a &lt;&lt; endl; // 1 int *b; b = new int; test2(b); cout &lt;&lt; "test2: " &lt;&lt; *b &lt;&lt; endl; // 2 int *c; test3(c); cout &lt;&lt; "test3: " &lt;&lt; *c &lt;&lt; endl; // Error int *d; test4(&amp;d); cout &lt;&lt; "test4: " &lt;&lt; *d &lt;&lt; endl; // 4 int *e; test5(e); cout &lt;&lt; "test5: " &lt;&lt; *e &lt;&lt; endl; // 5&#125; C++在创建指针的时候，只分配存储地址的内存，并不会分配存储数据的内存，所以指针可能指向任何位置。 test1中，函数形参i只是实参a的拷贝，但在函数内部，我们为i重新分配了地址，然后为这个地址指向的位置赋值为1，然后将这个新分配的地址返回，即赋值给a，此时的a已经不是原来的a了。 test2中，我们首先为实参b分配了一个int内存，然后传入test2，形参i同样是是实参b的拷贝，在函数内部，为i指向的位置赋值2。 test3中，函数形参i只是实参c的一个拷贝，但在函数中，我们为i重新分配了一个int内存，此时i和c指向两个不同的地方，给这个新的位置赋值为3，跟*c没有什么关系。 参考 c++函数中new申请的内存是如何释放的？]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++函数中使用指针]]></title>
    <url>%2F2018%2F04-14-Cpp-PointerAsFunctionParameters%2F</url>
    <content type="text"><![CDATA[转载自c/c++使用指针做函数返回值和指针作函数参数问题 使用指针做函数返回值1、当使用指针做为函数的返回值时，主函数处的char *p;将获得调用函数char *pf;的值，即一个地址值，如oxAE72。此时需要我们注意的是该地址值所指向的空间是否存在(即已向操作系统声明注册，不会被释放，即可能被其他操作修改)； 2、使用栈内存返回指针是明显错误的，因为栈内存将在调用结束后自动释放，从而主函数使用该地址空间将很危险。 12345678910char* GetMemory()&#123; char p[] = "hi"; return p;&#125;void main()&#123; char *str = GetMemory(); //出错! 得到一块已释放的内存 printf(str);&#125; 3、使用堆内存返回指针是正确的，但是注意可能产生内存泄露问题，在使用完毕后主函数中释放该段内存。 例如：12345678910char* GetMemory()&#123; char *p = new char[100]; return p;&#125;void main()&#123; char *str = GetMemory(); delete [] str; //防止内存泄露!&#125; 使用指针做函数参数1、有的情况下我们可能需要需要在调用函数中分配内存，而在主函数中使用，而针对的指针此时为函数的参数。此时应注意形参与实参的问题，因为在C语言中，形参只是继承了实参的值，是另外一个量(ps:返回值也是同理，传递了一个地址值(指针)或实数值)，形参的改变并不能引起实参的改变。 2、直接使用形参分配内存的方式显然是错误的，因为实参的值并不会改变，如下则实参一直为NULL: 12345678910void GetMemory(char* p)&#123; char *p = new char[100];&#125;void main()&#123; char *str; GetMemory(str); strcpy(str, "hi"); // str = NULL&#125; 3、由于通过指针是可以传值的，因为此时该指针的地址是在主函数中申请的栈内存，我们通过指针对该栈内存进行操作，从而改变了实参的值。 1234567891011void Change(char *p)&#123; *p = 'b';&#125;void main()&#123; char a = 'a'; char* p = &amp;a; Change(p); printf("%c\n", a); //值a改变!&#125; 4、根据上述的启发，我们也可以采用指向指针的指针来进行在调用函数中申请，在主函数中应用。如下：假设a的地址为ox23，内容为&#39;a&#39;；而str的地址是ox46，内容为ox23；而pstr的地址是ox79，内容为ox46。 我们通过调用函数GetMemory，从而将pstr的内容赋给了p，此时p = ox46。通过对*p(ox23)的操作，即将内存地址为ox23之中的值改为char[100]的首地址，从而完成了对char* str地址的分配。 123456789101112void GetMemory(char** p)&#123; char *p = new char[100];&#125;void main()&#123; char a = 'a'; char* str = &amp;a; char** pstr = &amp;str; GetMemory(pstr); strcpy(str, "hi");&#125; 5、注意指针的释放问题，可能形成悬浮指针。 当我们释放掉一个指针p后，只是告诉操作系统该段内存可以被其他程序使用，而该指针p的地址值(如ox23)仍然存在。如果再次给这块地址赋值是危险的，应该将p指针置为NULL。 调用函数删除主函数中的内存块时，虽然可以通过地址传递直接删除，但由于无法对该指针赋值(形参不能传值)，可能造成悬浮指针，所以此时也应该采用指向指针的指针的形参。 例如：1234567891011void MemoryFree(char** p)&#123; delete *p; *p = NULL;&#125;void main()&#123; char *str = new char[100]; char *pstr = &amp;str; MemoryFree(pstr);&#125; 参考 c/c++使用指针做函数返回值和指针作函数参数问题]]></content>
      <categories>
        <category>repost</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML颜色表]]></title>
    <url>%2F2018%2F04-13-ColorTable%2F</url>
    <content type="text"><![CDATA[HTML颜色表 颜色名 十六进制颜色值 颜色 AliceBlue #F0F8FF rgb(240, 248, 255) AntiqueWhite #FAEBD7 rgb(250, 235, 215) Aqua #00FFFF rgb(0, 255, 255) Aquamarine #7FFFD4 rgb(127, 255, 212) Azure #F0FFFF rgb(240, 255, 255) Beige #F5F5DC rgb(245, 245, 220) Bisque #FFE4C4 rgb(255, 228, 196) Black #000000 rgb(0, 0, 0) BlanchedAlmond #FFEBCD rgb(255, 235, 205) Blue #0000FF rgb(0, 0, 255) BlueViolet #8A2BE2 rgb(138, 43, 226) Brown #A52A2A rgb(165, 42, 42) BurlyWood #DEB887 rgb(222, 184, 135) CadetBlue #5F9EA0 rgb(95, 158, 160) Chartreuse #7FFF00 rgb(127, 255, 0) Chocolate #D2691E rgb(210, 105, 30) Coral #FF7F50 rgb(255, 127, 80) CornflowerBlue #6495ED rgb(100, 149, 237) Cornsilk #FFF8DC rgb(255, 248, 220) Crimson #DC143C rgb(220, 20, 60) Cyan #00FFFF rgb(0, 255, 255) DarkBlue #00008B rgb(0, 0, 139) DarkCyan #008B8B rgb(0, 139, 139) DarkGoldenRod #B8860B rgb(184, 134, 11) DarkGray #A9A9A9 rgb(169, 169, 169) DarkGreen #006400 rgb(0, 100, 0) DarkKhaki #BDB76B rgb(189, 183, 107) DarkMagenta #8B008B rgb(139, 0, 139) DarkOliveGreen #556B2F rgb(85, 107, 47) Darkorange #FF8C00 rgb(255, 140, 0) DarkOrchid #9932CC rgb(153, 50, 204) DarkRed #8B0000 rgb(139, 0, 0) DarkSalmon #E9967A rgb(233, 150, 122) DarkSeaGreen #8FBC8F rgb(143, 188, 143) DarkSlateBlue #483D8B rgb(72, 61, 139) DarkSlateGray #2F4F4F rgb(47, 79, 79) DarkTurquoise #00CED1 rgb(0, 206, 209) DarkViolet #9400D3 rgb(148, 0, 211) DeepPink #FF1493 rgb(255, 20, 147) DeepSkyBlue #00BFFF rgb(0, 191, 255) DimGray #696969 rgb(105, 105, 105) DodgerBlue #1E90FF rgb(30, 144, 255) Feldspar #D19275 rgb(209, 146, 117) FireBrick #B22222 rgb(178, 34, 34) FloralWhite #FFFAF0 rgb(255, 250, 240) ForestGreen #228B22 rgb(34, 139, 34) Fuchsia #FF00FF rgb(255, 0, 255) Gainsboro #DCDCDC rgb(220, 220, 220) GhostWhite #F8F8FF rgb(248, 248, 255) Gold #FFD700 rgb(255, 215, 0) GoldenRod #DAA520 rgb(218, 165, 32) Gray #808080 rgb(128, 128, 128) Green #008000 rgb(0, 128, 0) GreenYellow #ADFF2F rgb(173, 255, 47) HoneyDew #F0FFF0 rgb(240, 255, 240) HotPink #FF69B4 rgb(255, 105, 180) IndianRed #CD5C5C rgb(205, 92, 92) Indigo #4B0082 rgb(75, 0, 130) Ivory #FFFFF0 rgb(255, 255, 240) Khaki #F0E68C rgb(240, 230, 140) Lavender #E6E6FA rgb(230, 230, 250) LavenderBlush #FFF0F5 rgb(255, 240, 245) LawnGreen #7CFC00 rgb(124, 252, 0) LemonChiffon #FFFACD rgb(255, 250, 205) LightBlue #ADD8E6 rgb(173, 216, 230) LightCoral #F08080 rgb(240, 128, 128) LightCyan #E0FFFF rgb(224, 255, 255) LightGoldenRodYellow #FAFAD2 rgb(250, 250, 210) LightGrey #D3D3D3 rgb(211, 211, 211) LightGreen #90EE90 rgb(144, 238, 144) LightPink #FFB6C1 rgb(255, 182, 193) LightSalmon #FFA07A rgb(255, 160, 122) LightSeaGreen #20B2AA rgb(32, 178, 170) LightSkyBlue #87CEFA rgb(135, 206, 250) LightSlateBlue #8470FF rgb(132, 112, 255) LightSlateGray #778899 rgb(119, 136, 153) LightSteelBlue #B0C4DE rgb(176, 196, 222) LightYellow #FFFFE0 rgb(255, 255, 224) Lime #00FF00 rgb(0, 255, 0) LimeGreen #32CD32 rgb(50, 205, 50) Linen #FAF0E6 rgb(250, 240, 230) Magenta #FF00FF rgb(255, 0, 255) Maroon #800000 rgb(128, 0, 0) MediumAquaMarine #66CDAA rgb(102, 205, 170) MediumBlue #0000CD rgb(0, 0, 205) MediumOrchid #BA55D3 rgb(186, 85, 211) MediumPurple #9370D8 rgb(147, 112, 216) MediumSeaGreen #3CB371 rgb(60, 179, 113) MediumSlateBlue #7B68EE rgb(123, 104, 238) MediumSpringGreen #00FA9A rgb(0, 250, 154) MediumTurquoise #48D1CC rgb(72, 209, 204) MediumVioletRed #C71585 rgb(199, 21, 133) MidnightBlue #191970 rgb(25, 25, 112) MintCream #F5FFFA rgb(245, 255, 250) MistyRose #FFE4E1 rgb(255, 228, 225) Moccasin #FFE4B5 rgb(255, 228, 181) NavajoWhite #FFDEAD rgb(255, 222, 173) Navy #000080 rgb(0, 0, 128) OldLace #FDF5E6 rgb(253, 245, 230) Olive #808000 rgb(128, 128, 0) OliveDrab #6B8E23 rgb(107, 142, 35) Orange #FFA500 rgb(255, 165, 0) OrangeRed #FF4500 rgb(255, 69, 0) Orchid #DA70D6 rgb(218, 112, 214) PaleGoldenRod #EEE8AA rgb(238, 232, 170) PaleGreen #98FB98 rgb(152, 251, 152) PaleTurquoise #AFEEEE rgb(175, 238, 238) PaleVioletRed #D87093 rgb(216, 112, 147) PapayaWhip #FFEFD5 rgb(255, 239, 213) PeachPuff #FFDAB9 rgb(255, 218, 185) Peru #CD853F rgb(205, 133, 63) Pink #FFC0CB rgb(255, 192, 203) Plum #DDA0DD rgb(221, 160, 221) PowderBlue #B0E0E6 rgb(176, 224, 230) Purple #800080 rgb(128, 0, 128) Red #FF0000 rgb(255, 0, 0) RosyBrown #BC8F8F rgb(188, 143, 143) RoyalBlue #4169E1 rgb(65, 105, 225) SaddleBrown #8B4513 rgb(139, 69, 19) Salmon #FA8072 rgb(250, 128, 114) SandyBrown #F4A460 rgb(244, 164, 96) SeaGreen #2E8B57 rgb(46, 139, 87) SeaShell #FFF5EE rgb(255, 245, 238) Sienna #A0522D rgb(160, 82, 45) Silver #C0C0C0 rgb(192, 192, 192) SkyBlue #87CEEB rgb(135, 206, 235) SlateBlue #6A5ACD rgb(106, 90, 205) SlateGray #708090 rgb(112, 128, 144) Snow #FFFAFA rgb(255, 250, 250) SpringGreen #00FF7F rgb(0, 255, 127) SteelBlue #4682B4 rgb(70, 130, 180) Tan #D2B48C rgb(210, 180, 140) Teal #008080 rgb(0, 128, 128) Thistle #D8BFD8 rgb(216, 191, 216) Tomato #FF6347 rgb(255, 99, 71) Turquoise #40E0D0 rgb(64, 224, 208) Violet #EE82EE rgb(238, 130, 238) VioletRed #D02090 rgb(208, 32, 144) Wheat #F5DEB3 rgb(245, 222, 179) White #FFFFFF rgb(255, 255, 255) WhiteSmoke #F5F5F5 rgb(245, 245, 245) Yellow #FFFF00 rgb(255, 255, 0) YellowGreen #9ACD32 rgb(154, 205, 50) 生成代码123456789101112131415161718#-*- coding:utf-8 -*-fin = open('color.txt', 'r')inLines = fin.readlines() #按行读出文件内容fin.close()outTxt = ''for line in inLines: temp1 = line.strip('\n') #去掉每行最后的换行符'\n' temp2 = temp1.split('\t') #以'\t'为标志，将每行分割成列表 # print(temp2[0]) outLine = '&lt;span style="color:' + temp2[0] + '"&gt;' + temp2[0] + '&lt;/span&gt; | ' outLine = outLine + '&lt;span style="color:' + temp2[0] + '"&gt;' + temp2[1] + '&lt;/span&gt; | ' outLine = outLine + '&lt;span style="background-color:' + temp2[0] + '"&gt;' + temp2[2] + '&lt;/span&gt;\n' outTxt = outTxt + outLinefout = open('color-1.txt', 'w')fout.write(outTxt)fout.close() 参考 CSDN-markdown编辑器语法——字体、字号与颜色 python读取txt文件到列表中]]></content>
      <categories>
        <category>knowledge</category>
        <category>miscellaneous</category>
      </categories>
      <tags>
        <tag>html</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown Tips]]></title>
    <url>%2F2018%2F04-12-MD-MarkdownTips%2F</url>
    <content type="text"><![CDATA[字体字体颜色内嵌HTML标签 &lt;span style=&quot;color:red&quot;&gt;Aquamarine&lt;/span&gt; =&gt; Aquamarine &lt;span style=&quot;background-color:MistyRose&quot;&gt;Aquamarine&lt;/span&gt; =&gt; Aquamarine 借助MathJax \color{blue}{MathJax} =&gt; $\color{blue}{MathJax}$ \color{blue}\text{MathJax} =&gt; $\color{blue}\text{MathJax}$ \text{\color{blue}{MathJax}} =&gt; $\text{\color{blue}{MathJax}}$ 字体大小 &lt;span style=&quot;font-size: 120%;&quot;&gt;1.2 larger text&lt;/span&gt; =&gt; 1.2 larger text &lt;span style=&quot;font-size: 80%;&quot;&gt;0.8 smaller text&lt;/span&gt; =&gt; 0.8 smaller text 同时设置[3] &lt;span style=&quot;color:red; background-color:lightgray; font-size: 120%; text-align: center;&quot;&gt;Test Text&lt;/span&gt; =&gt; Test Text 表格表格内容位置 内容居左 内容居中 内容居右 A A A B B B 参考 为什么markdown不支持字号和字体颜色？ CSDN-markdown编辑器语法——字体、字号与颜色 Jekyll kramdown how to center text]]></content>
      <categories>
        <category>tools</category>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>writing</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS静态库与动态库]]></title>
    <url>%2F2018%2F04-10-Mac-StaticAndDynamicLibrary%2F</url>
    <content type="text"><![CDATA[Static Library(.a)Static libraries allow an application to load code into its address space at compile time.This results in a larger size on disk and slower launch times. Because the library’s code is added directly to the linked target’s binary, it means that to update any code in the library, the linked target would also have to be rebuilt. Dynamic Library(.dylib)Dynamic libraries allow an application to load code into its address space when it’s actually needed at run time. Because the code isn’t statically linked into the executable binary, there are some benefits from loading at runtime. Mainly, the libraries can be updated with new features or bug-fixes without having to recompile and relink executable. In addition, being loaded at runtime means that individual code libraries can have their own initializers and clean up after their own tasks before being unloaded from memory Link Library with CMakeCMake favours passing the full path to link libraries Assuming libfoo.a is in ${CMAKE_SOURCE_DIR}, to link the library using:12add_executable(main main.cpp)target_link_libraries(main $&#123;CMAKE_SOURCE_DIR&#125;/libfoo.a) The same for dynamic library:1234add_executable(main main.cpp)target_link_libraries(main /usr/local/Cellar/open-mesh/6.3/lib/libOpenMeshTools.6.3.dylib /usr/local/Cellar/open-mesh/6.3/lib/libOpenMeshCore.6.3.dylib) Reference What is the difference between .dylib and .a lib in ios? How do I tell CMake to link in a static library in the source directory?]]></content>
      <categories>
        <category>knowledge</category>
        <category>coding</category>
      </categories>
      <tags>
        <tag>macOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMake Tutorial]]></title>
    <url>%2F2018%2F04-07-Cpp-CMakeTutorial%2F</url>
    <content type="text"><![CDATA[翻译自cmake-tutorial. 注意：翻译不一定准确，内容仅供参考! A Basic Starting Point (Step1)一个最基本的项目是由一些源代码文件构建得到的可执行文件，对于一个简单的项目，只需要一个三行的CMakeLists.txt文件，这个文件也将是本教程的出发点。这样一个CMakeLists.txt文件像下面这样： 123cmake_minimum_required (VERSION 2.6)project (Tutorial)add_executable(Tutorial tutorial.cxx) 注意这个例子中的CMakeLists.txt文件使用了小写字母的命令，CMake支持大写、小写、混合字母的命令。源代码tutorial.cxx计算一个数字的平方根，第一个版本非常简单，如下所示： 12345678910111213141516// A simple program that computes the square root of a number#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;math.h&gt;int main (int argc, char *argv[])&#123; if (argc &lt; 2) &#123; fprintf(stdout,"Usage: %s number\n",argv[0]); return 1; &#125; double inputValue = atof(argv[1]); double outputValue = sqrt(inputValue); fprintf(stdout,"The square root of %g is %g\n", inputValue, outputValue); return 0;&#125; Adding a Version Number and Configured Header File这里要添加的第一个特性是为可执行文件和项目提供一个版本号，虽然这可以在源代码中完成，但在CMakeLists.txt中添加更加具有灵活性，只需简单修改CMakeLists.txt文件便可以添加一个版本号，这里将其做如下修改： 123456789101112131415161718cmake_minimum_required (VERSION 2.6)project (Tutorial)# The version number.set (Tutorial_VERSION_MAJOR 1)set (Tutorial_VERSION_MINOR 0)# configure a header file to pass some of the CMake settings to the source codeconfigure_file ( "$&#123;PROJECT_SOURCE_DIR&#125;/TutorialConfig.h.in" "$&#123;PROJECT_BINARY_DIR&#125;/TutorialConfig.h" )# add the binary tree to the search path for include files# so that we will find TutorialConfig.hinclude_directories("$&#123;PROJECT_BINARY_DIR&#125;")# add the executableadd_executable(Tutorial tutorial.cxx) 由于配置文件将被写入二进制树(binary tree)，所以必须将该文件路径添加到搜索路径中以包含这些文件。然后在源代码树(source tree)中创建一个包含以下内容的TutorialConfig.h.in文件： 123// the configured options and settings for Tutorial#define Tutorial_VERSION_MAJOR @Tutorial_VERSION_MAJOR@#define Tutorial_VERSION_MINOR @Tutorial_VERSION_MINOR@ 当CMake配置该头文件时，@Tutorial_VERSION_MAJOR@和@Tutorial_VERSION_MINOR@的值将会被替换为CMakeLists.txt文件中定义的值，接下来在tutorial.cxx文件中包含配置头文件以使用版本号，修改后的源代码如下所示： 1234567891011121314151617181920212223// A simple program that computes the square root of a number#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;math.h&gt;#include "TutorialConfig.h"int main (int argc, char *argv[])&#123; if (argc &lt; 2) &#123; fprintf(stdout,"%s Version %d.%d\n", argv[0], Tutorial_VERSION_MAJOR, Tutorial_VERSION_MINOR); fprintf(stdout,"Usage: %s number\n",argv[0]); return 1; &#125; double inputValue = atof(argv[1]); double outputValue = sqrt(inputValue); fprintf(stdout,"The square root of %g is %g\n", inputValue, outputValue); return 0;&#125; 主要的改变是添加了头文件TutorialConfig.h的包含，并将版本号作为使用信息的一部分输出到屏幕。 Adding a Library (Step 2)现在需要添加一个库到项目中，这个库包含了平方根计算的实现，可执行文件便可以使用这个库，而不去使用编译器提供的平方根计算函数。本教程将把这个库放在一个叫MathFunctions的子文件夹中，CMakeLists.txt文件需要有下面这样一行：1add_library(MathFunctions mysqrt.cxx) 源文件mysqrt.cxx有一个叫mysqrt的函数，拥有跟编译器自带的sqrt函数同样的功能，为了使用这个新库，需要在CMakeLists.txt文件的上一层添加add_subdirectory的调用，以让该库得到构建。此外还添加一个包含路径来让该函数能够找到MathFunctions/MathFunctions.h头文件，最后一点需要改变的是添加这个新的库到可执行文件。现在CMakeLists.txt文件中的最后几行如下所示：123456include_directories ("$&#123;PROJECT_SOURCE_DIR&#125;/MathFunctions")add_subdirectory (MathFunctions)# add the executableadd_executable (Tutorial tutorial.cxx)target_link_libraries (Tutorial MathFunctions) 接下来考虑将MathFunctions库设置为可选的，在本教程中虽然没有必要这样做，但在使用更大的库，或者库依赖第三方代码时可能需要这样去做。首先需要在CMakeLists.txt文件的前面添加一个选项。 123# should we use our own math functions?option (USE_MYMATH "Use tutorial provided math implementation" ON) 这个选项将在CMake GUI中出现，且默认值为ON，并且用户可以自己修改这个值。这个设置会被存储在缓存中，所以用户不用在每次用CMake运行这个项目时都来设置。接下来需要进一步将关于MathFunctions库的构建和链接设置为可选，只需将CMakeLists.txt文件按如下示例更改：1234567891011# add the MathFunctions library?#if (USE_MYMATH) include_directories ("$&#123;PROJECT_SOURCE_DIR&#125;/MathFunctions") add_subdirectory (MathFunctions) set (EXTRA_LIBS $&#123;EXTRA_LIBS&#125; MathFunctions)endif (USE_MYMATH)# add the executableadd_executable (Tutorial tutorial.cxx)target_link_libraries (Tutorial $&#123;EXTRA_LIBS&#125;) 这里使用USE_MYMATH的设置来决定MathFunctions是否被编译与使用，注意这里使用变量EXTRA_LIBS来收集所有可选的库来在后面链接到可执行文件，这是一个保持有许多可选部件的大项目干净的通用方法。源代码对应的改变非常直观明了： 1234567891011121314151617181920212223242526272829303132// A simple program that computes the square root of a number#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;math.h&gt;#include "TutorialConfig.h"#ifdef USE_MYMATH#include "MathFunctions.h"#endifint main (int argc, char *argv[])&#123; if (argc &lt; 2) &#123; fprintf(stdout,"%s Version %d.%d\n", argv[0], Tutorial_VERSION_MAJOR, Tutorial_VERSION_MINOR); fprintf(stdout,"Usage: %s number\n",argv[0]); return 1; &#125; double inputValue = atof(argv[1]);#ifdef USE_MYMATH double outputValue = mysqrt(inputValue);#else double outputValue = sqrt(inputValue);#endif fprintf(stdout,"The square root of %g is %g\n", inputValue, outputValue); return 0;&#125; 在源代码中同样使用USE_MYMATH，它是CMake通过配置文件TutorialConfig.h.in提供给源代码的，只需在配置文件中添加下面一行：1#cmakedefine USE_MYMATH Installing and Testing (Step 3)接下来添加安装规则(Install Rules)和测试(Testing)支持到项目中，安装规则非常直接明了，对于MathFunctions库，可以通过在MathFunctions的CMakeLists.txt文件中添加下面两行代码来安装库和头文件：12install (TARGETS MathFunctions DESTINATION bin)install (FILES MathFunctions.h DESTINATION include) 对于应用程序，通过在CMakeLists.txt文件的前面添加下面几行来安装可执行文件和配置头文件：1234# add the install targetsinstall (TARGETS Tutorial DESTINATION bin)install (FILES "$&#123;PROJECT_BINARY_DIR&#125;/TutorialConfig.h" DESTINATION include) 这便是全部内容，现在便可以构建这个教程，然后输入make install或者从IDE构建INSTALL目标，便可以安装合适的头文件、库文件和可执行文件。CMake变量CMAKE_INSTALL_PREFIX将决定这些文件安装的根目录。 添加测试同样也是一个非常简单直接的过程，通过在CMakeLists.txt文件的末尾添加一些基础测试来检验应用程序是否正常工作。 12345678910111213141516include(CTest)# does the application runadd_test (TutorialRuns Tutorial 25)# does it sqrt of 25add_test (TutorialComp25 Tutorial 25)set_tests_properties (TutorialComp25 PROPERTIES PASS_REGULAR_EXPRESSION "25 is 5")# does it handle negative numbersadd_test (TutorialNegative Tutorial -25)set_tests_properties (TutorialNegative PROPERTIES PASS_REGULAR_EXPRESSION "-25 is 0")# does it handle small numbersadd_test (TutorialSmall Tutorial 0.0001)set_tests_properties (TutorialSmall PROPERTIES PASS_REGULAR_EXPRESSION "0.0001 is 0.01")# does the usage message work?add_test (TutorialUsage Tutorial)set_tests_properties (TutorialUsage PROPERTIES PASS_REGULAR_EXPRESSION "Usage:.*number") 构建以后在命令行工具中输入ctest便可以运行所有测试，第一个测试验证程序是否能够正常运行，不会出现段错误(segfault)或者崩溃(crash)，能够返回0，这是CTest测试最基本的形式。接下来的几个测试适当地使用PASS_REGULAR_EXPRESSION测试属性来验证测试的输出结构是否包含特定的字符串。在这个例子中即验证计算的平方根是否正确，当提供错误的参数时将使用信息打印到屏幕。如果需要添加许多测试来检测不同的输入值，可以考虑定义一个下面这样的宏：12345678910#define a macro to simplify adding tests, then use itmacro (do_test arg result) add_test (TutorialComp$&#123;arg&#125; Tutorial $&#123;arg&#125;) set_tests_properties (TutorialComp$&#123;arg&#125; PROPERTIES PASS_REGULAR_EXPRESSION $&#123;result&#125;)endmacro (do_test)# do a bunch of result based testsdo_test (25 "25 is 5")do_test (-25 "-25 is 0") 这样对于do_test的每一次调用，一个新的测试便被添加到项目中，名字、输入、结果便是传入的相关参数。 Adding System Introspection (Step 4)下面考虑向项目中添加一些代码，而这些代码取决于目标平台可能没有的特性。对于这个例子，这里将根据目标平台是否有log和exp函数来添加一些代码，当然，几乎所有的平台都包含这两个函数，但这个教程假设目标平台不是常见的那些。如果平台有log函数，则直接在mysqrt函数中使用这个函数计算平方根。这里首先在CMakeLists.txt文件中利用CheckFunctionExists.cmake宏来测试这些函数的可用性：1234# does this system provide the log and exp functions?include (CheckFunctionExists)check_function_exists (log HAVE_LOG)check_function_exists (exp HAVE_EXP) 然后修改TutorialConfig.h.in文件，如果CMake在平台找到这些函数则定义相应的值，如下所示：123// does the platform provide exp and log functions?#cmakedefine HAVE_LOG#cmakedefine HAVE_EXP 很重要的一点，函数log和exp的可用性测试需要在TutorialConfig.h文件的configure_file命令调用前完成，configure_file命令立即在CMake中用当前的设置配置文件。最后在mysqrt函数中添加一个基于log和exp函数的替代实现，如果系统可以获取这两个函数，具体代码如下：12345// if we have both log and exp then use them#if defined (HAVE_LOG) &amp;&amp; defined (HAVE_EXP) result = exp(log(x)*0.5);#else // otherwise use an iterative approach . . . Adding a Generated File and Generator (Step 5)这一小节介绍怎样在一个应用程序的构建过程中添加生成的源代码，这个例子将创建一个表，表中存储一些预先计算好的平方根作为构建过程的一部分，然后将这个表编译进应用程序。首先需要一个程序来生成这个表，在MathFunctions子文件夹，一个叫MakeTable.cxx的新的源文件会完成这个工作：123456789101112131415161718192021222324252627282930313233343536// A simple program that builds a sqrt table#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;math.h&gt;int main (int argc, char *argv[])&#123; int i; double result; // make sure we have enough arguments if (argc &lt; 2) &#123; return 1; &#125; // open the output file FILE *fout = fopen(argv[1],"w"); if (!fout) &#123; return 1; &#125; // create a source file with a table of square roots fprintf(fout,"double sqrtTable[] = &#123;\n"); for (i = 0; i &lt; 10; ++i) &#123; result = sqrt(static_cast&lt;double&gt;(i)); fprintf(fout,"%g,\n",result); &#125; // close the table with a zero fprintf(fout,"0&#125;;\n"); fclose(fout); return 0;&#125; 注意这个表是通过一个有效的C++代码生成的，输出文件的文件名由传入的参数确定。接下来要添加合适的命令到MathFunctions的CMakeLists.txt文件中来构建MakeTable可执行文件，然后将其作为构建过程的一部分。下面是完成这些所需要的一些命令：123456789101112131415# first we add the executable that generates the tableadd_executable(MakeTable MakeTable.cxx)# add the command to generate the source codeadd_custom_command ( OUTPUT $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/Table.h COMMAND MakeTable $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/Table.h DEPENDS MakeTable )# add the binary tree directory to the search path for include filesinclude_directories( $&#123;CMAKE_CURRENT_BINARY_DIR&#125; )# add the main libraryadd_library(MathFunctions mysqrt.cxx $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/Table.h ) 首先，MakeTable的可执行文件跟其他可执行文件的添加方式一样，然后添加一个自定义命令(custom command)来具体说明怎样通过运行MakeTable来生成Table.h，接下来必须让CMake知道mysqrt.cxx依赖于生成的Table.h文件，具体通过将生成的Table.h文件添加到MathFunctions库的源文件代码列表中完成。此外还必须将当前的二进制路径添加到包含文件的列表中，以让Table.h文件能够被mysqrt.cxx找到并包含。当这个项目被构建时，首先会构建MakeTable可执行文件，然后运行MakeTable生成Table.h，最后再编译包含有Table.h的mysqrt.cxx来生成MathFunctions库。现在拥有这些已添加的特性的CMakeLists.txt文件如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172cmake_minimum_required (VERSION 2.6)project (Tutorial)include(CTest)# The version number.set (Tutorial_VERSION_MAJOR 1)set (Tutorial_VERSION_MINOR 0)# does this system provide the log and exp functions?include ($&#123;CMAKE_ROOT&#125;/Modules/CheckFunctionExists.cmake)check_function_exists (log HAVE_LOG)check_function_exists (exp HAVE_EXP)# should we use our own math functionsoption(USE_MYMATH "Use tutorial provided math implementation" ON)# configure a header file to pass some of the CMake settings# to the source codeconfigure_file ( "$&#123;PROJECT_SOURCE_DIR&#125;/TutorialConfig.h.in" "$&#123;PROJECT_BINARY_DIR&#125;/TutorialConfig.h" )# add the binary tree to the search path for include files# so that we will find TutorialConfig.hinclude_directories ("$&#123;PROJECT_BINARY_DIR&#125;")# add the MathFunctions library?if (USE_MYMATH) include_directories ("$&#123;PROJECT_SOURCE_DIR&#125;/MathFunctions") add_subdirectory (MathFunctions) set (EXTRA_LIBS $&#123;EXTRA_LIBS&#125; MathFunctions)endif (USE_MYMATH)# add the executableadd_executable (Tutorial tutorial.cxx)target_link_libraries (Tutorial $&#123;EXTRA_LIBS&#125;)# add the install targetsinstall (TARGETS Tutorial DESTINATION bin)install (FILES "$&#123;PROJECT_BINARY_DIR&#125;/TutorialConfig.h" DESTINATION include)# does the application runadd_test (TutorialRuns Tutorial 25)# does the usage message work?add_test (TutorialUsage Tutorial)set_tests_properties (TutorialUsage PROPERTIES PASS_REGULAR_EXPRESSION "Usage:.*number" )#define a macro to simplify adding testsmacro (do_test arg result) add_test (TutorialComp$&#123;arg&#125; Tutorial $&#123;arg&#125;) set_tests_properties (TutorialComp$&#123;arg&#125; PROPERTIES PASS_REGULAR_EXPRESSION $&#123;result&#125; )endmacro (do_test)# do a bunch of result based testsdo_test (4 "4 is 2")do_test (9 "9 is 3")do_test (5 "5 is 2.236")do_test (7 "7 is 2.645")do_test (25 "25 is 5")do_test (-25 "-25 is 0")do_test (0.0001 "0.0001 is 0.01") TutorialConfig.h.in文件如下：12345678// the configured options and settings for Tutorial#define Tutorial_VERSION_MAJOR @Tutorial_VERSION_MAJOR@#define Tutorial_VERSION_MINOR @Tutorial_VERSION_MINOR@#cmakedefine USE_MYMATH// does the platform provide exp and log functions?#cmakedefine HAVE_LOG#cmakedefine HAVE_EXP MathFunctions的CMakeLists.txt文件如下所示：1234567891011121314151617# first we add the executable that generates the tableadd_executable(MakeTable MakeTable.cxx)# add the command to generate the source codeadd_custom_command ( OUTPUT $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/Table.h DEPENDS MakeTable COMMAND MakeTable $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/Table.h )# add the binary tree directory to the search path# for include filesinclude_directories( $&#123;CMAKE_CURRENT_BINARY_DIR&#125; )# add the main libraryadd_library(MathFunctions mysqrt.cxx $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/Table.h)install (TARGETS MathFunctions DESTINATION bin)install (FILES MathFunctions.h DESTINATION include) Building an Installer (Step 6)接下来假设需要将项目发布给其他人使用，需要在众多平台提供二进制和源代码发布包，这和在前面第三节讲的安装从源代码构建得到的二进制文件还有一点区别。这个例子将讲解如何构建安装包，并支持二进制安装和包管理这些在cygwin、debian、RPMs等平台都拥有的特性。为了达到这个目的，这里使用CPack来生成各个平台的安装程序，在Chapter Packaging with CPack有介绍。具体来说，在CMakeLists.txt文件的底部需要加下面几行代码：1234567# build a CPack driven installer packageinclude (InstallRequiredSystemLibraries)set (CPACK_RESOURCE_FILE_LICENSE "$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/License.txt")set (CPACK_PACKAGE_VERSION_MAJOR "$&#123;Tutorial_VERSION_MAJOR&#125;")set (CPACK_PACKAGE_VERSION_MINOR "$&#123;Tutorial_VERSION_MINOR&#125;")include (CPack) 上面就是需要添加的东西，这里首先包含InstallRequiredSystemLibraries，这个模块将包含项目在当前平台所需要的所有运行库(runtime libraries)，接下来设置一些CPack变量，包括许可协议和项目的版本信息，版本信息利用教程前面设置的变量。最后再包含CPack模块，CPack将使用这些变量和当前系统的一些特性来设置安装程序。 下一步便使用通常的方法构建项目，然后运行CPack。要构建一个二进制发布包(binary distribution)使用如下命令：1cpack --config CPackConfig.cmake 要生成一个源代码发布包(source distribution)需要输入如下命令：1cpack --config CPackSourceConfig.cmake Adding Support for a Dashboard (Step 7)增加提交测试结果到仪表板(dashboard)的支持非常简单，前面的教程已经在项目中定义了许多测试，现在只需运行这些测试然后提交到仪表板。为了添加仪表板的支持，首先需要在CMakeLists.txt文件的前面包含CTest模块：12# enable dashboard scriptinginclude (CTest) 然后创建一个CTestConfig.cmake文件，并在里面设置这个项目在仪表板中的名字：1set (CTEST_PROJECT_NAME "Tutorial") CTest在运行时会读入这个文件。如果想创建一个简单的仪表板，可以在项目中运行CMake，然后将路径切换到二进制树目录，运行ctest –D Experimental命令，仪表板的结果会被上传到Kitware的公共仪表板。 参考 cmake-tutorial cmake-tutorial-code]]></content>
      <categories>
        <category>translation</category>
        <category>cmake</category>
      </categories>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV 图片的读取与存储]]></title>
    <url>%2F2018%2F04-07-CV-OpenCVImageIO%2F</url>
    <content type="text"><![CDATA[翻译自OpenCV 3.1.0 Docs - Image file reading and writing EnumerationsImread flags123456789101112131415enum cv::ImreadModes&#123; cv::IMREAD_UNCHANGED = -1, // 导入原始图片，包括透明度通道 cv::IMREAD_GRAYSCALE = 0, // 图片转化为单通道灰度图 cv::IMREAD_COLOR = 1, // 图片转化为三通道*BGR*图片 cv::IMREAD_ANYDEPTH = 2, // 若输入图片是16或32位深度图直接读入，否则转化位8位深度图 cv::IMREAD_ANYCOLOR = 4, // 任何形式的图片格式都直接读入 cv::IMREAD_LOAD_GDAL = 8, // 使用GDAL驱动读入图片 cv::IMREAD_REDUCED_GRAYSCALE_2 = 16, // 图片转化位单通道灰度图，尺寸缩小1/2 cv::IMREAD_REDUCED_COLOR_2 = 17, // 图片转化为三通道BGR图，尺寸缩小1/2 cv::IMREAD_REDUCED_GRAYSCALE_4 = 32, // 图片转化位单通道灰度图，尺寸缩小1/4 cv::IMREAD_REDUCED_COLOR_4 = 33, // 图片转化为三通道BGR图，尺寸缩小1/4 cv::IMREAD_REDUCED_GRAYSCALE_8 = 64, // 图片转化位单通道灰度图，尺寸缩小1/8 cv::IMREAD_REDUCED_COLOR_8 = 65 // 图片转化为三通道BGR图，尺寸缩小1/8&#125; Imwrite flags1234567891011121314enum cv::ImwriteFlags&#123; cv::IMWRITE_JPEG_QUALITY = 1, // 存为JPEG图片，质量从0到100，默认95 cv::IMWRITE_JPEG_PROGRESSIVE = 2, // 使用JPEG特性，0或1，默认FALSE cv::IMWRITE_JPEG_OPTIMIZE = 3, // 使用JPEG特性，0或1，默认FALSE cv::IMWRITE_JPEG_RST_INTERVAL = 4, // JPEG restart interval, 0 - 65535, default is 0 - no restart. cv::IMWRITE_JPEG_LUMA_QUALITY = 5, // 单独的亮度质量水平，从0到100，默认0，不使用 cv::IMWRITE_JPEG_CHROMA_QUALITY = 6,// 单独的色度质量水平，从0到100，默认0，不使用 cv::IMWRITE_PNG_COMPRESSION = 16, // 对于PNG图片， 压缩水平从0到9，值越大图片越小，默认3， cv::IMWRITE_PNG_STRATEGY = 17, // One of cv::ImwritePNGFlags, default is IMWRITE_PNG_STRATEGY_DEFAULT. cv::IMWRITE_PNG_BILEVEL = 18, // Binary level PNG, 0 or 1, default is 0. cv::IMWRITE_PXM_BINARY = 32, // For PPM, PGM, or PBM, it can be a binary format flag, 0 or 1. Default value is 1. cv::IMWRITE_WEBP_QUALITY = 64 // For WEBP, it can be a quality from 1 to 100 (the higher is the better). By default (without any parameter) and for quality above 100 the lossless compression is used.&#125; Imwrite PNG specific flags这些flags会修改PNG图片压缩的方式，并被传递到接下来zlib处理阶段。 IMWRITE_PNG_STRATEGY_FILTERED的效果force more Huffman coding and less string matching，处于IMWRITE_PNG_STRATEGY_DEFAULT和IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY之间。 IMWRITE_PNG_STRATEGY_RLE的速度几乎和IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY一样快，但对于PNG图片有更好的压缩效果。 策略参数(strategy parameter)只影响压缩率，不影响压缩后数据的正确性。 IMWRITE_PNG_STRATEGY_FIXED避免dynamic Huffman codes，对特殊的应用允许简单的解码器。 1234567891011enum cv::ImwritePNGFlags&#123; cv::IMWRITE_PNG_STRATEGY_DEFAULT = 0, // Use this value for normal data cv::IMWRITE_PNG_STRATEGY_FILTERED = 1, // Use this value for data produced by a filter (or predictor). // Filtered data consists mostly of small values with a somewhat random distribution. // In this case, the compression algorithm is tuned to compress them better. cv::IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2, // Use this value to force Huffman encoding only (no string match) cv::IMWRITE_PNG_STRATEGY_RLE = 3, // Use this value to limit match distances to one (run-length encoding) cv::IMWRITE_PNG_STRATEGY_FIXED = 4 // Using this value prevents the use of dynamic Huffman codes, // allowing for a simpler decoder for special applications&#125; Functionsimdecode &amp; imencode函数cv::imdecode从内存中的缓存读取图片，如果缓存太短或包含不合法的数据，则返回一个空矩阵，即Mat::data==NULL。注意对于彩色图片，颜色通道安装B G R的顺序存储。1234Mat cv::imdecode( InputArray buf, // input array or vector of bytes. int flags // The same flags as in cv::imread) 下面是为了方便定义的一个重载函数，区别主要是接收的参数。1234567Mat cv::imdecode( InputArray buf, int flags, Mat* dst // The optional output placeholder for the decoded matrix. // It can save the image reallocations when the function is // called repeatedly for images of the same size.) 函数cv::imencode压缩图片并将其存储在合适大小的内存缓冲区，支持的图片格式和flags描述见函数cv::imwrite。123456bool cv::imencode( const String&amp; ext, // File extension that defines the output format. InputArray img, // Image to be written. std::vector&lt;uchar&gt;&amp; buf, // Output buffer resized to fit the compressed image. const std::vector&lt;int&gt;&amp; params = std::vector&lt;int&gt;() // Format-specific parameters. See cv::imwrite and cv::ImwriteFlags.) imread &amp; imwrite函数cv::imread从给定的文件中读取并返回图片，如果图片不能被正确读取（由于文件丢失，没有读取权限，不支持或不合法的格式等等），函数返回一个空矩阵，即Mat::data==NULL。1234Mat cv::imread( const String&amp; filename, // Name of file to be loaded. int flags = IMREAD_COLOR // Flag that can take values of cv::ImreadModes) 目前支持的格式包括： Windows bitmaps - .bmp, .dib (always supported) JPEG files - .jpeg, .jpg, *.jpe (see the Notes section) JPEG 2000 files - *.jp2 (see the Notes section) Portable Network Graphics - *.png (see the Notes section) WebP - *.webp (see the Notes section) Portable image format - .pbm, .pgm, .ppm .pxm, *.pnm (always supported) Sun rasters - .sr, .ras (always supported) TIFF files - .tiff, .tif (see the Notes section) OpenEXR Image files - *.exr (see the Notes section) Radiance HDR - .hdr, .pic (always supported) Raster and Vector geospatial data supported by Gdal (see the Notes section) 注意： The function determines the type of an image by the content, not by the file extension. In the case of color images, the decoded images will have the channels stored in B G R order. On Microsoft Windows OS and MacOSX, the codecs shipped with an OpenCV image (libjpeg, libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs, and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware that currently these native image loaders give images with different pixel values because of the color management embedded into MacOSX. On Linux, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for codecs supplied with an OS image. Install the relevant packages (do not forget the development files, for example, “libjpeg-dev”, in Debian and Ubuntu*) to get the codec support or turn on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake. In the case you set WITH_GDAL flag to true in CMake and IMREAD_LOAD_GDAL to load the image, then GDAL driver will be used in order to decode the image by supporting the following formats: Raster, Vector. 函数cv::imreadmulti从给定的文件读入multi-page image并存储在一个包含Mat的std::vector对象中。12345bool cv::imreadmulti( const String &amp; filename, // Name of file to be loaded. std::vector&lt;Mat&gt; &amp; mats, // Flag that can take values of cv::ImreadModes, default with cv::IMREAD_ANYCOLOR. int flags = IMREAD_ANYCOLOR // A vector of Mat objects holding each page, if more than one.) 函数cv::imwrite将图片保存到给定的文件中，图片格式根据文件的扩展名确定，只有8-bit (or 16-bit unsigned (CV_16U) in case of PNG, JPEG 2000, and TIFF) single-channel or 3-channel (with &#39;BGR&#39; channel order) images能够使用这个函数保存。如果格式format、深度depth或通道顺序channel order不是这些，需要在保存前使用Mat::convertTo和cv::cvtColor函数将图片转化，或者使用更通用的FileStorage I/O函数将图片保存为XML或YAML格式。 1234567bool cv::imwrite( const String &amp; filename, // Name of the file. InputArray img, // Image to be saved. const std::vector&lt;int&gt; &amp; params = std::vector&lt;int&gt;() // Format-specific parameters // encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags) 使用这个函数在存储PNG图片时能够同时保存透明度alpha channel，要这样做，创建一个8-bit (or 16-bit) 4-channel image BGRA，完全透明(transparent)的像素的alpha值为0，完全不透明(opaque)的像素的alpha值为255/65535。 下面的例子展示了怎样来创建这样一个BGRA图片，并存储为PNG文件，例子同时说明了怎样设置一个压缩参数。12345678910111213141516171819202122232425262728293031323334#include &lt;opencv2/opencv.hpp&gt;using namespace cv;using namespace std;void createAlphaMat(Mat &amp;mat)&#123; CV_Assert(mat.channels() == 4); for (int i = 0; i &lt; mat.rows; ++i) &#123; for (int j = 0; j &lt; mat.cols; ++j) &#123; Vec4b&amp; bgra = mat.at&lt;Vec4b&gt;(i, j); bgra[0] = UCHAR_MAX; // Blue bgra[1] = saturate_cast&lt;uchar&gt;((float (mat.cols - j)) / ((float)mat.cols) * UCHAR_MAX); // Green bgra[2] = saturate_cast&lt;uchar&gt;((float (mat.rows - i)) / ((float)mat.rows) * UCHAR_MAX); // Red bgra[3] = saturate_cast&lt;uchar&gt;(0.5 * (bgra[1] + bgra[2])); // Alpha &#125; &#125;&#125;int main(int argv, char **argc)&#123; // Create mat with alpha channel Mat mat(480, 640, CV_8UC4); createAlphaMat(mat); vector&lt;int&gt; compression_params; compression_params.push_back(IMWRITE_PNG_COMPRESSION); compression_params.push_back(9); try &#123; imwrite("alpha.png", mat, compression_params); &#125; catch (cv::Exception&amp; ex) &#123; fprintf(stderr, "Exception converting image to PNG format: %s\n", ex.what()); return 1; &#125; fprintf(stdout, "Saved PNG file with alpha data.\n"); return 0;&#125; Reference OpenCV 3,1,0 Documentation: Image file reading and writing]]></content>
      <categories>
        <category>translation</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMakeCommands]]></title>
    <url>%2F2018%2F04-07-Cpp-CMakeCommands%2F</url>
    <content type="text"><![CDATA[翻译自cmake-commands. 注意：翻译不一定准确，内容仅供参考! 参考 cmake-commands]]></content>
      <categories>
        <category>translation</category>
        <category>cmake</category>
      </categories>
      <tags>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用dlib进行人脸关键点检测]]></title>
    <url>%2F2018%2F04-07-CV-FaceDetectionUsingDlib%2F</url>
    <content type="text"><![CDATA[代码12 结果参考 Opencv与dlib联合进行人脸关键点检测与识别 Dlib人脸关键点检测顺序 How to convert mat to array2d?]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>dlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS配置DLib]]></title>
    <url>%2F2018%2F04-06-Mac-ConfigureDlib%2F</url>
    <content type="text"><![CDATA[写在前面首先，不要使用Homebrew安装，我用brew install dlib安装后，运行例子提示错误12error: "DLIB_NO_GUI_SUPPORT is defined so you can't use the GUI code. Turn DLIB_NO_GUI_SUPPORT off if you want to use it."error: "Also make sure you have libx11-dev installed on your system" 查询相关资料说是需要安装XQuartz，可我已经通过brew cask install xquartz安装了，而且也有人说安装后还是不行，仍然报错，最后发现作者说不要通过Homebrew安装，因为Homebrew中所有的dlib版本默认都设置为不使用X11，下面是作者在GitHub-Issues回答的原话：1Don&apos;t use homebrew. Whatever copy of dlib is in homebrew is configured to not use X11. 所以需要按照官网提供的教程进行安装。 编译卸载通过Homebrew安装的dlib，命令如下：1brew uninstall dlib 因为我已经安装了XQuartz，如果没有安装，还需要通过官网安装包或者brew cast进行安装。 接下来编译dlib12345git clone https://github.com/davisking/dlib.gitcd dlib/examplesmkdir buildcmake ..cmake --build . --config Release 经过漫长的编译过程后，就可以测试其中的例子了，如人脸关键点检测例子。借助dlib官网提供的训练好的模型文件，再提供一张带人脸的图片，便可以定位人脸的68个关键点了。1./face_landmark_detection_ex shape_predictor_68_face_landmarks.dat faces/2008_002506.jpg 结果如下图所示 官网提供的模型只能定位68个关键点，不过你可以通过作者给的例子自己训练新的模型。 测试当然，你也可以在你自己的项目中使用dlib，CMakeLists.txt如下 12345678910project(hello)cmake_minimum_required(VERSION 2.8)set(CMAKE_CXX_STANDARD 11)set(CMAKE_CXX_STANDARD_REQUIRED ON)add_subdirectory(/Users/liwei/myLibs/dlib/dlib ./dlib_build)aux_source_directory(./src SRC_LIST)add_executable($&#123;PROJECT_NAME&#125; $&#123;SRC_LIST&#125;)target_link_libraries($&#123;PROJECT_NAME&#125; dlib::dlib) 参考 DLib C++ Library cmake项目引入dlib方法 Mac下dlib安装 Install dlib (the easy, complete guide) Dlib系列之在iOS中提取人脸特征点（第一篇）]]></content>
      <categories>
        <category>tutorials</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>dlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MemoryDebugVS]]></title>
    <url>%2F2018%2F04-05-VS-MemoryDebug%2F</url>
    <content type="text"><![CDATA[打开首先需要打开VS的内存查看窗口，即 QuickWatch window，快捷键是Ctrl + Alt + Q 。 查看变量内存地址查看指针地址查看对象地址查看数组地址Reference How to read the debug memory window in Visual Studio]]></content>
      <categories>
        <category>tools</category>
        <category>IDEs</category>
      </categories>
      <tags>
        <tag>vs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度相机的原理]]></title>
    <url>%2F2018%2F03-28-CG-DepthSensorPrinciple%2F</url>
    <content type="text"><![CDATA[深度相机分类 基于三角化(Triangulation)的方法 双目立体视觉(Stereo Vision)通过计算两张图片的视差来估计深度 结构光(Structured Light)方法先向场景投射散斑，通过pattern的distortion来估计视差 基于飞行时间(Time-of-Flight)的方法 激光雷达 (Light detection and ranging, LIDAR) ToF相机通过计算飞行时间来估计深度 被动测距传感双目立体视觉主动测距传感TOF相机结构光激光雷达总结参考 深度图像的获取原理 深度相机哪家强？ 可测深度摄像头（TOF Camera）原理是什么？ https://zhuanlan.zhihu.com/p/32199990 https://zhuanlan.zhihu.com/p/32375622]]></content>
      <categories>
        <category>knowledge</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
        <tag>kinect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kinect比较]]></title>
    <url>%2F2018%2F03-28-CG-KinectComparison%2F</url>
    <content type="text"><![CDATA[技术比较Kinect v1的深度传感器采用了一种称为“光编码”（Light Coding）的技术（源自以色列的PrimeSense Inc.的技术，该公司2013年被苹果收购）。该方法读取投影的红外图案(Pattern)，并从图案的变形中获取深度信息。因此，深度传感器分为投射红外图案的IR投影仪（左）和读取红外图案的IR相机（右）。此外，在深度传感器中间安装彩色摄像机。 Kinect v2的深度传感器采用称为“飞行时间”（Time of Flight, ToF）的方法（貌似来自微软收购的一家公司(3DV Systems，Canesta)），该方法从反射和返回反射红外线的时间获得深度信息。从外部看不到的深度传感器配备了红外摄像头（左侧）和投射脉冲调制红外线的投影机（右侧）。彩色摄像头在旁边一侧。 下图简单展示了“光编码”方法与“飞行时间”方法之间的区别： Kinect v1红外激光(infrared laser)发射一束光柱，然后通过衍射光栅机制分成多份光束，投影到物体的光斑被红外照相机(infrared camera)捕捉，并与已知的参考光斑比较，投影光斑和观察到的光斑用来计算深度信息。 规格比较 Items Kinect v1 Kinect v2 彩色图(Color) 640x480@30fps 1920x1080@30fps 深度图(Depth) 320x240@30fps 512x424@30fps 范围(Range) 0.8~4.0m 0.5~4.5m 传感器远离(Sensor) 结构光(Structured Light) 飞行时间(Time of Flight) 视场(Angle of View) 水平/垂直 57/43 70/60 麦克风阵列(Microphone Array) O O 人体(Body) 6 people 6 people 人体标记(BodyIndex) 2 people 6 people 关节(Joint) 20 joints/people 25 joints/people Hand State Open / Closed Open / Closed / Lasso Gesture X O Face O O Speech/Beamforming O O 彩色图 Kinect v1 640 x 480 x 24 bpp 4:3 RGB @ 30fps 640 x 480 x 16 bpp 4:3 YUV @ 15fps Kinect v2 1920 x 1080 x 16 bpp 16:9 YUY2 @ 30 fps 注：bpp代表像素深度 Kinect v1的Color Camera的分辨率仅为640x480，Kinect v2的分辨率大幅提高，能够达到1920×1080。 注：v1的要求是USB2.0，理论传输速率是60MB/s，v2是USB3.0，理论传输速率是500MB/s。 可以计算一下，以XRGB Color为例，30fps，那么每秒所需传输的数据大小为640 x 480 x 4 x 30约为35M；再加上USHORT格式的Depth Color，30fps，大小为320 x 240 x 2 x 30约为4M。总计约为40MB/s，因为带宽有限，所以在保证画面帧率稳定的情况下，分辨率只能如此，而且基本上必须独占一个USB Controller。 再算算v2的情况，Color = 1920 x 1080 x 4 x 30 约为237M，Depth = 512 x 424 x 2 x 30约为12M，总计约为250M/s。所以非USB3.0不可，否则传输不了这么大的数据量。 显而易见，Color Map是最占带宽的，其实可以通过一些其他格式，比如I420或MJPG来减少数据量，然后通过CPU或GPU来进行解压和回放。 深度图Kinect v2预览版的深度传感器的分辨率也提高到512×424，而Kinect v1是可以取640×480分辨率的深度数据，乍一看规格好像下降了，其实Kinect v1的深度传感器的物理分辨率是320x240，Up Scaling到640x480而已（注：猜测是Runtime处理的）。另外，深度传感器的方式也是从Light Coding变更为Time of Flight(TOF)。 Kinect v1 320 x 240 x 16 bpp, 13-bit depth, 3-bit玩家信息 Kinect v2 512 x 424 x 16 bpp, 13-bit depth, 3-bit玩家信息 Kinect v1的深度图有16位，其中13bit保存深度信息（包括一个标志位），另外3bit保存player information，如果player index没有被申请，则这3bit为0。12bit的深度信息能够覆盖2^12=4096mm，也就是4米的距离。 标志位存储如下信息 Too near: 0x0000 Too far: 0x7ff8 Unknown: 0xfff8 深度图像是12位的，存储格式是PNG。 JPG只能存储8位，且属于有损压缩。 PNG最大支持单通道16位，且PNG是一种无损压缩格式。 人体关节Kinect v1中可用的关节每人有20个，但Kinect v2有25个关节，如图所示有增加了 颈部(NECK) 指尖(HAND_TIP_LEFT, HAND_TIP_RIGHT) 拇指(THUMB_LEFT, THUMB_RIGHT)这五个位置。 数据采集流程比较 参考 Kinect v2 Introduction and Tutorial 目前市面上的RGBD相机对比 Kinect v1和Kinect v2之间的全面比较 翻译 - Kinect v1和Kinect v2的彻底比较 Kinect v2编程（C ++） - 深度 Kinect for Windows SDKs &gt; Kinect for Windows v1 SDK &gt; Kinect uses 12 bits or 13 bits for depth data? Kinect API Overview)]]></content>
      <categories>
        <category>knowledge</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
        <tag>kinect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows10配置DNSCrypt]]></title>
    <url>%2F2018%2F03-26-Win-Windows10DNSCrypt%2F</url>
    <content type="text"><![CDATA[使用XXNET, dropbox下载不了，看网上说是因为DNS被污染的问题，开启XX-NET，安装并使用DNSCrypt就好了。 下载dnscrypt-proxy地址：https://download.dnscrypt.org/dnscrypt-proxy/ 我选择 LATEST-win64-full.zip 并解压 下载dnscrypt-winclient地址：https://github.com/Noxwizard/dnscrypt-winclient 将LATEST-win64-full/dnscrypt-proxy-win64文件夹下的文件拷贝到 dnscrypt-winclient/binaries/Release文件夹下 运行dnscrypt-winclient右键管理员运行dnscrypt-winclient.exe，并做如下操作： 在NICs勾选要用的网络 将Config -&gt; Provider(服务器) 选择为 Cisco OpenDNS 点击Start开始。 修改DNS服务器地址将Windows网络链接TCP/IP的DNS修改为127.0.0.1 控制面板 -&gt; 网络与Internet -&gt; 网络与共享中心 -&gt; 更改适配器设置 -&gt; 选择某个本地连接 右键属性，选择Internet协议版本4(TCP/IP)，点击下面的属性。 ` 网络链接TCP/IP的首选DNS服务器修改为127.0.0.1 ` 顺便记得把 dnscrypt-winclient 里面的 Install （Install Dnscrypt as a Windows service) 点击一下，之后就不用每次重启再运行了。 参考 DNSCrypt在Windows和Ubuntu下的配置 dnscrypt怎么用？使用dnscrypt解决dns污染问题 Windows 10怎么设置IP地址与DNS怎么设置 希望加入Dropbox客户端支持 #3482]]></content>
      <categories>
        <category>tutorials</category>
        <category>miscellaneous</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown中的代码语法高亮]]></title>
    <url>%2F2018%2F03-22-MD-CodeSyntaxHighlighting%2F</url>
    <content type="text"><![CDATA[代码高亮语法Markdown中代码高亮的一般书写格式为： ```language_key if (condition) { &emsp; return true; } ``` 其中language_key填写具体的语言类型。 代码语言支持注意：不同的Markdown编辑器所支持的语言可能不同。 Language language_key 1C 1c ActionScript actionscript Apache apache AppleScript applescript AsciiDoc asciidoc AspectJ asciidoc AutoHotkey autohotkey AVR Assembler avrasm Axapta axapta Bash bash BrainFuck brainfuck Cap’n Proto capnproto Clojure REPL clojure Clojure clojure CMake cmake CoffeeScript coffeescript C++ cpp C# cs CSS css D d Dart d Delphi delphi Diff diff Django django DOS .bat dos Dust dust Elixir elixir ERB (Embedded Ruby) erb Erlang REPL erlang-repl Erlang erlang FIX fix F# fsharp G-code (ISO 6983) gcode Gherkin gherkin GLSL glsl Go go Gradle gradle Groovy groovy Haml haml Handlebars handlebars Haskell haskell Haxe haxe HTTP http Ini file ini Java java JavaScript javascript JSON json Lasso lasso Less less Lisp lisp LiveCode livecodeserver LiveScript livescript Lua lua Makefile makefile Markdown markdown Mathematica mathematica Matlab matlab MEL (Maya Embedded Language) mel Mercury mercury Mizar mizar Monkey monkey nginx nginx Nimrod nimrod Nix nix NSIS nsis Objective C objectivec OCaml ocaml Oxygene oxygene Parser 3 parser3 Perl perl PHP php PowerShell powershell Processing processing Python’s profiler output profile Protocol Buffers protobuf Puppet puppet Python python Q q R r RenderMan RIB rib Roboconf roboconf RenderMan RSL rsl Ruby ruby Oracle Rules Language ruleslanguage Rust rust Scala scala Scheme scheme Scilab scilab SCSS scss Smali smali SmallTalk smalltalk SML sml SQL sql Stata stata STEP Part 21 (ISO 10303-21) step21 Stylus stylus Swift swift Tcl tcl TeX tex Thrift thrift Twig twig TypeScript typescript Vala vala VB.NET vbnet VBScript in HTML vbscript-html VBScript vbscript Verilog verilog VHDL vhdl Vim Script vim Intel x86 Assembly x86asm XL xl 参考 markdown语法高亮支持]]></content>
      <categories>
        <category>tools</category>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Atom插件安装]]></title>
    <url>%2F2018%2F03-22-Atom-PackageInstalling%2F</url>
    <content type="text"><![CDATA[方法一：使用客户端安装使用客户端安装是最简单的方式。 点击Preference，在Settings中点击Install，然后输入需要安装的package的名字，搜索结束后点击需要安装的package栏目中的Install按钮即可。 方法二：使用apm安装apm(Atom Package Manager)是atom的包管理工具，可以方便的管理Atom的插件。 首先打开你的terminal,切换到atom的插件目录1cd ~/.atom/packages 然后使用命令 1apm install &lt;package-name&gt; 安装名称为&lt;package-name&gt;的插件 Note：如果不知道具体的插件目录，点击Preference，在Settings中点击Open Config Folder，然后在packages文件夹上右击选择Show in Finder，便能在Finder中打开插件目录。 方法三：使用git安装这种方法可能会少相关依赖，需要自行在packages里运行npm install &lt;node-package&gt;安装相关的依赖。在atom.io上(或者在atom设置界面中跳转到插件的网页)找到插件页面，点击Repo跳到插件的github仓库，将&lt;package-repo-url&gt;替换为仓库的地址，然后在packages目录下运行下列命令： 1git clone &lt;package-repo-url&gt; 参考 Atom安装插件的几个方法]]></content>
      <categories>
        <category>tools</category>
        <category>editor</category>
      </categories>
      <tags>
        <tag>atom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS配置OpenCV]]></title>
    <url>%2F2018%2F03-21-CV-macOSConfigureOpenCV%2F</url>
    <content type="text"><![CDATA[安装OpenCV库使用Homebrew安装，非常简单，一个命令搞定 1brew install opencv XCode测试新建项目新建XCode的macOS Command Line Tool项目，具体方式如图所示。 项目配置配置项目的Header Search Paths和Library Search Paths，即包含目录和库目录。 双击项目打开testOpenCV.xcodeproj，点击Build Setting，再点击下方的Basic。在Header Search Paths和Library Search Paths分别添加OpenCV的相应路径。具体路径可以通过brea info opencv获得。 测试代码测试代码通过MBP自带摄像头捕捉图像，并转化为灰度图，进行适当降燥后计算Canny算子，完整代码如下。 123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;using namespace std;using namespace cv;int main(int argc, char** argv)&#123; VideoCapture capture(0); Mat gray,edge; while(1) &#123; Mat frame; capture &gt;&gt; frame; printf("Camera capturing....\n"); cvtColor(frame, gray, CV_BGR2GRAY); //转换为灰度图 blur(gray, edge, Size(3,3)); //降噪 Canny(edge, edge, 3, 9); //运行Canny算子，3为threshold1，9为threshold2 imshow("Capture",edge); waitKey(30); &#125; return 0;&#125; 运行结果下面运行结果的一张截图 QtCreator测试CMake测试CMakeLists.txt文件 12345678910111213cmake_minimum_required(VERSION 3.9)set(PROJECT_NAME TestOpenCVCmake)project($&#123;PROJECT_NAME&#125;)set(CMAKE_CXX_STANDARD 11)find_package(OpenCV REQUIRED)include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)add_executable(TestOpenCVCmake main.cpp)target_link_libraries($&#123;PROJECT_NAME&#125; $&#123;OpenCV_LIBS&#125;) 参考 配置OpenCV3 + Python3的简易方法（macOS)]]></content>
      <categories>
        <category>tutorials</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for MacOS]]></title>
    <url>%2F2018%2F03-20-Tips4MacOS%2F</url>
    <content type="text"><![CDATA[快捷键文件/文件夹 预览文件或文件夹：选中对象，按Space 复制文件或文件夹：Command + C 粘贴文件或文件夹：Command + V 剪切文件或文件夹：Command + Option + V 显示文件或文件夹信息：选中对象，按Command + I 显示隐藏文件和文件夹：Command + Shift + . 前往任意文件夹：Finder下Command + Shift + G 复制文件夹路径：选中文件夹，按Command + Option + C即可 复制文件夹路径：按Command + C拷贝文件夹，再按Command + V粘贴到终端(Terminal) 复制文件夹路径：将文件夹拖进终端(Terminal)，可以显示路径 输入法 切换系统输入法：单击Caps Lock 切换系统输入法：按住Control，再按Space可以切换输入法 切换搜狗输入法：单击Shift 锁定大写：长按Caps Lock至绿灯亮 窗口 最小化窗口：窗口在非全屏状态下，点击Command + M 最大化窗口：按住Option，全屏按钮就会变为+，点击就是垂直最大化 最大化窗口；按住Option + Shift，全屏按钮也会变为+，点击就是最大化，而不是全屏 全屏：点击Command + Control + F进入或退出全屏 分屏：窗口在非全屏状态下，点击全屏图标不动，直至进入分屏选择状态，点击另一窗口进入分屏，或点击桌面退出分屏。 修改hostMacOS系统的hosts文件在/etc文件夹下，直接同文本编辑器打开默认是没有修改权限的。下面提供一种修改hosts文件的方法 首先打开Terminal，然后输入sudo vim /etc/hosts，然后输入电脑的密码进入hosts文件，按i键进入编辑状态，修改hosts。然后，ESC退出编辑状态，:wq保存并退出vim。 其他当前文件夹打开Terminal或VS Code[1]在Applications文件夹下，按住Command，点击Terminal或VS Code拖动到上面的工具栏，出现绿色➕号，即可将其添加到工具栏，如下图所示： 下次在Finder中将文件夹拖动到工具栏的Terminal或VS Code图标上，即可在当前文件夹下打开。 更改文件的默认打开方式[2]方法一：更改某一个文件的默认打开方式第一步：右键单击该文件。 第二步：按下Option键，你会看到Open With [打开方式]选项变成了Always Open With [始终以此方式打开]。 第三步：选择Always Open With [始终以此方式打开]列表内的某一个应用程序。 注：它仅适用于你所选择的这一个文件，并不适用于其他文件，包括同一类型的文件。 方法二：更改同类型文件的默认打开方式第一步：右键单击该文件，然后选择Get Info [显示简介]选项。 第二步：找到Open with [打开方式]项目，点击倒三角选择你想指定的默认应用程序。 第三步：单击Change All [全部更改]按钮即可生效。 注：要恢复到系统原来指定的默认应用程序，只需再次按照上述步骤，并选择原来的应用程序即可。 参考 Mac OS X: Open in Visual Studio Code 基础教程：如何更改 Mac 文件的默认打开方式]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>macOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Jekyll搭建个人博客网站]]></title>
    <url>%2F2018%2F03-15-Mis-JekyllBlog%2F</url>
    <content type="text"><![CDATA[借助jekyll-theme-next搭建博客新建GitHub仓库打开创建代码仓库页面，新建仓库并取名为your-username.github.io，这样后面你就可以通过your-username.github.io访问你的博客。 前期jekyll环境准备确保已安装Ruby 2.1.0 或更高版本1ruby --version 安装Bundler：1gem install bundler 本地安装NexT新建博客文件夹，如jekyll-theme-next。初始化本地Git仓库：1git init 下载NexT主题，这里还有更多的主题，你可以选择自己喜欢的，方法是一样的。解压后把文件拷贝到你的博客文件夹，即jekyll-theme-next。进入博客文件夹：1cd jekyll-theme-next 安装依赖：1bundle install 运行 Jekyll：1bundle exec jekyll server 此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。 推送本地代码到GitHub将本地代码推送到GitHub：1234git add *git commit -m &quot;first commit&quot;git remote add origin https://github.com/your-repositorygit push -u origin master 这里your-repository即为your-username.github.io。此时打开网址your-username.github.io便能看到你的博客。 借助七牛图床存储博客图片装载自Markdown 配置七牛云作为图床 配置七牛云注册完成之后，在资源主页中的对象存储里添加对象，如添加image，但不能为Bucket注意选择的地区可能会影响到图片能否加载成功，如果出错修改地区重试。 上传图片看如下动图：点击复制外链，粘贴到Markdown文档中即可。 借助LeanCloud统计访问人数装载自为NexT主题添加文章阅读量统计功能 注册完成LeanCloud帐号并验证邮箱之后，我们就可以登录我们的LeanCloud帐号，进行一番配置之后拿到AppID以及AppKey这两个参数即可正常使用文章阅读量统计的功能了。 配置LeanCloud创建应用 新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示： 在出现的界面点击创建应用： 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的: 这里为了演示的方便，我新创建一个取名为test的应用。创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置： 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建Class：在弹出的选项中选择创建Class来新建Class用来专门保存我们博客的文章访问量等数据:点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter: 由于LeanCloud升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。 创建完成之后，左侧数据栏应该会多出一栏名为Counter的栏目，这个时候我们点击顶部的设置，切换到test应用的操作界面:在弹出的界面中，选择左侧的应用Key选项，即可发现我们创建应用的AppID以及AppKey，有了它，我们就有权限能够通过主题中配置好的Javascript代码与这个应用的Counter表进行数据存取操作了: 复制AppID以及AppKey并在NexT主题的_config.yml文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子:1234leancloud_visitors: enable: true app_id: joaeuuc4hsqudUUwx4gIvGF6-gzGzoHsz app_key: E9UJsJpw1omCHuS22PdSpKoh 这个时候重新生成部署博客，应该就可以正常使用文章阅读量统计的功能了。需要特别说明的是：记录文章访问量的唯一标识符是文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计。 后台管理当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们LeanCloud对应的应用的Counter表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的Counter表中。 我们可以修改其中的time字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。 url字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。 title字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。 其他字段皆为自动生成，具体作用请查阅LeanCloud官方文档，如果你不知道有什么作用请不要随意修改。 Web安全因为AppID以及AppKey是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。 选择应用的设置的安全中心选项卡: 在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全: 借助Disqus添加评论支持转载自为Hexo博客加入Disqus评论 注册Disqus打开Disqus主页，注册一个账号。 配置Disqus登录后，点击首页的GET STARTED按钮，之后选择I want to install Disqus on my site选项，新建一个站点。 Websit Name： 就是等会配置文件中要填写的shortname，自己填写即可，但是要求全网唯一，设定后不可改变，比如这里假设为your-name：这个在配置jekyll的时候需要用到。 Category：选择种类，可以随便选，我这里选Tech。 Language：语言选Chinese或者English。 然后点Create Site等待界面跳转。接下来在页面的左侧点击Configure Disqus 配置jekyll打开_config.yml文件，搜索找到disqus，将配置更改为如下：1234disqus: enable: true shortname: your-name count: true 将更改推送到GitHub打开博客网址便可以看到下面出现了评论框了。 撰写博客参考 Simpleyyt/jekyll-theme-next NexT使用文档 Jekyll中文文档 用Jekyll搭建的Github Pages个人博客 Markdown 配置七牛云作为图床 为NexT主题添加文章阅读量统计功能 为Hexo博客加入Disqus评论]]></content>
      <categories>
        <category>tutorials</category>
        <category>miscellaneous</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源协议]]></title>
    <url>%2F2018%2F03-14-License%2F</url>
    <content type="text"><![CDATA[BSD开源协议（original BSD license、FreeBSD license、Original BSD license）BSD开源协议是一个给于使用者很大自由的协议。基本上使用者可以”为所欲为”,可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。 但”为所欲为”的前提当你发布使用了BSD协议的代码，或则以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件： 如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议。 如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。 不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。 BSD代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销售，因此是对 商业集成很友好的协议。而很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。 Apache Licence 2.0（Apache License, Version 2.0、Apache License, Version 1.1、Apache License, Version 1.0）Apache Licence是著名的非盈利开源组织Apache采用的协议。该协议和BSD类似，同样鼓励代码共享和尊重原作者的著作权，同样允许代码修改，再发布（作为开源或商业软件）。需要满足的条件也和BSD类似： 需要给代码的用户一份Apache Licence 如果你修改了代码，需要再被修改的文件中说明。 在延伸的代码中（修改和有源代码衍生的代码中）需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明。 如果再发布的产品中包含一个Notice文件，则在Notice文件中需要带有Apache Licence。你可以在Notice中增加自己的许可，但不可以表现为对Apache Licence构成更改。 Apache Licence也是对商业应用友好的许可。使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布/销售。 GPL（GNU General Public License）我们很熟悉的Linux就是采用了GPL。GPL协议和BSD, Apache Licence等鼓励代码重用的许可很不一样。GPL的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代 码做为闭源的商业软件发布和销售。这也就是为什么我们能用免费的各种linux，包括商业公司的linux和linux上各种各样的由个人，组织，以及商 业软件公司开发的免费软件了。 GPL协议的主要内容是只要在一个软件中使用(”使用”指类库引用，修改后的代码或者衍生代码)GPL 协议的产品，则该软件产品必须也采用GPL协议，既必须也是开源和免费。这就是所谓的”传染性”。GPL协议的产品作为一个单独的产品使用没有任何问题， 还可以享受免费的优势。 由于GPL严格要求使用了GPL类库的软件产品必须使用GPL协议，对于使用GPL协议的开源代码，商业软件或者对代码有保密要求的部门就不适合集成/采用作为类库和二次开发的基础。 其它细节如再发布的时候需要伴随GPL协议等和BSD/Apache等类似。 关于开源协议GPL V2和V3单从开源行业的GPL协议上来看，似乎开源linux产品上的一切是可以无条件的开放和共享的，但是从实际的操作来看，在GPL相对的许可授权之下，又有其相对封闭的一面，就这次的GPL v2到GPL v3的修订改版来说，正是GPL协议“封闭”一面的具体体现。 根据GPL v2的相关规定：只要这种修改文本在整体上或者其某个部分来源于遵循GPL的程序，该修改文本的整体就必须按照GPL流通，不仅该修改文本的源码必须向社 会公开，而且对于这种修改文本的流通不准许附加修改者自己作出的限制。而在GPL v3的修订草案中，不仅要求用户公布修改的源代码，还要求公布相关硬件，恰恰是这一条，由于触及和其他相关数字版权管理(DRM)及其产品的关系，并且也 由于有和开源精神相违的地方，所以备受争议，甚至因此也遭到了有着“LINUX之父”之称的托瓦尔兹的反对。 从表面上看，GPL v2到GPL v3的升级之困只不过是对协议修订过程中某一条款的分歧，而更为严重的是在两种协议都合法存在的前提下，具体的开源软件或者开源产品的所有者有权选择是遵 循GPL v2协议还是恪守GPL v3协议，因此冲突也就来了，这种冲突正如中科红旗的CTO郑忠源描述的那样：“世界有如此多软件都在GPL v2的约束之下，而自由软件是集合全世界程序员劳动，即使是贡献一行代码，如果该程序员只同意这一代码只遵循GPL v2之下，就不能随便去修改协议。如果计划将软件转移到GPL v3之下，理论上讲，必须征得所有代码人的同意。但是目前还很难确定有多少开发人员愿意转移到新版本之下，如果有的人愿意转，有的人不愿意转，这其中就有 很多的麻烦；而如果多数人都不愿意改变，那这一事情也许就无声无息……” 通过业内人士的精辟描述，相信大家一定对开源行业和开源软件产品有了一个全新的认识吧，就那熟悉的LINUX系统来说，虽然表面上看起来大家有权按 照自己的需要和目的进行任意的改写重组，但是在诸多的独立程序面前，别人是只能共享使用，而无权修改的，当然获得授权就另当别论了。而就GPL v2到GPL v3的协议升级来说，这种协议的选择上的分歧实际上也是开源行业里一种观念认知上的相左，到底谁的选择是正确的？绝对不是一两句话能说得清的，尤其是在各 种利益交织之下。 情势之下，开源社区的GPL v2与GPL v3选择之困很现实的会在相当一段时间内给这个行业及其产品造成“兼容问题”，说白了就是两种协议以及两种协议之下的矛盾，不管是人的还是产品的都将会持 续下去，而这种僵持对整个开源行业来说未必是一件好事，最起码从“精神”方面来说这个行业已经在开始分道扬镳。 LGPL（GNU Lesser General Public License）LGPL是GPL的一个为主要为类库使用设计的开源协议。和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同。 LGPL 允许商业软件通过类库引用(link)方式使用LGPL类库而不需要开源商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并 发布和销售。 但是如果修改LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议。因此LGPL协议的开源 代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。 GPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品 MIT（MIT License）MIT是和BSD一样宽范的许可协议,作者只想保留版权,而无任何其他了限制.也就是说,你必须在你的发行版里包含原许可协议的声明,无论你是以二进制发布的还是以源代码发布的. 参考 Open Source Licenses by Category 各种开源协议介绍 BSD、Apache Licence、GPL V2 、GPL V3 、LGPL、MIT 知乎：主流开源协议之间有何异同？ Choose an open source license SPDX License List]]></content>
      <categories>
        <category>knowledge</category>
        <category>miscellaneous</category>
      </categories>
      <tags>
        <tag>license</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for Homebrew]]></title>
    <url>%2F2018%2F03-13-Tips4Homebrew%2F</url>
    <content type="text"><![CDATA[更新Homebrew要获取最新的包的列表，首先得更新Homebrew自己，这可以用brew update办到。1brew update 安装包安装包的命令如下：1brew install $FORMULA # 安装指定的包 可以在Homebrew Formulas搜索你需要安装的包。比如你要安装OpenMesh库，在该网页中输入OpenMesh点击搜索，会得到一个搜索结果，点击进去，可以看到安装命令如下：1brew install open-mesh 但通过brew install安装某些包提示找不到，可能已经废弃了，如ann，通过下面命令可以安装，参考这里.1brew install brewsci/science/ann 12brew unlink opencvbrew link --force opencv@3 卸载包1brew uninstall $FORMULA # 卸载指定的包 注意：卸载一个包不会同时卸载其依赖的包。 更新包通过下面命令可以查看所有可更新的包1brew outdated 然后就可以用brew upgrade去更新了。Homebrew会安装新版本的包，但旧版本仍然会保留。12brew upgrade # 更新所有的包brew upgrade $FORMULA # 更新指定的包 清理旧版本一般情况下，新版本安装了，旧版本就不需要了。可以用brew cleanup清理旧版本和缓存文件。Homebrew只会清除比当前安装的包更老的版本，所以不用担心有些包没更新但被删了。123brew cleanup # 清理所有包的旧版本brew cleanup $FORMULA # 清理指定包的旧版本brew cleanup -n # 查看可清理的旧版本包，不执行实际操作 锁定包如果经常更新的话，brew update一次更新所有的包是非常方便的。但有时候会担心自动升级把一些不希望更新的包更新了，这时可用brew pin去锁定这个包，然后brew update就会略过它了。12brew pin $FORMULA # 锁定某个包brew unpin $FORMULA # 取消锁定 查看包信息brew info可以查看包的相关信息，包括包依赖和相应的命令。12brew info $FORMULA # 显示某个包的信息brew info # 显示安装了包数量，文件数量，和总占用空间 brew deps可以显示包的依赖关系，可以用来查看已安装的包的依赖，然后判断哪些包是可以安全删除的。1brew deps --installed --tree # 查看已安装的包的依赖，树形显示 如输出的python依赖如下：1234567python├── gdbm├── openssl├── readline├── sqlite│ └── readline└── xz 查看包列表1brew list # 列出所有安装的包 brew list命令还有一些可选参数[--unbrewed][--versions [--multiple]] [--pinned][formulae] 如果给定[formulae]参数，将列出该包的所安装的文件，如果加上--verbose参数，将进一步列出该包所有子文件夹中的文件。 如果给定--unbrewed，list all files in the Homebrew prefix not installed by Homebrew. 如果给定--versions，显示所有包或者某个包([formulae]参数指定)的版本号，如果进一步给定—multiple参数，将只显示多版本安装的包。 如果给定--pinned参数，将现实所有或者某个包被pin的包。 安装其他仓库的包一般情况下我们安装的都是主仓库(Homebrew’s master repository)中的包，但有些包可能在其他仓库，如brewsci/science和homebrew/core。这时可以通过brew tap命令做到，比如：1brew tap brewsci/science 然后你就可以直接用brew install安装brewsci/science中的包：1brew install $FORMULA 更新源如果homebrew在下载或者更新的时候速度超级慢，可以考虑更换homebrew的下载源，速度会大大提升。 中科大源12345678910111213141516# 替换 Homebrewgit -C "$(brew --repo)" remote set-url origin https://mirrors.ustc.edu.cn/brew.git# 替换 Homebrew Coregit -C "$(brew --repo homebrew/core)" remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git# 替换 Homebrew Caskgit -C "$(brew --repo homebrew/cask)" remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git# 替换 Homebrew-bottles# 对于 bash 用户：echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' &gt;&gt; ~/.bash_profilesource ~/.bash_profile# 对于 zsh 用户：echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' &gt;&gt; ~/.zshrcsource ~/.zshrc 参考LUG@USTC、知乎 清华源1234567git -C "$(brew --repo)" remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.gitgit -C "$(brew --repo homebrew/core)" remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.gitgit -C "$(brew --repo homebrew/cask)" remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.gitbrew update 参考tuna@tsinghua 恢复源1234567git -C "$(brew --repo)" remote set-url origin https://github.com/Homebrew/brew.gitgit -C "$(brew --repo homebrew/core)" remote set-url origin https://github.com/Homebrew/homebrew-core.gitgit -C "$(brew --repo homebrew/cask)" remote set-url origin https://github.com/Homebrew/homebrew-cask.gitbrew update 参考 你应该定期更新Homebrew Keeping Your Homebrew Up to Date Homebrew-OpenMesh]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>macOS</tag>
        <tag>homebrew</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS配置OpenMesh]]></title>
    <url>%2F2018%2F03-07-Mac-ConfigureOpenMesh%2F</url>
    <content type="text"><![CDATA[安装OpenMesh库使用Homebrew安装，非常简单，一个命令搞定 1brew install open-mesh 安装后可能不知道库的具体位置，通过下面命令 1brew info open-mesh 可以查看库的信息，其中就包括了库的路径，我得到的是 1/usr/local/Cellar/open-mesh/6.3 用Qt Creator测试OpenMeshCreator的安装这里不再赘述，新建控制台项目，如test。在test.pro中添加保护目录和库文件 1234INCLUDEPATH += /usr/local/Cellar/open-mesh/6.3/includeLIBS += /usr/local/Cellar/open-mesh/6.3/lib/libOpenMeshCore.aLIBS += /usr/local/Cellar/open-mesh/6.3/lib/libOpenMeshTools.a main函数中的代码如下： 12345678910111213141516171819202122232425262728293031#include &lt;QCoreApplication&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;OpenMesh/Core/IO/MeshIO.hh&gt;#include &lt;OpenMesh/Core/Mesh/PolyMesh_ArrayKernelT.hh&gt;#include &lt;OpenMesh/Core/IO/reader/OBJReader.hh&gt;#include &lt;OpenMesh/Core/IO/writer/OBJWriter.hh&gt;typedef OpenMesh::PolyMesh_ArrayKernelT&lt;&gt; MyMesh;int main(int argc, char *argv[])&#123; QCoreApplication a(argc, argv); OpenMesh::IO::_OBJReader_(); OpenMesh::IO::_OBJWriter_(); MyMesh mesh; std::string file = "bunny.obj"; if (!OpenMesh::IO::read_mesh(mesh, file)) &#123; std::cout &lt;&lt; "Failed to read mesh" &lt;&lt; std::endl; return a.exec(); &#125; if (!OpenMesh::IO::write_mesh(mesh, "out.obj")) &#123; std::cout &lt;&lt; "Failed to save mesh" &lt;&lt; std::endl; return a.exec(); &#125; return a.exec();&#125; 程序正确运行。 注意：一开始程序中没加 12#include &lt;OpenMesh/Core/IO/reader/OBJReader.hh&gt;#include &lt;OpenMesh/Core/IO/writer/OBJWriter.hh&gt; 和 12OpenMesh::IO::_OBJReader_();OpenMesh::IO::_OBJWriter_(); 运行时报错[OpenMesh::IO::_IOManager_] No reading modules available!，搜索这里发现需要加上面的内容。 在Cmake中使用OpenMesh在Cmake中使用OpenMesh，CmakeList.txt如下所示： 12345678910111213cmake_minimum_required(VERSION 3.9)project(testOpenMeshCMake)set(SOURCE_FILES main.cpp)set(CMAKE_CXX_STANDARD 11)add_executable(testOpenMeshCMake main.cpp)target_link_libraries (testOpenMeshCMake /usr/local/Cellar/open-mesh/6.3/lib/libOpenMeshTools.6.3.dylib /usr/local/Cellar/open-mesh/6.3/lib/libOpenMeshCore.6.3.dylib ) Reference OpenMesh — BrewFormulas - Homebrew Formulas [OpenMesh] A question about the Examples]]></content>
      <categories>
        <category>tutorials</category>
      </categories>
      <tags>
        <tag>openmesh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX文档多图排版]]></title>
    <url>%2F2018%2F02-16-Tex-imageTypesetting%2F</url>
    <content type="text"><![CDATA[两图并排两图竖直排放，共享标题1234567\begin&#123;figure&#125;[htbp] \centering \includegraphics[width=0.2\textwidth]&#123;number_1.jpg&#125; \\ \includegraphics[width=0.2\textwidth]&#123;number_2.jpg&#125; \caption&#123;两图竖直摆放，共享标题&#125; \label&#123;fig:ver_2figs_1cap&#125;\end&#123;figure&#125; 两图水平排放，共享标题123456789101112\begin&#123;figure&#125; \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125; \centering \includegraphics[width=2.2in]&#123;number_1.jpg&#125; \end&#123;minipage&#125; \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125; \centering \includegraphics[width=2.2in]&#123;number_2.jpg&#125; \end&#123;minipage&#125; \caption&#123;两图水平摆放，共享标题&#125; \label&#123;fig:hor_2figs_1cap&#125;\end&#123;figure&#125; 两图竖直排放，独立标题12345678910111213141516\begin&#123;figure&#125; \centering \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125; \centering \includegraphics[width=2.2in]&#123;number_1.jpg&#125; \caption&#123;标题一&#125; \label&#123;fig:ver_2figs_2cap_1&#125; \end&#123;minipage&#125; \\ \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125; \centering \includegraphics[width=2.2in]&#123;number_2.jpg&#125; \caption&#123;标题二&#125; \label&#123;fig:ver_2figs_2cap_2&#125; \end&#123;minipage&#125;\end&#123;figure&#125; 两图水平排放，独立标题1234567891011121314\begin&#123;figure&#125; \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125; \centering \includegraphics[width=2.2in]&#123;number_1.jpg&#125; \caption&#123;标题一&#125; \label&#123;fig:hor_2figs_2cap_1&#125; \end&#123;minipage&#125; \begin&#123;minipage&#125;[t]&#123;0.5\linewidth&#125; \centering \includegraphics[width=2.2in]&#123;number_2.jpg&#125; \caption&#123;标题二&#125; \label&#123;fig:hor_2figs_2cap_2&#125; \end&#123;minipage&#125;\end&#123;figure&#125; 两图水平排放，统一大标题，独立子标题1234567891011121314151617\begin&#123;figure&#125; \centering \subfigure[子标题一]&#123; \begin&#123;minipage&#125;[b]&#123;0.2\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_1.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:hor_2figs_1cap_2subcap_1&#125; &#125; \subfigure[子标题二]&#123; \begin&#123;minipage&#125;[b]&#123;0.2\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_2.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:hor_2figs_1cap_2subcap_2&#125; &#125; \caption&#123;两图水平排放，统一大标题，独立子标题&#125; \label&#123;fig:hor_2figs_1cap_2subcap&#125;\end&#123;figure&#125; 四图并排四图水平摆放，统一大标题，两两独立子标题12345678910111213141516171819\begin&#123;figure&#125; \centering \begin&#123;minipage&#125;[b]&#123;0.45\textwidth&#125; \subfigure[标题一]&#123; \includegraphics[width=0.4\textwidth]&#123;number_1.jpg&#125; \includegraphics[width=0.4\textwidth]&#123;number_2.jpg&#125; \label&#123;fig:hor_4figs_1cap_2subcap_1&#125; &#125; \end&#123;minipage&#125; \begin&#123;minipage&#125;[b]&#123;0.45\textwidth&#125; \subfigure[标题二]&#123; \includegraphics[width=0.4\textwidth]&#123;number_3.jpg&#125; \includegraphics[width=0.4\textwidth]&#123;number_4.jpg&#125; \label&#123;fig:hor_4figs_1cap_2subcap_2&#125; &#125; \end&#123;minipage&#125; \caption&#123;四图水平摆放，统一大标题，两两独立子标题&#125; \label&#123;fig:hor_4figs_1cap_2subcap&#125;\end&#123;figure&#125; 四图栅格摆放，统一大标题，两两独立子标题1234567891011121314151617181920\ref&#123;fig:grid_4figs_1cap_2subcap_2&#125;。\begin&#123;figure&#125; \centering \subfigure[标题一]&#123; \begin&#123;minipage&#125;[b]&#123;0.5\textwidth&#125; \includegraphics[width=0.4\textwidth]&#123;number_1.jpg&#125; \includegraphics[width=0.4\textwidth]&#123;number_2.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:grid_4figs_1cap_2subcap_1&#125; &#125; \subfigure[标题二]&#123; \begin&#123;minipage&#125;[b]&#123;0.5\textwidth&#125; \includegraphics[width=0.4\textwidth]&#123;number_3.jpg&#125; \includegraphics[width=0.4\textwidth]&#123;number_4.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:grid_4figs_1cap_2subcap_2&#125; &#125; \caption&#123;四图栅格摆放，统一大标题，两两独立子标题&#125; \label&#123;fig:grid_4figs_1cap_2subcap&#125;\end&#123;figure&#125; 四图栅格摆放，统一大标题，独立子标题123456789101112131415161718192021222324252627282930\begin&#123;figure&#125; \centering \subfigure[标题一]&#123; \begin&#123;minipage&#125;[b]&#123;0.3\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_1.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:grid_4figs_1cap_4subcap_1&#125; &#125; \subfigure[标题二]&#123; \begin&#123;minipage&#125;[b]&#123;0.3\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_2.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:grid_4figs_1cap_4subcap_2&#125; &#125; \\ \subfigure[标题三]&#123; \begin&#123;minipage&#125;[b]&#123;0.3\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_3.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:grid_4figs_1cap_4subcap_3&#125; &#125; \subfigure[标题四]&#123; \begin&#123;minipage&#125;[b]&#123;0.3\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_4.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:grid_4figs_1cap_4subcap_4&#125; &#125; \caption&#123;四图栅格布局摆放，统一大标题，独立子标题&#125; \label&#123;fig:grid_4figs_1cap_4subcap&#125;\end&#123;figure&#125; 并排摆放，统一大标题，独立子标题如果想要两幅并排的图片共享一个标题，并各有自己的子标题，一般有两种办法。但需要特别注意：这两种方法不互相兼容，即在同一份文档中，你只能选择其中的一种方法使用。 方法一：使用宏包subfig(不推荐)使用subfig宏包提供的\subfloat命令，需要使用宏包\usepackage{graphicx}和\usepackage{subfig}。subfloat命令缺少宽度参数，虽然可以用\hspace命令调整子图的距离，子标题却只能和子图本身一样宽，会出现折行。为了避免子标题折行，一般在\subfloat里再嵌套个minipage，因为后者是有宽度的。12345678910111213141516171819\begin&#123;figure&#125;[htbp] \centering \subfloat[子标题一]&#123; \label&#123;fig:1&#125; \begin&#123;minipage&#125;[c]&#123;0.45\textwidth&#125; \centering \includegraphics[width=\textwidth]&#123;number_1.jpg&#125; \end&#123;minipage&#125; &#125; \subfloat[子标题二]&#123; \label&#123;fig:2&#125; \begin&#123;minipage&#125;[c]&#123;0.45\textwidth&#125; \centering \includegraphics[width=\textwidth]&#123;number_2.jpg&#125; \end&#123;minipage&#125; &#125; \caption&#123;两图水平排放，统一大标题，独立子标题&#125; \label&#123;fig:2figs&#125;\end&#123;figure&#125; 方法二：使用宏包subfigure(推荐)使用subfigure宏包提供的\subfigure命令，需要使用宏包\usepackage{graphicx}和\usepackage{subfigure}，subfigure不支持\\换行，可以把minipage放在subfigure{}中，在minipage{}里换行。1234567891011121314151617\begin&#123;figure&#125; \centering \subfigure[子标题一]&#123; \begin&#123;minipage&#125;[b]&#123;0.2\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_1.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:hor_2figs_1cap_2subcap_1&#125; &#125; \subfigure[子标题二]&#123; \begin&#123;minipage&#125;[b]&#123;0.2\textwidth&#125; \includegraphics[width=1\textwidth]&#123;number_2.jpg&#125; \end&#123;minipage&#125; \label&#123;fig:hor_2figs_1cap_2subcap_2&#125; &#125; \caption&#123;两图水平排放，统一大标题，独立子标题&#125; \label&#123;fig:hor_2figs_1cap_2subcap&#125;\end&#123;figure&#125; 以上显示在单列。如果希望跨列显示，并且修改相应的宽度参数。 ReferenceLatex基础——图片位置排版技巧]]></content>
      <categories>
        <category>tools</category>
        <category>latex</category>
      </categories>
      <tags>
        <tag>latex</tag>
        <tag>writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX的minipage命令]]></title>
    <url>%2F2018%2F02-15-Tex-minipageCommand%2F</url>
    <content type="text"><![CDATA[The minipage is often used to put things next to each other, which can otherwise be hard put together. For example, two pictures side by side, two tables next to a text or a picture or vice versa. The idea behind the minipage command is that within an existing page “built in” an additional page. By this one has the opportunity to use this new page, for example, set two pictures side by side, then just set two minipages side by side. Here than in Figure 1 is set in the first and Figure 2 in the second minipage. The command minipageWherein minipage is an environment that has a specific orientation, and a predetermined width.12345\begin&#123;minipage&#125;[adjusting]&#123;width of the minipage&#125; Text ... \ \ Images ... \ \ Tables ... \ \\end&#123;minipage&#125; AdjustmentWhen adjusting the choices is: c (for centers) t (for top) and b (for bottom). By default, c is used for centering. It is aligned by t and/or b at the highest (top line) and/or at the lowest line (bottom line). WidthThe width can be set as absolute or as relative value. That means one can indicate 0.2 to the 6cm or 60mm or 0.3\textwidth or \textwidth as value for the width can. Whereby one must note here that the widths of several minipages those do not lay next to each other than the width of the text are larger, since otherwise everything cannot be indicated. Further optionsBesides there are still further options, which however in practical application the minipage does not play a role like the height and the adjustment (again c, t and b) within the minipage. Example of further options1\begin&#123;minipage&#125;[t][5cm][b]&#123;0.5\textwidth&#125; This minipage now has a defined height of 5cm, and the content will now be aligned to the bottom of the minipage.Hint A mistake that is often made is, there is a blank line between the \end{minipage} and \begin{minipage} left. Then the pages are no longer together. Examples minipagePut three pictures side by side, where you should set the width of the image using width = \textwidth, if they are too wide.123456789\begin&#123;minipage&#125;[t]&#123;0.3\textwidth&#125;\includegraphics[width=\textwidth]&#123;pic1&#125;\end&#123;minipage&#125;\begin&#123;minipage&#125;[t]&#123;0.3\textwidth&#125;\includegraphics[width=\textwidth]&#123;pic2&#125;\end&#123;minipage&#125;\begin&#123;minipage&#125;[t]&#123;0.3\textwidth&#125;\includegraphics[width=\textwidth]&#123;pic3&#125;\end&#123;minipage&#125; With minipage also text can be put next to an image, this also can be implemented by the usepackage sidecap.12345678910\begin&#123;minipage&#125;&#123;0.5\textwidth&#125;\includegraphics[width=\textwidth]&#123;pic1&#125;\end&#123;minipage&#125;\begin&#123;minipage&#125;&#123;0.5\textwidth&#125;on pic 1 you find the word pic 1\\on pic 1 you find the word pic 1\\on pic 1 you find the word pic 1\\on pic 1 you find the word pic 1\\on pic 1 you find the word pic 1\ \vspace&#123;2.5cm&#125;\\\end&#123;minipage&#125; Two tables next to each other:1234567891011121314151617181920\begin&#123;minipage&#125;&#123;0.2\textwidth&#125;\begin&#123;tabular&#125;&#123;|c|c|c|&#125;\hline A &amp; B &amp; C \\\hline 1 &amp; 2 &amp; 3 \\\hline 4 &amp; 5 &amp; 6 \\\hline\end&#123;tabular&#125;\end&#123;minipage&#125;\begin&#123;minipage&#125;&#123;0.2\textwidth&#125;\begin&#123;tabular&#125;&#123;c|c|c&#125; A &amp; B &amp; C \\\hline 1 &amp; 2 &amp; 3 \\\hline 4 &amp; 5 &amp; 6 \\\end&#123;tabular&#125;\end&#123;minipage&#125; ReferenceLatex minipage]]></content>
      <categories>
        <category>tools</category>
        <category>latex</category>
      </categories>
      <tags>
        <tag>latex</tag>
        <tag>writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for Bat]]></title>
    <url>%2F2018%2F01-06-Tips4Bat%2F</url>
    <content type="text"><![CDATA[打开文件夹 1start F:/Folder/ 设置变量 注意在设置变量时不要在变量名和变量值直接加空格，SET foo = bar 是错误的，应该是 SET foo=bar。 12345echo offset var=1echo %var%set image_%%05d The /A switch supports arthimetic operations during assigments. This is a useful tool if you need to validated that user input is a numerical value. 12SET /A four=2+24 注释 REM或者 ::都可以注释！ 12REM this is a comment!:: This is also a conment! 参考 Guide to Windows Batch Scripting]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tips for Git]]></title>
    <url>%2F2018%2F01-06-Tips4Git%2F</url>
    <content type="text"><![CDATA[Git名词 master: 默认开发分支 origin: 默认远程版本库 Index / Stage：暂存区 Workspace：工作区 Repository：仓库区（或本地仓库） Remote：远程仓库 管理文件丢弃未追踪文件12git clean -f # 删除 untracked filesgit clean -fd # 删除 untracked 文件和目录 在用上述git clean前，强烈建议==加上-n参数==来先看看会删掉哪些文件，防止重要文件被误删，即12git clean -nfgit clean -nfd 然后再执行删除命令。 丢弃工作区修改丢弃针对工作区文件的修改，使文件恢复到未修改前的状态 12git checkout -- &lt;file&gt; # restore &lt;file&gt;git checkout . # restore all files, 不包括untracked files 丢弃暂存区修改使用如下命令可以把暂存区的修改撤销掉（unstage file），重新放回工作区123git reset # unstage all the filesgit reset &lt;file&gt; # unstage file, 重置暂存区的指定文件，与上一次commit保持一致git reset HEAD &lt;file&gt; # unstage &lt;file&gt; only 丢弃本次提交如果git commit之后后悔了，可以通过下面命令撤销操作123git reset --soft HEAD^ # 撤销commit，仍然保留更改git reset --mixed HEAD^ # 撤销commit、add，仍然保留更改git reset --hard HEAD^ # 撤销commit、add，丢弃更改 如果git commit注释写错了，只是想改一下注释，只需要：1git commit --amend 此时会进入默认vim编辑器，修改注释完毕后保存就好了。 删除远程仓库文件夹在更新本地.gitignore文件后，可能会忽略本地的某个文件夹，但此时该文件夹可能已经存在于远程仓库了，此时需要删除掉远程仓库中的对应文件夹 123git rm -r --cached some-directorygit commit -m "Remove the now ignored directory some-directory"git push origin master 版本回退12git reset --hard HEAD^ # 回退到上个版本git reset --hard commit_id # 退到/进到指定commit_id 更新/合并代码远程 -&gt; 本地克隆远程仓库到本地12git clone repo-url # Clone the repositorygit clone --recursive repo-url # Clone the repository with submodules 更新子模块：12git submodule update --init --recursivegit submodule update --recursive --remote 有时候clone的时候老是出现错误error: RPC failed; curl transfer closed with outstanding read data remaining或者错误remote end hung up unexpectedly，这可能是由于远程仓库太大，这时候可以先进行shallow clone，即只拷贝最后一次递交的代码，然后再更新整个git历史： 123git clone http://github.com/large-repository --depth 1cd large-repositorygit fetch --unshallow # update the repository with its history. 合并远程分支到本地1234(master)$ git fetch origin master:temp # 从远程origin仓库的master分支下载到本地并新建一个分支temp(master)$ git diff temp # 比较本地的仓库和远程仓库的区别(master)$ git merge temp # 合并temp分支到master分支(master)$ git branch -d temp # 如果不想要temp分支了，可以删除此分支 拉取远程分支到本地拉取远程分支到本地，并在本地创建分支12git checkout -b temp1 origin/temp2 # 拉取远程temp1分支到本地创建temp2分支，并切换到temp2分支git fetch origin temp1:temp2 # 拉取远程temp1分支到本地创建temp2分支 本地 -&gt; 远程推送本地分支到远程1(dev)$ git push origin dev:dev # 推送本地dev分支到远程dev分支 本地仓库连接远程仓库本地仓库首次连接远程仓库123456cd existing_folder # 进入本地仓库文件夹git init # 初始化git，生成.git隐藏文件夹git remote add origin https://github.com/user/repo.git # 连接远程仓库git add .git commit -m "Initial commit"git push -u origin master # 推送本地到远程 克隆仓库并推送到自己仓库首先克隆别人的仓库到本地1git clone https://github.com/other-account/other-repository.git 再在github.com创建自己的空仓库如your-account/your-repository， 将本地仓库当前的origin重命名为upstream：1git remote rename origin upstream 将origin重新设置为指向自己的远程仓库1git remote add origin https://github.com/your-account/your-repository.git 将本地仓库推送到自己的远程仓库1git push origin master 现在，origin指向你自己的仓库，upstream指向别人的仓库，创建一个你自己的分支如git checkout -b my-feature-branch，你可以推送自己的更改到仓库，也可以通过命令git pull upstream master来拉取别人仓库的更改到你的master分支。 参考 stackoverflow branch常用命令123456789git branch # 查看本地分支git branch -r # 查看远程分支git branch --all # 查看所有分支git branch [name] # 创建本地分支git checkout [name] # 切换到某分支git checkout -b [name] # 创建新分支并立即切换到该分支git branch -d [name] # 删除已有分支，-d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项git merge [name] # 合并分支，将名称为[name]的分支与当前分支合并git push origin [name] # 创建远程分支(本地分支push到远程) 创建分支在本地仓库创建分支1git checkout -b temp # 新建并切换到temp分支 如果想在本地仓库新建一个空白分支123git checkout --orphan tmep # 新建并切换到temp分支# 但此时分支中的文件处于staged，未commit的状态，需要清空indexgit reset --hard # 删除分支123git branch -d temp # 删除本地temp分支git branch -D temp # 强制删除本地temp分支git push origin --delete temp # 删除远程temp分支 tag123456git tag # 查看版本git tag [name] # 创建版本 git tag -d [name] # 删除版本git tag -r # 查看远程版本git push origin [name] # 创建远程版本(本地版本push到远程)git push origin :refs/tags/[name] # 删除远程版本 gitignore更新.gitignore文件 123git rm -r --cached .git add -Agit commit -m "update .gitignore" submodule添加/更新子模块1234git submodule add [url] [path]# 如git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libsgit submodule init # 初始化子模块，只在首次检出仓库时运行一次就行git submodule update #更新子模块，每次更新或切换分支后都需要运行一下 删除子模块删除项目中的一个子模块分四步走：1) git rm --cached [path]2) 编辑“.gitmodules”文件，将子模块的相关配置节点删除掉3) 编辑“.git/config”文件，将子模块的相关配置节点删除掉4) 手动删除子模块残留的目录 其他命令12git statusgit ls-files # 查看git追踪的文件 下载GitHub仓库的某个文件夹到本地 12SUBDIR=foosvn export https://github.com/google-research/google-research/trunk/$SUBDIR GitHub 提交Pull Request首先 folk 开源仓库到自己 GitHub 仓库，然后 clone 自己的仓库到本地，更改并 commit，push 到自己的远程分支，在 GitHub 网页发起 Pull Request。 Ref: 如何给开源项目贡献代码 Key Generate Key 1ssh-keygen -t rsa -C &quot;your.email@example.com&quot; -b 4096 然后提示Enter file in which to save the key，输入文件名（包括文件路径），如Windows系统一般是/c/Users/your_user_name/.ssh/id_rsa，按回车，提示Enter passphrase (empty for no passphrase):，直接回车，提示Enter same passphrase again:，再按回车，完成。 Add key 用文本编辑器打开/c/Users/your_user_name/.ssh/id_rsa，全选复制，再打开Gitlab网页端，点击右上角Setting, 点击左侧SSH Keys，粘贴， 点击Add Key，完成。 Ref Git 常用命令]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kmeans算法及其推广]]></title>
    <url>%2F2017%2F11-03-Alg-kmeans%2F</url>
    <content type="text"><![CDATA[K-means是一种经典的聚类算法。但其有两个非常大的缺陷 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。 K-Means算法需要用初始随机种子点来搞，这个随机种子点太重要，不同的随机种子点会有得到完全不同的结果。 k-means的算法有很多推广： k-means++： 该算法有效地解决了k-means算法的第二个缺陷，通过简单直观地改进：初始的聚类中心之间的相互距离要尽可能的远，很好地改进了初始聚类中心的选取。 ISODATA：ISODATA的全称是迭代自组织数据分析法，该算法有效地解决了k-means算法的第一个缺陷，通过类的自动合并和分裂，得到较为合理的类型数目k。 Kernel K-means：传统K-means采用欧式距离进行样本间的相似度度量，显然并不是所有的数据集都适用于这种度量方式。参照支持向量机中核函数的思想，该算法将所有样本映射到另外一个特征空间中再进行聚类，就有可能改善聚类效果。 k-means算法原理分析k-means算法是聚类分析中使用最广泛的算法之一。它把n个对象根据它们的属性分为k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。 给定训练样本$X=\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$，k-means算法的基本过程如下所示： (1) 任意选择$k$个初始类中心 $c_{1},c_{2},…,c_{k}$ (2) 计算$X$中的每个对象$x^{(i)}$与这些中心对象的距离，并根据最小距离重新对相应对象$x^{(i)}$进行划分确定其所属类$C^{(i)}$； C^{(i)} = \arg\min_j ||x^{(i)} - c_j||^2(3) 重新计算每个类$C_{i}​$的中心$c_{i}​$的值 C_i: c_i = \frac{1}{|C_i|} \sum_{x \in C_i} x(4) 计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则重复步骤(2)，(3)。 k-means算法的缺点k-means算法虽然简单快速，但是存在下面的缺点： 聚类中心的个数K需要事先给定，但在实际中K值的选定是非常困难的，特别是遇到高维度、海量的数据集时。 k-means算法随机地确定初始聚类中心，但不同的初始聚类中心可能导致完全不同的聚类结果。 k-means++算法原理分析k-means++算法选择初始聚类中心的基本原则是：初始的聚类中心之间的相互距离要尽可能的远。它选择初始聚类中心的步骤是： (1) 从输入的数据点集合中随机选择一个点作为第一个聚类中心$c_1$ (2) 对于数据集$X$中的每一个点$x$，计算它与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离），用$D(x)$表示，接着计算每个样本被选为下一个聚类中心的概率 $\frac{D(x)^2}{\sum_{x \ in X} D(x)^2}$，并根据概率选择出下一个聚类中心$c_{i}$ 。即选择的原则是：$D(x)$较大的点，被选取作为聚类中心的概率较大。 (3) 重复过程(2)直到找到$k$个聚类中心。 算法的关键是第(2)步，依次计算每个数据点与最近的种子点（聚类中心）的距离，依次得到$D(1),D(2),\cdots,D(m)$构成的集合$D$，其中$m$表示数据集的大小。在$D$中，为了避免噪声，不能直接选取值最大的元素，应该选择值较大的元素，然后将其对应的数据点作为种子点。如何选择值较大的元素呢，一种算法如下。 求所有的距离和Sum(D(x)) 取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其Random&lt;=0，此时的点就是下一个“种子点”。 为什么用这样的方式呢？我们换一种比较好理解的方式来说明。把集合D中的每个元素D(x)想象为一根线L(x)，线的长度就是元素的值。将这些线依次按照L(1)、L(2)、...、L(n)的顺序连接起来，组成长线L。L(1)、L(2)、…、L(n)称为L的子线。根据概率的相关知识，如果我们在L上随机选择一个点，那么这个点所在的子线很有可能是比较长的子线，而这个子线对应的数据点就可以作为种子点。 k-means++参考代码 k-means++算法的缺点虽然k-means++算法可以确定地初始化聚类中心，但是从可扩展性来看，它存在一个缺点，那就是它内在的有序性特性：下一个中心点的选择依赖于已经选择的中心点。针对这种缺陷，k-means||算法提供了解决方法。 ISODATA算法ISODATA算法在运行过程中能够根据各个类别的实际情况进行两种操作来调整聚类中心数K：(1)分裂操作，对应着增加聚类中心数；(2)合并操作，对应着减少聚类中心数。 下面首先给出ISODATA算法的输入（输入的数据和迭代次数不再单独介绍）： 预期的聚类中心数目$K_o$：虽然在ISODATA运行过程中聚类中心数目是可变的，但还是需要由用户指定一个参考标准。事实上，该算法的聚类中心数目变动范围也由$K_o$决定。具体地，最终输出的聚类中心数目范围是 $[K_o/2, 2K_o]$。 每个类所要求的最少样本数目$N_{min}$：用于判断当某个类别所包含样本分散程度较大时是否可以进行分裂操作。如果分裂后会导致某个子类别所包含样本数目小于$N_{min}$，就不会对该类别进行分裂操作。 最大方差$Sigma$：用于衡量某个类别中样本的分散程度。当样本的分散程度超过这个值时，则有可能进行分裂操作（注意同时需要满足[2]中所述的条件）。 两个类别对应聚类中心之间所允许最小距离$d_{min}$：如果两个类别靠得非常近（即这两个类别对应聚类中心之间的距离非常小），则需要对这两个类别进行合并操作。是否进行合并的阈值就是由$d_{min}$决定。 ISODATA算法的原理非常直观，不过由于它和其他两个方法相比需要额外指定较多的参数，并且某些参数同样很难准确指定出一个较合理的值，因此ISODATA算法在实际过程中并没有K-means++受欢迎。 参考文献 K-means聚类算法的三种改进介绍与对比 k-means、k-means++以及k-means算法分析 k-means聚类算法C++实现 K-means聚类算法 K-means++ clustering K-Means++算法]]></content>
      <categories>
        <category>algorithm</category>
        <category>math</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子序列 & 最长公共子串]]></title>
    <url>%2F2017%2F10-29-Alg-LongestCommonSubstring%26Subsequence%2F</url>
    <content type="text"><![CDATA[最长公共子序列（longest common sequence）和最长公共子串（longest common substring）不是一回事儿。什么是子序列呢？即一个给定的序列的子序列，就是将给定序列中零个或多个元素去掉之后得到的结果。什么是子串呢？给定串中任意个连续的字符组成的子序列称为该串的子串。 最长公共子序列题目分析解决LCS问题，需要把原问题分解成若干个子问题，所以需要刻画LCS的特征。设A=“a0，a1，…，am”，B=“b0，b1，…，bn”，且Z=“z0，z1，…，zk”为它们的最长公共子序列。不难证明有以下性质： 如果am=bn，则zk=am=bn，且“z0，z1，…，z(k-1)”是“a0，a1，…，a(m-1)”和“b0，b1，…，b(n-1)”的一个最长公共子序列； 如果am!=bn，则若zk!=am，蕴涵“z0，z1，…，zk”是“a0，a1，…，a(m-1)”和“b0，b1，…，bn”的一个最长公共子序列； 如果am!=bn，则若zk!=bn，蕴涵“z0，z1，…，zk”是“a0，a1，…，am”和“b0，b1，…，b(n-1)”的一个最长公共子序列。 递归公式假设我们用c[i,j]表示Xi 和 Yj 的LCS的长度（直接保存最长公共子序列的中间结果不现实，需要先借助LCS的长度）。其中$X = {x_1, \cdots, x_m}, Y ={y_1, \cdots,y_n}$, $X_i = {x_1,\cdots,x_i}, Y_j={y_1,\cdots,y_j}$。可得递归公式如下： 最长公共子串Reference 动态规划 最长公共子序列 过程图解 最长公共子序列（LCS）问题 动态规划：求最长公共子串/最长公共子序列 最长公共子序列求解：递归与动态规划方法]]></content>
      <categories>
        <category>algorithm</category>
        <category>math</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++结构体]]></title>
    <url>%2F2017%2F09-23-Cpp-Struct%2F</url>
    <content type="text"><![CDATA[结构体类型结构体对象结构体对象的定义形式定义结构体对象有三种形式： 先定义结构体类型，再定义结构体对象 12345struct DATE&#123; int year, month, day;&#125;;DATE a, b; // C++的方式struct DATE c, d; // C语言的方式 定义结构体类型的同时定义结构体对象 123struct DATE&#123; int year, month, day;&#125;d1, d2; 直接定义结构体对象 123struct &#123; int year, month, day;&#125;d1, d2; 结构体对象的内存形式结构体各成员是根据在结构体定义时出现的顺序依次分配空间的，其内存长度是各个成员内存长度之和，推荐使用sizeof运算，由编译器自动确定内存长度。 注意：在有的编译器中，sizeof得到的结构体内存长度可能比理论值大。如：123456789101112struct A&#123; int a; // 4 字节 char b; // 1 字节 short c; // 2 字节&#125;;struct B&#123; char b; // 1 字节 int a; // 4 字节 short c; // 2 字节&#125;; 这两个结构体类型成员相同（仅顺序不同），理论上它们的内存长度都是4+1+2=7。但实际上sizeof(A)的结构为8，sizeof(B)的结构为12。 原因：为了加快数据存储的速度，编译器默认情况下会对结构体成员和结构体本身（其他数据成员也是如此）存储位置进行处理，使其存放的起始位置是一定字节数的倍数，而不是顺序存放，称为字节对齐。 设对齐字节数为n(n=1, 2, 4, 8, 16)，每个字节内存长度为Li,Max(Li)为最大的成员内存长度。字节对齐的规则是： 结构体对象的起始位置能够被Max(Li)所整除； 结构体中每个成员相对于起始地址的偏移量，即对齐值应是min(n, Li)的倍数。若不满足对齐值的要求，编译器会在成员之间填充若干个字节，称为internal padding； 结构体的总长度值应是min(n, Max(Li))的倍数，若不满足总长度的要求，编译器在为最后一个成员分配空间后，会在其后填充若干个字节，称为trailing padding。 VC默认的对齐字节数为n=8，则A与B的内存长度分析如下： A的第一个成员a为int，对齐值min(n, sizeof(int))=4，成员a相对于结构体起始地址从0偏移开始，满足4字节对齐要求； 第二个成员b为char，对齐值min(n, sizeof(char))=1，b紧接着a后面从偏移4开始，满足1字节对齐要求； 第三个成员c为short，对齐值min(n, sizeof(short))=2，如果c紧接着b后面从偏移5开始就不满足2字节对齐要求，因此需要补充一个字节，从偏移6开始存储。 结构体A的内存长度为4+1+1(补充)+2=8。 B的第一个成员b为char，对齐值min(n, sizeof(char))=1，成员b相对于结构体起始地址从0偏移开始，满足1字节对齐要求； 第二个成员a为int，对齐值min(n, sizeof(int))=4，如果a紧接着b后面从偏移1开始，不满足4字节对齐要求，因此补充3个字节，从偏移4开始存储； 第三个成员c为short，对齐值min(n, sizeof(short))=2，c紧接着a后面从偏移8开始，满足2字节对齐要求。 则当前总的内存长度为1+3(补充)+4+2=10，由于n大于最大的成员内存长度4，故结构体长度应是4的倍数，因此最后需要再补充2个字节。 结构体B的内存长度为1+3(补充)+4+2+2(补充)=12。 使用预处理命令#progma pack(n)可以设定对齐字节数n(n=1, 2, 4, 8, 16)。例如：123456789#progma pack(push) // 保存对齐字节数#progma pack(1) // 设定对齐字节数为1struct A&#123; int a; // 4 字节 char b; // 1 字节 short c; // 2 字节&#125;;#progma pack(pop) // 恢复对齐字节数 此时sizeof(A)的结果为7。 Reference C++程序设计]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++函数指针]]></title>
    <url>%2F2017%2F09-22-Cpp-FunctionPointer%2F</url>
    <content type="text"><![CDATA[指向函数可以将函数的地址赋值给函数指针变量，它要求函数指针变量必须与指向函数必须有相同的返回类型，参数个数，参数类型。例如，假设：123int max(int a, int b); int min(int a, int b); int (*p)(int a, int b); //定义函数指针变量 则1p = max; 称p指向函数max。当然也可以指向min，即它可以指向所有与它有相同的返回类型，参数个数和参数类型的函数。 通过函数指针调用函数对函数指针间接引用即是通过函数指针调用函数。通过函数指针调用函数，在实参，参数传递，返回值等方面与函数名调用相同。例如：12c = p(a, b);b = (*p)(a, b); // 等价 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;int max(int a, int b)&#123; return a&gt;b?a:b;&#125;int min(int a, int b)&#123; return a&gt;b?b:a;&#125;int main()&#123; int (*p)(int a, int b); // 定义函数指针变量 p = max; // p指向max函数 cout &lt;&lt; p(3, 4) &lt;&lt; " "; // 通过p调用函数，输出 4 p = min; // p指向min函数 cout &lt;&lt; p(3, 4) &lt;&lt; " "; // 通过p调用函数，输出 3 return 0;&#125; 函数指针作为参数指向函数的指针多用于指向不同的函数，从而可以利用指针变量调用不同函数，相当于将函数调用由静态方式（固定地调用指定函数）变为动态方式（调用哪个函数由指针值来确定）。 如下面的例子：123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;cmath&gt;using namespace std;double integral(double a, double b, double (*f)(double x)) //求定积分&#123; int n = 1000, i; int h, x, s = 0.0; h = (b-a)/n; for (i = 0; i &lt;= n; i++) &#123; x = a + (i-1)*h; s = s + (f(x) + f(x+h)) * h / 2; // 调用 f 函数求 f(x) 与 f(x+h) &#125; return s;&#125;double f1(double x)&#123; return x + 1;&#125;double f2(double x)&#123; return exp(-x*x/2);&#125;double f3（double x)&#123; return x*x*x;&#125;int main()&#123; double a = 0.0, b = 1.0; cout &lt;&lt; integral(a, b, f1) + integral(a, b, f2) + integral(a, b, f3) &lt;&lt; endl; // 输出2.60562 return 0;&#125; Reference C++程序设计]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression模型]]></title>
    <url>%2F2017%2F09-21-ML-LogisticRegression%2F</url>
    <content type="text"><![CDATA[Reference Logistic Regression 模型简介]]></content>
      <categories>
        <category>algorithm</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的权重初始化]]></title>
    <url>%2F2017%2F09-18-ML-WeightInitialization%2F</url>
    <content type="text"><![CDATA[文章转载自知乎聊一聊深度学习的weight initialization。 Weight Initialization matters!!!深度学习中的weight initialization对模型收敛速度和模型质量有重要影响！ 在ReLU activation function中推荐使用Xavier Initialization的变种，暂且称之为He Initialization： 12import numpy as npW = np.random.randn(node_in, node_out) / np.sqrt(node_in / 2) 使用Batch Normalization Layer可以有效降低深度网络对weight初始化的依赖： 123import tensorflow as tf# put this before nonlinear transformationlayer = tf.contrib.layers.batch_norm(layer, center=True, scale=True, is_training=True) 实验代码请参见feixia586 背景深度学习模型训练的过程本质是对weight（即参数 W）进行更新，这需要每个参数有相应的初始值。有人可能会说：“参数初始化有什么难点？直接将所有weight初始化为0或者初始化为随机数！” 对一些简单的机器学习模型，或当optimization function是convex function时，这些简单的方法确实有效。然而对于深度学习而言，非线性函数被疯狂叠加，产生如本文题图所示的non-convex function，如何选择参数初始值便成为一个值得探讨的问题 —- 其本质是初始参数的选择应使得objective function便于被优化。事实上，在学术界这也是一个被actively研究的领域。 TLDR里已经涵盖了本文的核心要点，下面在正文中，我们来深入了解一下前因后果。 初始化为0的可行性？答案是不可行。 这是一道送分题 哈哈！为什么将所有W初始化为0是错误的呢？是因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的 —- gradient相同，weight update也相同。这显然是一个不可接受的结果。 可行的几种初始化方式pre-trainingpre-training是早期训练神经网络的有效初始化方法，一个便于理解的例子是先使用greedy layerwise auto-encoder做unsupervised pre-training，然后再做fine-tuning。具体过程可以参见UFLDL的一个tutorial，因为这不是本文重点，就在这里简略的说一下： pre-training阶段，将神经网络中的每一层取出，构造一个auto-encoder做训练，使得输入层和输出层保持一致。在这一过程中，参数得以更新，形成初始值 fine-tuning阶段，将pre-train过的每一层放回神经网络，利用pre-train阶段得到的参数初始值和训练数据对模型进行整体调整。在这一过程中，参数进一步被更新，形成最终模型。 随着数据量的增加以及activation function (参见我的另一篇文章) 的发展，pre-training的概念已经渐渐发生变化。目前，从零开始训练神经网络时我们也很少采用auto-encoder进行pre-training，而是直奔主题做模型训练。不想从零开始训练神经网络时，我们往往选择一个已经训练好的在任务A上的模型（称为pre-trained model），将其放在任务B上做模型调整（称为fine-tuning）。 random initialization随机初始化是很多人目前经常使用的方法，然而这是有弊端的，一旦随机分布选择不当，就会导致网络优化陷入困境。下面举几个例子。核心代码见下方 12345678910111213data = tf.constant(np.random.randn(2000, 800))layer_sizes = [800 - 50 * i for i in range(0,10)]num_layers = len(layer_sizes)fcs = [] # To store fully connected layers' outputfor i in range(0, num_layers - 1): X = data if i == 0 else fcs[i - 1] node_in = layer_sizes[i] node_out = layer_sizes[i + 1] W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01 fc = tf.matmul(X, W) fc = tf.nn.tanh(fc) fcs.append(fc) 这里我们创建了一个10层的神经网络，非线性变换为tanh，每一层的参数都是随机正态分布，均值为0，标准差为0.01。下图给出了每一层输出值分布的直方图。 随着层数的增加，我们看到输出值迅速向0靠拢，在后几层中，几乎所有的输出值x都很接近0！回忆优化神经网络的back propagation算法，根据链式法则，gradient等于当前函数的gradient乘以后一层的gradient，这意味着输出值x是计算gradient中的乘法因子，直接导致gradient很小，使得参数难以被更新！ 让我们将初始值调大一些： 1W = tf.Variable(np.random.randn(node_in, node_out)) 均值仍然为0，标准差现在变为1，下图是每一层输出值分布的直方图：几乎所有的值集中在-1或1附近，神经元saturated了！注意到tanh在-1和1附近的gradient都接近0，这同样导致了gradient太小，参数难以被更新。 Xavier initializationXavier initialization可以解决上面的问题！其初始化方式也并不复杂。Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。注意，为了问题的简便，Xavier初始化的推导过程是基于线性函数的，但是它在一些非线性神经元中也很有效。让我们试一下： 1W = tf.Variable(np.random.randn(node_in, node_out)) / np.sqrt(node_in) Woohoo！输出值在很多层之后依然保持着良好的分布，这很有利于我们优化神经网络！之前谈到Xavier initialization是在线性函数上推导得出，这说明它对非线性函数并不具有普适性，所以这个例子仅仅说明它对tanh很有效，那么对于目前最常用的ReLU神经元呢（关于不同非线性神经元的比较请参考这里）？继续做一下实验： 123W = tf.Variable(np.random.randn(node_in, node_out)) / np.sqrt(node_in)......fc = tf.nn.relu(fc) 前面看起来还不错，后面的趋势却是越来越接近0。幸运的是，He initialization可以用来解决ReLU初始化的问题。 He initializationHe initialization的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2： 123W = tf.Variable(np.random.randn(node_in,node_out)) / np.sqrt(node_in/2)......fc = tf.nn.relu(fc) 看起来效果非常好，推荐在ReLU网络中使用！ Batch Normalization LayerBatch Normalization是一种巧妙而粗暴的方法来削弱bad initialization的影响，其基本思想是：If you want it, just make it! 我们想要的是在非线性activation之前，输出值应该有比较好的分布（例如高斯分布），以便于back propagation时计算gradient，更新weight。Batch Normalization将输出值强行做一次Gaussian Normalization和线性变换： Batch Normalization中所有的操作都是平滑可导，这使得back propagation可以有效运行并学到相应的参数$\gamma$，$\beta$。需要注意的一点是Batch Normalization在training和testing时行为有所差别。Training时$\mu_\mathcal{B}$和$\sigma_\mathcal{B}$由当前batch计算得出；在Testing时$\mu_\mathcal{B}$和$\sigma_\mathcal{B}$应使用Training时保存的均值或类似的经过处理的值，而不是由当前batch计算。 随机初始化，无Batch Normalization：123W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01......fc = tf.nn.relu(fc) 随机初始化，有Batch Normalization： 1234W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01......fc = tf.contrib.layers.batch_norm(fc, center=True, scale=True, is_training=True)fc = tf.nn.relu(fc) 很容易看到，Batch Normalization的效果非常好，推荐使用！ 参考资料Xavier initialization是由Xavier Glorot et al.在2010年提出，He initialization是由Kaiming He et al.在2015年提出，Batch Normalization是由Sergey Ioffe et al.在2015年提出。 另有知乎网友在评论中提到了一些其他相关工作：https://arxiv.org/abs/1511.06422， https://arxiv.org/pdf/1702.08591.pdf Xavier Glorot et al., Understanding the Difficult of Training Deep Feedforward Neural Networks Kaiming He et al., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication Sergey Ioffe et al., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Standord CS231n Reference 聊一聊深度学习的weight initialization GitHub-feixia586/zhihu_material]]></content>
      <categories>
        <category>repost</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络激活函数]]></title>
    <url>%2F2017%2F09-17-ML-ActivationFunction%2F</url>
    <content type="text"><![CDATA[文章转载自知乎聊一聊深度学习的activation function。 神经网络激活函数 深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过非线性的activation function，传入到下一层神经元；再经过该层神经元的activate，继续往下传递，如此循环往复，直到输出层。正是由于这些非线性函数的反复叠加，才使得神经网络有足够的capacity来抓取复杂的pattern，在各个领域取得state-of-the-art的结果。显而易见，activation function在深度学习中举足轻重，也是很活跃的研究领域之一。目前来讲，选择怎样的activation function不在于它能否模拟真正的神经元，而在于能否便于优化整个深度神经网络。下面我们简单聊一下各类函数的特点以及为什么现在优先推荐ReLU函数。 sigmoid \sigma(x) = \frac{1}{1+e^{-x}} Sigmoid函数是深度学习领域开始时使用频率最高的activation function。它是便于求导的平滑函数，其导数为$\sigma(x)(1-\sigma(x))$，这是优点。然而，Sigmoid有三大缺点： 容易出现gradient vanishing 函数输出并不是zero-centered 幂运算相对来讲比较耗时 Gradient Vanishing优化神经网络的方法是Back Propagation，即导数的后向传递：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。 Sigmoid函数在深度网络中常常会导致导数逐渐变为0，使得参数无法被更新，神经网络无法被优化。原因在于两点： 在上图中容易看出，当$\sigma(x)$中$x$较大或较小时，导数接近0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0 Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为1/1048576。请注意这里是“至少”，导数达到最大值这种情况还是很少见的。 输出不是zero-centeredSigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。举例来讲，对$\sigma(\sum_i w_i x_i + b)$，如果所有$x_i$均为正数或负数，那么其对$w_i$的导数总是正数或负数，这会导致如下图红色箭头所示的阶梯式更新，这显然并非一个好的优化路径。深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。 幂运算相对耗时相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省。之后我们会看到，在ReLU函数中，需要做的仅仅是一个thresholding，相对于幂运算来讲会快很多。 tanh \tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} tanh读作Hyperbolic Tangent(双曲正切函数)，如上图所示，它解决了zero-centered的输出问题，然而，gradient vanishing的问题和幂运算的问题仍然存在。 ReLU (Rectified Linear Unit) \text{ReLU}(x) = \max(0, x)ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点： 解决了gradient vanishing问题 (在正区间) 计算速度非常快，只需要判断输入是否大于0 收敛速度远快于sigmoid和tanh ReLU也有几个需要特别注意的问题： ReLU的输出不是zero-centered Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: 非常不幸的参数初始化，这种情况比较少见 learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！ Leaky ReLU f(x) = \max(0.01x, x) 人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为$0.01x$而非0。另外一种直观的想法是基于参数的方法，即Parametric ReLU: $f(x) = \max(\alpha x, x)$，其中$\alpha$可由back propagation学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。 ELU (Exponential Linear Units) f(x) = \begin{cases} x, \qquad \qquad \text{if} \,x>0 \\ \alpha(e^x-1), \,\,\; \textrm{otherwise} \end{cases} ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及: 不会有Deal ReLU问题 输出的均值接近0，zero-centered 它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。 Reference 知乎 聊一聊深度学习的activation function]]></content>
      <categories>
        <category>repost</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++利用宏控制打印]]></title>
    <url>%2F2017%2F09-17-Cpp-DbgPrintf%2F</url>
    <content type="text"><![CDATA[利用宏控制打印在程序调试时，我们经常需要输出一些调试信息，当调试完毕后，就不再需要使用了。那怎么快速的在调试状态和发布状态切换呢？通常我们使用预编译加宏定义来处理这个问题，例如：123#ifdef DEBUG 调试代码#endif 如果我们使用printf来显示一些调试信息，那么每个地方都加上#ifdef和#endif就很麻烦了。我们可以定义一个DbgPrintf的函 数来专门处理这些事情，只在DbgPrintf函数内放上#ifdef和#endif就行了。但是这样代码在运行时，还是有调用一次函数的，浪费了时间。 那么可不可以利用宏定义，实现完全没有编译代码产生的宏呢？可以尝试下面的宏代码：1234567//#define MYDEBUG#ifdef MYDEBUG#define DbgPrintf printf#else#define DbgPrintf /\/DbgPrintf#endif 如果MYDEBUG已经定义了，则用printf去代替DbgPrintf了。如果MYDEBUG未定义的情形，这个宏定义实际上是将DbgPrintf定义成了//DbgPrintf，由于续行符的作用，#define定义时不会发现注释符//，但是在展开到代码之后，就成了注释符//了，也就是说，如果你原来的代码是DbgPrintf(&quot;%d&quot;,x);，经过这个宏展开后成了//DbgPrintf(&quot;%d&quot;,x)，相当于自动在前面加了注释符//。 注意： 续行符后面的/一定要顶格写，否则就不是//了。 另外，这个宏只能单独一行使用，因为它将该行后面的代码都注释掉了。 打印信息的同时输出位置通过logI输出一些程序运行信息，可以借助logInfo控制是否打印，对于警告和错误，分别通过logW和logE输出。12345678910#define logInfo#ifdef logInfo#define logI (printf("--info-- in [%d@%s] ",__LINE__, __FUNCTION__), printf) #else#define logI /\/logI#endif#define logW (printf("--warn-- in [%d@%s] ",__LINE__, __FUNCTION__), printf) #define logE (printf("--error- in [%d@%s] ",__LINE__, __FUNCTION__), printf) Reference 利用宏控制打印]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ STL vector]]></title>
    <url>%2F2017%2F09-14-Cpp-vector%2F</url>
    <content type="text"><![CDATA[C++ STL vector简介vector是一个类模板，模板本身不是类或函数（类模板和函数模板），相反可以将模板看作编译器生成类或函数的一份说明。编译器根据模板创建类或函数的过程称为实例化(instantiation)，当使用模板时，需要指出编译器把类或函数实例化成何种类型。 C++能允许大多数类型的对象作为vector的元素，但是因为引用不是对象，所以不存在包含引用的vector。 vector常见成员函数 size() 返回容器中有多少个元素 capacity() 返回容器当前已经分配的内存能容纳多少个元素 resize(n) 强迫容器改变到包含n个元素的状态 reserve(n) 强迫容器把它的容量变为至少n，前提是n不小于当前的大小 vector对象增长机制为了支持快速随机访问，vector将元素连续存储，如果没有空间容纳新元素，而不得不获取新的内存空间时，vector和string的实现通常会分配比新的空间需求更大的内存空间，然后将已有元素从旧位置移动到新空间中，然后添加新元素，析构掉旧内存中的对象并释放旧存储空间。vector和string提供了一些成员函数允许我们与它的实现中内存分配部分互动。 c.shrink_to_fit() 将capacity()减少为size()同样大小 c.capacity() 不重新分配内存，c可以保存多少元素 c.reserve(n) 分配至少能容纳n个元素的内存空间注意： shrink_to_fit()只适用于vector, string和deque capacity()和reserve只适用于vector和 string 只有当需要的内存空间超过当前容量时，reserve调用才会改变vector的容量。如果需求大于当前容量，reserve至少分配与需求一样大的内存空间（可能更大）。如果小于等于当前容量，reserve什么也不做。 在新标准库中，可以调用shrink_to_fit来要求vector, string和deque退回不需要的内存空间。但是，具体的实现可以选择忽略此请求，即==调用shrink_to_fit也并不保证一定退回内存空间==。 vector内存回收机制由于vector的内存占用空间只增不减，比如你首先分配了10,000个字节，然后erase掉后面9,999个，留下一个有效元素，但是内存占用仍为10,000个。所有内存空间是在vector析构时候才能被系统回收。empty用来检测容器是否为空的，clear可以清空所有元素。但是即使clear，vector所占用的内存空间依然如故，无法保证内存的回收。 如果需要空间动态缩小，可以考虑使用deque。如果非vector不可，可以用swap()来帮助你释放内存。具体方法如下：1234vector&lt;int&gt; nums; nums.push_back(1);nums.push_back(2);vector&lt;int&gt;().swap(nums); //或者nums.swap(vector&lt;int&gt; ()) 或者如下所示，使用一对大括号，意思一样的：12345//加一对大括号是可以让tmp退出&#123;&#125;的时候自动析构&#123; std::vector&lt;int&gt; tmp = nums; nums.swap(tmp); &#125; swap()是交换函数，使vector离开其自身的作用域，从而强制释放vector所占的内存空间，总而言之，释放vector内存最简单的方法是vector&lt;int&gt;().swap(nums)。但是如果nums是一个类的成员，不能把vector&lt;int&gt;().swap(nums)写进类的析构函数中，否则会导致double free or corruption (fasttop)的错误，原因可能是重复释放内存。 如果vector中存放的是指针，那么当vector销毁时，这些指针指向的对象不会被销毁，那么内存就不会被释放。如下面这种情况，vector中的元素时由new操作动态申请出来的对象指针：123456789void doSomething()&#123; vector&lt;int*&gt; vip; for (int i = 0; i &lt; 5; i++) vip.push_back(new int); // do something for (vector&lt;int*&gt;::iterator i = vip.begin(); i != vip.end(); ++i) delete *i;&#125; Reference C++ Primer - Chapter 3.3 &amp; 9.4 Effective STL - Item 17]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bundle Adjustment简述]]></title>
    <url>%2F2017%2F09-13-CV-BundleAdjustment%2F</url>
    <content type="text"><![CDATA[Bundle Adjustment简述在SFM (structure from motion)的计算中BA (Bundle Adjustment)作为最后一步优化具有很重要的作用，在近几年兴起的基于图的SLAM (simultaneous localization and mapping)算法里面使用了图优化替代了原来的滤波器，这里所谓的图优化其实也是指BA。其实很多经典的文献对于BA都有深深浅浅的介绍，如果想对BA的全过程做一个全面的更深层次的了解，推荐阅读 Bundle Adjustment —A Modern Synthesis. 什么是BABundle Adjustment中文译为光束法平差，还有其他一些翻译如束调整、捆集调整或者捆绑调整等等。 所谓bundle，来源于bundle of light，其本意就是指的光束，这些光束指的是三维空间中的点投影到像平面上的光束，而重投影误差正是利用这些光束来构建的，因此称为光束法强调光束也正是描述其优化模型是如何建立的。 用一句话来描述BA那就是，BA的本质是一个优化模型，其目的是最小化重投影误差。重投影也就是指的第二次投影， 第一次投影指的是相机在拍照的时候三维空间点投影到图像上 然后我们利用这些图像对一些特征点进行三角定位 (triangulation)，利用几何信息构建三角形来确定三维空间点的位置，参考对极几何 最后利用计算得到的三维点的坐标（注意不是真实的）和我们计算得到的相机矩阵（当然也不是真实的）进行第二次投影，也就是重投影 重投影误差指的就是真实三维空间点在图像平面上的投影（也就是图像上的像素点）和重投影（其实是用我们的计算值得到的虚拟的像素点）的差值，因为种种原因计算得到的值和实际情况不会完全相符，也就是这个差值不可能恰好为0，此时也就需要将这些差值的和最小化获取 最优的相机参数及三维空间点的坐标。 \min_{a_j, b_i} \sum_{i=1}^n \sum_{j=1}^m v_{ij} d(Q(a_j, b_i), x_{ij})^2其中， $n$个3D点在$m$个view（拍摄场景）中， 向量$X_{ij}$：图像j上的第i个点projection（坐标） 值$v_{ij}$：如果点i在图像j上有映射，则$v_{ij}=1$; else $v_{ij}=0$; 每个图像j由向量$a_j$参数化 每个3D点由$b_i$参数化 $Q(a_i,b_j)$:点i在图像j上的predicted projection $d(x,y)$:向量$x,y$的欧式距离 Reference Bundle Adjustment - 基于feature的3D场景重建算法 Bundle Adjustment 简述]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Softmax Regression in TensorFlow]]></title>
    <url>%2F2017%2F09-08-TF-SoftmaxReg4MNIST%2F</url>
    <content type="text"><![CDATA[Softmax RegressionTensorFlow实现Softmax Regression识别手写数字，数据集用MINIST(Mixed National Institute of Standards and Technology database)，其由几万张$28 \times 28$像素的手写数字组成，这些图片只包含灰度信息。我们的任务就是对这些图片进行分类，转成0~9共10类。 定义算法公式在我们使用多分类任务时，通常需要使用Softmax Regression模型。它的工作原理很简单，将可以判定为某类的特征相加，然后将这些特征转化为判定是这一类的概率。上述特征可以通过一些简单的方法得到，比如对所有的像素求一个加权和，而权重是模型根据数据自动学习，训练出来的。即可以表示为 feature_i = \sum_i W_{ij}x_j + b_i然后对所有特征计算softmax，即 y = softmax(feature)其中softmax函数先对变量计算一个$\exp$函数，然后进行标准化（让所有类别输出的概率值和为1）。即 softmax(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}所以整个模型可以表示为 y = softmax(Wx+b) 其TensorFlow的实现是123456import tensorflow as tfsess = tf.InteractiveSession()x = tf.placeholder(tf.float32, [None, 784])W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))y = tf.nn.softmax(tf.matmul(x, W) + b) 首先载入TensorFlow库，并创建一个InteractiveSession，使用这个命令会将这个session注册为默认的session，之后的运算默认跑在这个session里，不同session之间的数据和运算应该都是互相独立的。接下来创建一个784]```代表tensor的shape，这里None指不限条数的输入，784代表每条输入是一个784维的向量。接下来创建模型的weights和bias变量，```Variables```是用来存储模型参数的，不同于存储数据的tensor一旦使用掉就会消失，```Variables```在模型训练迭代中是持久化的，她可以长期存在并在每轮迭代中被更新。1234567891011121314## 定义损失函数为了训练模型，一般需要定义一个 Loss Function来描述模型对问题的分类精度。Loss越小，代表模型的分类结果与真实值的偏差越小，也就是说模型越精确。训练的目的就是不断将这个Loss减少，达到一个全局最优或者局部最优解。对于多分类问题，通常使用 Cross-entropy 作为 Loss Function，用它来判断模型对真是概率分布估计的准确程度。其定义如下$$H_&#123;y&apos;&#125;(y)= - \sum y&apos;_i \log(y_i)$$在TensorFlow中，定义Cross-entropy的代码如下```pythony_ = tf.placeholder(tf.float32, [None, 10])cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1])) 先定义一个123456789## 选择优化方法并训练模型有了算法定义和损失函数，定义一个优化方法便可以开始训练。这里采用随机梯度下降(Stochastic Gradient Descent, SGD)算法。TensorFlow用SGD训练模型的的实现是```pythontrain_step = tf.train.GradientDescentOptimizier(0.5).moinimize(cross_entropy)tf.global_variables_initializer().run()for i in rang(1000): batch_xs, batch_ys = mnist.train.next_batch9100） train_step.run(&#123;x: batch_xs, y_: batch_ys&#125;) TensorFlow可以根据我们定义的整个计算图自动求导，并根据反向传播(Back Propagation)算法进行训练，在每一轮迭代时更新参数来减少Loss。这里每次都随机从训练集中抽取100条样本构成一个mini-batch，并feed给1234567## 评测算法完成训练后，还需要对模型的准确率进行验证。```pythoncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))print(accuracy.eval(&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 将测试数据的特征和Label输入评测流程Regression在MNIST数据进行分类识别，在测试集上的平均准确率可达92%左右。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364## 完整代码```python&quot;&quot;&quot;A very simple MNIST classifier.See extensive documentation athttps://www.tensorflow.org/get_started/mnist/beginners&quot;&quot;&quot;from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport argparseimport sysfrom tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfFLAGS = Nonedef main(_): # Import data mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True) # Create the model x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.matmul(x, W) + b # Define loss and optimizer y_ = tf.placeholder(tf.float32, [None, 10]) # The raw formulation of cross-entropy, # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), reduction_indices=[1])) # can be numerically unstable. # # So here we use tf.nn.softmax_cross_entropy_with_logits on the raw # outputs of &apos;y&apos;, and then average across the batch. cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)) train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) sess = tf.InteractiveSession() tf.global_variables_initializer().run() # Train for _ in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) # Test trained model correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser() parser.add_argument(&apos;--data_dir&apos;, type=str, default=&apos;./mnist/input_data&apos;, help=&apos;Directory for storing input data&apos;) FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) Reference TensorFlow: MNIST For ML Beginners]]></content>
      <categories>
        <category>algorithm</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Doxygen生成程序文档]]></title>
    <url>%2F2017%2F09-07-Cpp-DocCodeWithDoxygen%2F</url>
    <content type="text"><![CDATA[生成配置文件在项目中新建doc文件夹，在doc文件夹中打开Power Shell（Shift+右键，选择在此处打开Power Shell窗口），键入1doxygen -g 会在当前目录下生成Doxyfile文件，该文件是Doxygen工程的配置文件，里面含有一些配置信息。 修改配置信息 修改 PROJECT_NAME 等其他一些有关的项目信息； 设置 INPUT ,这个标记创建一个以空格分隔的所有目录的列表，这个列表包含需要生成文档的 C/C++ 源代码文件和头文件。例如，请考虑以下代码片段：INPUT = ../src ../include如果源代码层次结构是嵌套的，而且需要为所有层次上的 C/C++ 文件生成文档，就把 RECURSIVE 标记设置为 Yes.如果注释中含有中文，将 INPUT_ENCODING 从默认的UTF-8改为GBK。 设置 OUTPUT_DIRECTORY 即文档的输出目录； 将 GENERATE_LATEX 从默认的 YES 改为 NO，接下来，修改HTML的显示方式，将 DISABLE_INDEX，GENERATE_TREEVIEW 从默认的 NO 均改为 YES。 将 EXTRACT_ALL 从默认的NO改为YES，这个标记告诉 doxygen，即使各个类或函数没有文档，也要提取信息。必须把这个标记设置为 Yes。 将 EXTRACT_PRIVATE 标记设置为Yes。否则，文档不包含类的私有数据成员。 将 EXTRACT_STATIC 标记设置为 Yes。否则，文档不包含文件的静态成员（函数和变量）。 如果需要中文文档，将 OUTPUT_LANGUAGE 标记设置为Chinese。 如果要添加公式支持，将 USE_MATHJAX 标记设置为YES。 生成文档在Power Shell中键入1doxygen .\Doxyfile 配置文件参考Doxyfile Example 常用命令 命令 @file 档案的批注说明 @author 作者的信息 @brief 用于class 或function的简易说明 @param 主要用于函数说明中，后面接参数的名字，然后再接关于该参数的说明 @return 描述该函数的返回值情况 @retval 描述返回值类型 @note 注解 @attention 注意 @warning 警告信息 @enum 引用了某个枚举，Doxygen会在该枚举处产生一个链接 eg：@enum CTest::MyEnum @var 引用了某个变量，Doxygen会在该枚举处产生一个链接 eg：@var CTest::m_FileKey @class 引用某个类，格式：@class [] [] eg: @class CTest “inc/class.h” @exception 可能产生的异常描述 eg: @exception 本函数执行可能会产生超出范围的异常 字体 word``` 用斜体italics强调单词word，如果要让多个单词斜体，用 ```multiple words```1- ```\b word``` 用黑体bold强调单词word，如果要让多个单词黑体，用 ```&lt;b&gt;multiple words&lt;/b&gt; word``` 用打印机字体typewriter显示单词word，一般用来引用代码中的单词。 跟```word```等价，如果有多个单词，用```word```。123456 - 3. 公式 - 注意要将*USE_MATHJAX* 标记设置为YES， - 行内公式：将公式写在一对```\f$```之间。 eg: ```The distance between \f$(x_1,y_1)\f$ and \f$(x_2,y_2)\f$ is \f$\sqrt&#123;(x_2-x_1)^2+(y_2-y_1)^2&#125;\f$. 行间公式：将公式写在一对1eg: \f[ |I_2|=\left| \int_{0}^T \psi(t) \left\{ u(a,t)- \int_{\gamma(t)}^a \frac{d\theta}{k(\theta,t)} \int_{a}^\theta c(\xi)u_t(\xi,t)\,d\xi \right\} dt \right| ] ``` 参考Doxygen-Including formulas 表格 参考Doxygen-Including tables 图片 参考Doxygen-Graphs and diagrams Reference Doxygen Manual Doxygen Special Commands Doxygen HTML Commands 使用doxygen为C/C++程序生成中文文档 Doxygen 10 分钟入门教程 学习用 doxygen 生成源码文档]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICP算法]]></title>
    <url>%2F2017%2F08-29-Alg-ICP%2F</url>
    <content type="text"><![CDATA[ICP（Iterative Closest Point），即迭代最近点算法，是经典的数据配准算法。其特征在于，通过求取源点云和目标点云之间的对应点对，基于对应点对构造旋转平移矩阵，并利用所求矩阵，将源点云变换到目标点云的坐标系下，估计变换后源点云与目标点云的误差函数，若误差函数值大于阀值，则迭代进行上述运算直到满足给定的误差要求. ICP算法采用最小二乘估计计算变换矩阵，原理简单且具有较好的精度，但是由于采用了迭代计算，导致算法计算速度较慢，而且采用ICP进行配准计算时，其对配准点云的初始位置有一定要求，若所选初始位置不合理，则会导致算法陷入局部最优。 Align 3D Data如果空间中两组点云之间的对应关系已经明确，则很容易求得两者之间的刚性变换，即旋转和平移共6个参数，但这种对应关系一般很难事先知道。ICP算法假设两组点云之间的对应关系由最近点确定，一步步将源点云$P$匹配到目标点云$Q$。 ICP算法主要包含对应点确定和变换计算更新，简要流程如下 在源点云 $P$ 中选择一些随机点 $p_i, i=1,2, \cdots,n$ 在目标点云 $Q$ 中找到每个点 $p_i$ 的最近点 $q_i$ 剔除一些距离较远的点对 构建距离误差函数$E$ 极小化误差函数，如果对应点距离小于给定阈值设置，则算法结束；否则根据计算的旋转平移更新源点云，继续上述步骤。 Basic ICP传统的ICP算法主要有两种度量函数，即point-to-point和point-to-plane距离，一般来说，point-to-plane距离能够加快收敛速度，也更加常用。 Point-to-Point Error Metric E_{p o i n t}=\sum_{i}\left\|\mathrm{R} p_{i}+t-q_{i}\right\|^{2} Point-to-Plane Error Metric E_{plane} = \sum_{i}\left[\left(\mathrm{R} p_{i}+t-q_{i}\right) \cdot n_{q_i}\right]^{2} Colored ICPColored ICP算法[Park2017]针对有颜色的点云，在原始point-to-plane能量项的基础上，增加了一个对应点对之间的颜色约束，能够有更好的配准结果。其目标函数如下 E_{color} = (1-\delta) E_{C}+\delta E_{G}其中$E_{C}$是颜色能量项，$E_{G}$是几何能量项，\delta \in[0,1]是两者之间的相对权重。几何能量项就是传统的point-to-plane能量项： E_{G}=\sum_{i}\left[(R p_i + t - q_i) \cdot n_{q_i}\right]^{2}而颜色项衡量点$p_i$的颜色$\boldsymbol{C}(p_i)$与其投影到$q_i$的切平面上的颜色之差： E_{C}=\sum_{i}\left(\boldsymbol{C}_{q_i}(\Pi(R p_i + t))-\boldsymbol{C}(p_i)\right)^{2}其中$\Pi$将3D点投影到切平面，$\boldsymbol{C}_{q_i}$是提前计算好的定义在$q_{i}$的切平面上的连续函数。 Symmetrized ICPSymmetrized ICP算法极小化到点到由$n_p$和$n_q$决定的平面的距离平方和，同时优化拆分的旋转 : E_{two-plane}=\sum_{i}\left[\left(\mathrm{R} p_{i}-\mathrm{R}^{-1} q_{i}+t\right) \cdot\left(\mathrm{R} n_{p_i}\right)\right]^{2} + \left[\left(\mathrm{R} p_{i}-\mathrm{R}^{-1} q_{i}+t\right) \cdot\left(\mathrm{R}^{-1} n_{q_i}\right)\right]^{2}Symmetric ICPSymmetric ICP[Rusinkiewicz2019]是ICP算法的另一种改进。其核心是在原有点到面距离$(p-q) \cdot n_{q}$上做了一个微小的改动即$(p-q) \cdot\left(n_{p}+n_{q}\right)$，在几乎不增加计算量的基础上，能够有更快更可靠的收敛。其目标函数有两种，第一种是法向旋转版本(Rotated-Normals)的对称ICP目标函数： E_{symm-RN}=\sum_{i}\left[\left(\mathrm{R} p_{i}-\mathrm{R}^{-1} q_{i}+t\right) \cdot\left(\mathrm{R} n_{p_i}+\mathrm{R}^{-1} n_{q_i}\right)\right]^{2}另一种更简单但效果差不多的版本是 E_{symm}=\sum_{i}\left[\left(\mathrm{R} p_{i}-\mathrm{R}^{-1} q_{i}+t\right) \cdot\left(n_{p_i}+n_{q_i}\right)\right]^{2}将旋转拆分为两个相同的部分在进行目标函数线性化估计时会带来一些好处，因为在线性化时假设旋转角度很小，优化拆分的旋转即角度的一半使得近似误差更小。 SolveICP算法在极小化能量时通常都需要求解一个非线性最小二乘问题，但可以线性化，假设$\theta \approx 0$，则$\sin(\theta) \approx \theta$，$\cos(\theta) \approx 1$，忽略二次项，可以得到一个线性的最小二乘问题，再用Gauss-Newton或者Levenberg-Marquardt算法求解。 AlgorithmGo-ICPGo-ICP即Globally optimal ICP，提出了在L2误差度量下欧式空间中匹配两组点云的全局最优算法。 Sparse ICPCode ICP: libicp Iterative-Closest-Point Go-ICP Sparse ICP: sparseicp icpSparse CUDA ICPCUDA CudaICP ICP pose_refine OpenCL ICP Reference Dynamic Geometry Processing [Rusinkiewicz2019] Szymon Rusinkiewicz. A Symmetric Objective Function for ICP, SIGGRAOH 2019 [Park2017] J. Park, Q.-Y. Zhou, and V. Koltun. Colored Point Cloud Registration Revisited, ICCV 2017. [Bouaziz2013] Sofien Bouaziz, Andrea Tagliasacchi, Mark Pauly. Sparse Iterative Closest Point, SGP 2013.]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rules of C++]]></title>
    <url>%2F2017%2F08-28-Cpp-CppRules%2F</url>
    <content type="text"><![CDATA[Rules of C++如果一个类定义没有显式申明一个复制构造函数，一个非显式的函数会隐式地定义。如果该类定义申明了一个移动构造函数或者移动赋值运算符，隐式定义的复制构造函数被定义为以删除的；否则，其被定义为默认的。当该类有一个用户申明的复制赋值运算符或者析构函数，后者的情况被弃用。 If the class definition does not explicitly declare a copy constructor, a non-explicit one is declared implicitly. If the class definition declares a move constructor or move assignment operator, the implicitly declared copy constructor is defined as deleted; otherwise, it is defined as defaulted (8.4). The latter case is deprecated if the class has a user-declared copy assignment operator or a user-declared destructor. 三/五/零规则三规则若一个类需要用户定义析构函数、用户定义复制构造函数或用户定义的复制赋值运算符，则它几乎肯定要求三者全体。 因为 C++ 在各种场合（传递/按值返回、操作容器等）复制和复制赋值用户定义的对象，若可访问则会调用这些特殊成员函数，且若它们不为用户定义，则为编译器隐式定义。 若该类管理资源，而资源的柄是非类类型（生指针、 POSIX 文件描述符等），则隐式定义的特殊成员函数大体是错误的，其中析构函数无操作而复制构造函数/赋值运算符进行“浅复制”（复制柄的值，而不备份底下的资源）。 1234567891011121314151617181920212223242526272829303132class rule_of_three&#123; char* cstring; // 用于指向动态分配内存块的句柄的生指针 public: rule_of_three(const char* arg) : cstring(new char[std::strlen(arg)+1]) // 分配 &#123; std::strcpy(cstring, arg); // 复制内容 &#125; ~rule_of_three() &#123; delete[] cstring; // 解分配 &#125; rule_of_three(const rule_of_three&amp; other) // 复制构造函数 &#123; cstring = new char[std::strlen(other.cstring) + 1]; std::strcpy(cstring, other.cstring); &#125; rule_of_three&amp; operator=(const rule_of_three&amp; other) // 复制赋值 &#123; char* tmp_cstring = new char[std::strlen(other.cstring) + 1]; std::strcpy(tmp_cstring, other.cstring); delete[] cstring; cstring = tmp_cstring; return *this; &#125; // 另可重用析构函数和复制构造函数 // rule_of_three&amp; operator=(rule_of_three other) // &#123; // std::swap(cstring, other.cstring); // return *this; // &#125;&#125;; 通过可复制柄管理不可复制资源的类，必须声明复制赋值和复制构造函数为私有并不提供其定义，或定义它们为被删除。这是另一种三规则：删除一者并保留另一者为隐式定义很可能导致错误。 五规则因为用户定义析构函数、复制构造函数或复制赋值运算符的存在阻止移动构造函数和移动赋值运算符的隐式定义，任何需要移动语义的类必须声明所有五个特殊成员函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class rule_of_five&#123; char* cstring; // 用作指向动态分配内存块的句柄的生指针 public: rule_of_five(const char* arg) : cstring(new char[std::strlen(arg)+1]) // 分配 &#123; std::strcpy(cstring, arg); // 复制内容 &#125; ~rule_of_five() &#123; delete[] cstring; // 解分配 &#125; rule_of_five(const rule_of_five&amp; other) // 复制构造函数 &#123; cstring = new char[std::strlen(other.cstring) + 1]; std::strcpy(cstring, other.cstring); &#125; rule_of_five(rule_of_five&amp;&amp; other) : cstring(other.cstring) // 移动构造函数 &#123; other.cstring = nullptr; &#125; rule_of_five&amp; operator=(const rule_of_five&amp; other) // 复制赋值 &#123; char* tmp_cstring = new char[std::strlen(other.cstring) + 1]; std::strcpy(tmp_cstring, other.cstring); delete[] cstring; cstring = tmp_cstring; return *this; &#125; rule_of_five&amp; operator=(rule_of_five&amp;&amp; other) // 移动赋值 &#123; if(this!=&amp;other) // 阻止自移动 &#123; delete[] cstring; cstring = other.cstring; other.cstring = nullptr; &#125; return *this; &#125; // 另可将二个赋值运算符以下面的替换 // rule_of_five&amp; operator=(rule_of_five other) // &#123; // std::swap(cstring, other.cstring); // return *this; // &#125;&#125;; 不同于三规则，不提供移动构造函数和移动赋值运算符通常不是错误，然而是对优化机会的错失。 零规则拥有自定义析构函数、复制/移动构造函数或复制/移动赋值运算符的类应该排他地处理所有权（这遵循单一责任原则）。其他类不该有自定义析构函数、复制/移动构造函数或复制/移动赋值运算符。 123456class rule_of_zero&#123; std::string cppstring; public: rule_of_zero(const std::string&amp; arg) : cppstring(arg) &#123;&#125;&#125;; 当基类为多态使用而设时，其析构函数可能必须声明为公开且为虚。这阻止隐式移动（并将隐式复制过时化），从而特殊成员函数必须声明为默认 123456789class base_of_five_defaults&#123; public: base_of_five_defaults(const base_of_five_defaults&amp;) = default; base_of_five_defaults(base_of_five_defaults&amp;&amp;) = default; base_of_five_defaults&amp; operator=(const base_of_five_defaults&amp;) = default; base_of_five_defaults&amp; operator=(base_of_five_defaults&amp;&amp;) = default; virtual ~base_of_five_defaults() = default;&#125;; 然而，若导出类不被动态分配，或仅在被存储于 std::shared_ptr 时动态分配（例如通过 std::make_shared ），则可避免它：共享指针调用导出类的析构函数，即使在被转型为 std::shared_ptr 后。 Reference 三/五/零规则 C++ Compiler Error C2280 “attempting to reference a deleted function” in Visual Studio 2013 and 2015 Does rule of three/five apply to inheritance and virtual destructors?]]></content>
      <categories>
        <category>coding</category>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度传感器]]></title>
    <url>%2F2017%2F08-27-CG-DepthSensor%2F</url>
    <content type="text"><![CDATA[Microsoft KinectKinece二代的技术来自于PrimeSense，微软收购了这家以色列公司。Windows Kinect Sensor 包含 RGB相机 An RGB camera that stores three channel data in a 1280x960 resolution. This makes capturing a color image possible. IR红外发射器和深度传感器 An infrared (IR) emitter and an IR depth sensor. The emitter emits infrared light beams and the depth sensor reads the IR beams reflected back to the sensor. The reflected beams are converted into depth information measuring the distance between an object and the sensor. This makes capturing a depth image possible. 麦克风阵列 A multi-array microphone, which contains four microphones for capturing sound. Because there are four microphones, it is possible to record audio as well as find the location of the sound source and the direction of the audio wave. 3轴加速计 A 3-axis accelerometer configured for a 2G range, where G is the acceleration due to gravity. It is possible to use the accelerometer to determine the current orientation of the Kinect. Kinect Array Specifications Viewing angle 43° vertical by 57° horizontal field of view Vertical tilt range ±27° Frame rate (depth and color stream) 30 frames per second (FPS) Audio format 16-kHz, 24-bit mono pulse code modulation (PCM) Audio input characteristics A four-microphone array with 24-bit analog-to-digital converter (ADC) and Kinect-resident signal processing including acoustic echo cancellation and noise suppression Accelerometer characteristics A 2G/4G/8G accelerometer configured for the 2G range, with a 1° accuracy upper limit. The resolution of the depth stream is dependent on the frame rate, and is specified by the DepthImageFormat Enumeration enumeration. Similarly, the resolution of the color stream is specified by the ColorImageFormat Enumeration enumeration. See the depth space range section to see the depth data ranges as well as values for out-of-range data. Intel RealSenseIntel RealSense Camera R200Operating Range (Min-Max)0.5m - 3.5m Reference Kinect for Windows Sensor Components and Specifications Specifications for the Intel RealSense Camera R200]]></content>
      <categories>
        <category>knowledge</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方向梯度直方图]]></title>
    <url>%2F2017%2F08-20-CV-HOG%2F</url>
    <content type="text"><![CDATA[算法简介方向梯度直方图(Histogram of Oriented Gradient, HOG)是应用在计算机视觉和图像处理领域，用于目标检测(Object detection)的特征描述器。 算法步骤Reference wikipedia:方向梯度直方图]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机抽样一致算法]]></title>
    <url>%2F2017%2F08-20-CV-RANSAC%2F</url>
    <content type="text"><![CDATA[算法简介算法步骤Reference wikipedia：随机抽样一致算法]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SIFT-尺度不变特征转换]]></title>
    <url>%2F2017%2F08-20-CV-SIFT%2F</url>
    <content type="text"><![CDATA[算法简介尺度不变特征转换(Scale-invariant feature transform 或 SIFT)是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe 在1999年所发表，2004年完善总结。 Sift算法就是用不同尺度（标准差）的高斯函数对图像进行平滑，然后比较平滑后图像的差别，差别大的像素就是特征明显的点。 Sift（Scale Invariant Feature Transform）是一个很好的图像匹配算法，同时能处理亮度、平移、旋转、尺度的变化，利用特征点来提取特征描述符，最后在特征描述符之间寻找匹配。 算法步骤 构建尺度空间，检测极值点，获得尺度不变性 特征点过滤并进行精确定位，剔除不稳定的特征点 在特征点处提取特征描述符，为特征点分配方向值 生成特征描述子，利用特征描述符寻找匹配点 计算变换参数 Reference wikipedia：尺度不变特征转换 经典算法研究系列：九、图像特征提取与匹配之SIFT算法]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉中的 F 矩阵，E 矩阵与 H 矩阵]]></title>
    <url>%2F2017%2F08-14-CV-Matrix-F-E-H%2F</url>
    <content type="text"><![CDATA[F 矩阵 - 基础矩阵 (Fundamental Matrix)在计算机视觉中，基础矩阵 $\mathrm {F}$ 是一个3×3的矩阵，表达了立体像对的像点之间的对应关系。在对极几何中，对于立体像对中的一对同名点，它们的齐次化图像坐标分别为 $p$ 与 $p’$，$\mathrm {F} p$ 表示一条必定经过 $p$ 的直线（极线）。这意味着立体像对的所有同名点对都满足： p'\mathrm {F} p=0F 矩阵中蕴含了立体像对的两幅图像在拍摄时相互之间的空间几何关系（外参数）以及相机检校参数（内参数），包括旋转、位移、像主点坐标和焦距。因为 $\mathrm {F}$ 矩阵的秩为2，并且可以自由缩放（尺度化），所以只需7对同名点即可估算出 F 的值。尽管本质矩阵也满足类似的关系式，但本质矩阵中并不蕴含相机检校参数。本质矩阵与基础矩阵之间的关系可由下式表达： \mathrm {E} = \mathrm{K'^TFK}其中 $\mathrm{K}$ 和 $\mathrm{K’}$ 分别为两个相机的内参数矩阵。 E 矩阵 - 本质矩阵 (Essential Matrix)本质矩阵也满足类似的关系式， H 矩阵 - 单应矩阵 (Homography)推导基础矩阵有许多种推导方式，下面介绍其中一种。在双相机的拍摄场景中建立一个空间直角坐标系，称为世界坐标系（如下图中蓝色坐标系）。物点就是场景中物体表面上的点，比如说点 $P$ 在世界坐标系中的坐标为 $(X,Y,Z)^T$。相机的光心从物理上讲就是相机镜头组的光学中心。以光心为原点，主光轴为Z轴建立空间直角坐标系，称为相机坐标系（如下图中绿色和红色坐标系）。像平面在相机坐标系中的方程即为 $z=1$，像点就是在物点在像平面上的投影，这个投影关系是透视投影。用一句话来概括相机的拍摄模型，就是物点、像点、光心三点一线，此模型称为针孔相机模型。在此模型中，世界坐标系到左右相机坐标系的变换是刚性变换，即只包含旋转和平移，因此我们分别用增广矩阵 $[R;t]$ 和 $[R’;t’]$ 表示，其中 $R$ 和 $R’$ 是 $3 \times 3$ 的旋转矩阵， $t$ 和 $t’$ 为平移向量。令 $\tilde{P}$ 为 $P$ 的齐次化坐标，那么物点 $P$ 在左右相机坐标系下的坐标分别为 P_{cam}(X_{C},Y_{C},Z_{C})^T=[R;t]\tilde {P}和 P_{cam}' (X_{C'},Y_{C'},Z_{C'})^T = [R';t']{ \tilde {P} }以一台相机为例，如图所示， $C$ 为相机光心，$Z$ 轴为主轴。物点在相机坐标系下的坐标 $\tilde{P}$ 和以相片左下角为原点的像点坐标 $p$ 有如下关系： x = (\frac{f_xX_c}{Z_c} + x_0)^T y = (\frac{f_yY_c}{Z_c} + y_0)^T式中 $(x_0,y_{0},f)$ 为像主点在相机坐标系下的坐标。设两相机内参数矩阵同为： K = \left( \begin{matrix} f_x & 0 & p_{c_x} \\ 0 & f_y & p_{c_y} \\ 0 & 0 & 1 \end{matrix} \right)那么物点与像点之间的关系为： p={\frac {1}{Z_{C}}}KP_{cam}={\frac{1}{Z_{C}}}K[R;t]P p'={\frac {1}{Z_{C}}}KP'_{cam}={\frac {1}{Z_{C}}}K[R';t']P将 $P=[R;t]^{+}P_{cam}=Z’_{C}K[R;t]^{+}p$代入上式，并令 $H_{\pi}=K[R’;t’]K[R;t]^{+}$，得： p'={\frac {Z'_{C}}{Z_{C}}}H_{\pi }p由于物点、像点、光心三点一线，那么物点、一对同名点和2个光心这5个点一定处于同一个平面上，我们将这个平面称为 $\pi$ 平面。 $\pi$ 平面和像平面的交线称为极线 $l’$。显然，左片上的每一个像点 $p$ 对应于右片上的一条极线 $l’$，且 $p’$ 一定在 $l’$ 上。两个相机光心的连线与右片像平面的交点称为极点，用 $e’$ 表示。 在右边像平面内，极线 $l’$ 的方程可以表示为 $Ax+By+C=0$。这个平面直线方程的一般式可以视为： (A,B,C)^T \cdot (x,y,1)^T=0因此，我们可以用一个三维向量 $(A,B,C)$ 来表示极线 $l’$，并且 $l’$ 的方程可以简单的由 $e’$ 坐标向量与 $p’$ 坐标向量做向量积得到，即 $l’:e’\times p’=[e’]_{\times }p’$。其中 [e']_{\times } = \left( \begin{matrix} 0 & -1 & y_0 \\ 1 & 0 & -x_0 \\ -y_0 & x_0 & 0 \end{matrix} \right)令 $[e’]x$ 表示向量积的矩阵形式，那么再将同名点之间的变换关系代入，得到极线的方程为： l':{\frac {Z'_{C}}{Z_{C}}}[e']_{\times }H_{\pi }p因为 $p’$ 在 $l’$ 上，所以显然有： p'l'=p'[e']_{\times }H_{\pi }p=0令$ \mathrm{F} =[e’]_{\times} H_{\pi}$，即得到： {p'}^T\mathrm{F}p = 0Reference 维基百科-基础矩阵]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Algorithm Summary]]></title>
    <url>%2F2017%2F08-14-ML-AlgorithmsSummary%2F</url>
    <content type="text"><![CDATA[文章转载自think big data。 Which are the best known machine learning algorithms? InfographicWhich are the most important machine learning algorithms? Anyone who has been part of this domain must have faced or posed this question at some point of time. I too am asked this often. First things first – there are no winning algorithms. For different circumstances, different algorithms, even though they might be designed for similar outcome, result in differently oriented output. Depending upon what you want with your data analytics, an algorithm might be better suited to you than the others for that situation. Size of the data set also plays a key role in determining the model to apply. Also, iterations in existing algorithms, thereby increasing their relevance to myriad of applications, is also common. What is important to remember is that simpler algorithms aren’t bad or obsolete. So, my suggestion is instead of searching for the best algorithms, one should focus on gaining awareness about fundamentals of different algorithms and their applications. Most of us familiar with the subject would recall that in 2006, IEEE Conference on Data Mining identified the top 10 machine learning algorithms. That list is widely available over the internet, so we’ll not reproduce it here. What we’ll do instead is mention over a dozen algorithms, segregated by their application intent, that should be in the repertoire of every data scientist. For usefulness purposes, I am reproducing the list in an infographic format – that can easily go onto a wall! For suggestions and improvements – please add your comments below. Reference think big data]]></content>
      <categories>
        <category>repost</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Explained - Algorithms Are Your Friend]]></title>
    <url>%2F2017%2F08-14-ML-PredictionAlgorithmsSummary%2F</url>
    <content type="text"><![CDATA[文章转载自Dataiku Inc。 We hear the term “machine learning” a lot these days, usually in the context of predictive analysis and artificial intelligence. Machine learning is, more or less, a way for computers to learn things without being specifically programmed. But how does that actually happen?The answer is, in one word, algorithms. Algorithms are sets of rules that a computer is able to follow. Think about how you learned to do long division — maybe you learned to take the denominator and divide it into the first digits of the numerator, then subtracting the subtotal and continuing with the next digits until you were left with a remainder. Well, that’s an algorithm, and it’s the sort of thing we can program into a computer, which can perform these sorts of calculations much, much faster than we can. We’ve put together a brief summary of the top algorithms used in predictive analysis, which you can see just below. Read on for more detail on these algorithms. What Does Machine Learning Look Like?In machine learning, our goal is either prediction or clustering. Today, we’re going to focus on prediction (we’ll cover clustering in a future article). Prediction is a process where, from a set of input variables, we estimate the value of an output variable. For example, using a set of characteristics of a house, we can predict its sale price.Prediction problems are divided into two main categories: Regression problems, where the variable to predict is numerical (e.g., the price of a house)Classification problems, where the variable to predict is a “Yes/No” answer (for example, predict whether a certain piece of equipment will experience a mechanical failure)With that in mind, today we’re not going to reveal a secret to do prediction better than the computers, or even how to be a data scientist! What we are going to do is introduce the most prominent and common algorithms used in machine learning historically and today. Our algorithms come in three groups: linear models, tree-based models, and neural networks. Linear Model ApproachA linear model uses simple formulas to find the “best fit” line through a set of data points. This methodology dates back over 200 years, and it has been used widely throughout statistics and machine learning. It is useful for statistics because of its simplicity — the variable you want to predict (the dependent variable) is represented as an equation of variables you know (independent variables), and so prediction is just a matter of inputting the independent variables and having the equation spit out the answer. For example, you might want to know how long it will take to bake a cake, and your regression analysis might yield an equation t = 0.5x + 0.25y, where t is the baking time in hours, x is the weight of the cake batter in kg, and y is a variable which is 1 if it is chocolate and 0 if it is not. If you have 1 kg of chocolate cake batter (we love cake), then you plug your variables into our equation, and you get t = (0.5 x 1) + (0.25 x 1) = 0.75 hours, or 45 minutes. Linear RegressionLinear regression, or more specifically “least squares regression,” is the most standard form of linear model. For regression problems, linear regression is the most simple linear model. Its drawback is that there is a tendency for the model to “overfit” — that is, for the model to adapt too exactly to the data on which it has been trained at the expense of the ability to generalize to previously unseen data. For this reason, linear regression (along with logistic regression, which we’ll get to in a second) in machine learning is often “regularized,” which means the model has certain penalties to prevent overfit. Another drawback of linear models is that, since they’re so simple, they tend to have trouble predicting more complex behaviors when the input variables are not independent. Logistic RegressionLogistic regression is simply the adaptation of linear regression to classification problems (once again, discussed above). The drawbacks of logistic regression are the same as those of linear regression.Because of its shape, the logistic function is very good for classification problems, as it introduces a threshold effect. Tree-Based Model ApproachWhen you hear tree-based, think decision trees, i.e., a sequence of branching operations. Decision TreeA decision tree is a graph that uses a branching method to show each possible outcome of a decision. Like if you’re ordering a salad, you first decide the type of lettuce, then the toppings, then the dressing. We can represent all possible outcomes in a decision tree. To train a decision tree, we take the train data set (that is, the data set that we use to train the model) and find which attribute best “splits” the train set with regards to the target. For example, in a fraud detection case, we could find that the attribute which best predicts the risk of fraud is the country. After this first split, we have two subsets which are the best at predicting if we only know that first attribute. Then we can iterate on the second-best attribute for each subset and resplit each subset, continuing until we have used enough of the attributes to satisfy our needs. Random ForestA random forest is the average of many decision trees, each of which is trained with a random sample of the data. Each single tree in the forest is weaker than a full decision tree, but by putting them all together, we get better overall performance thanks to diversity. Random forest is a very popular algorithm in machine learning today. It is very easy to train, and it tends to perform quite well. Its downside is that it can be slow to output predictions relative to other algorithms, so you might not use it when you need lightning-fast predictions. Gradient BoostingGradient boosting, like random forest, is also made from “weak” decision trees. The big difference is that in gradient boosting, the trees are trained one after another. Each subsequent tree is trained primarily with data that had been wrongly identified by previous trees. This allows gradient boost to focus less on the easy-to-predict cases and more on difficult cases. Gradient boosting is also pretty fast to train and performs very well. However, small changes in the training data set can create radical changes in the model, so it may not produce the most explainable results. Neural NetworksNeural networks refer to a biological phenomenon comprised of interconnected neurons that exchange messages with each other. This idea has now been adapted to the world of machine learning and is called ANN (Artificial Neural Networks). Deep learning, which you’ve heard a lot about, is just several layers of neural networks put one after the other. ANNs are a family of models that are taught to adopt cognitive skills to function like the human brain. No other algorithms can handle extremely complex tasks, such as image recognition, as well as neural networks. However, just like the human brain, it takes a very long time to train the model, and it requires a lot of power (just think about how much we eat to keep our brains working!). We hope this has been helpful in shedding some light on machine learning. If you’re interested in learning more, you can read about the machine learning features within Dataiku DSS, or even check out a very cool application of machine learning to predict crime rates in London. Enjoy, and keep in touch! Reference Dataiku Inc.]]></content>
      <categories>
        <category>repost</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯角度看 L1 & L2 正则化]]></title>
    <url>%2F2017%2F08-13-ML-Regularization%2F</url>
    <content type="text"><![CDATA[贝叶斯角度看 L1 &amp; L2 正则化从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布。其中, L1正则化对应于拉普拉斯分布， L2正则化对应于高斯分布。 频率学派 VS 贝叶斯学派频率学派(Frequentists)与贝叶斯学派(Bayesians)都基于样本数据做分析。其不同点主要在于对于 $X\sim F(x;\theta)$，概率学派将样本看作随机变量，而将 $\theta$ 看作常数，而贝叶斯学派将样本和 $\theta$ 均看作随机变量。概率学派的着眼点在样本空间，基本依托于大数定律和中心极限定理，其贵在于得到越多越好的样本，然后计算其频率来逼近该事件的真实概率。而贝叶斯学派的着眼点在参数空间，更加重视参数 $\theta$ 的分布，表现在对事件有一定的先验知识了解，然后再用似然度去修正之前的知识了解，如果当前的观测值越符合我们的先验知识，那么似然度就越大，得到的后验概率也就越大，反之亦然。 贝叶斯概率论假设观察者对某事件处于某个知识状态中（例如：有一个袋子里面装了红球和黑球，已知这个袋子里面是5黑5红的概率是0.8，是10黑5红的概率是0.2），之后观察者开始新的观测或实验（有放回抽取100次，得到80次黑的，20次红的）。经过中间的独立重复试验，观察者获得了一些新的观测结果，这些新的观测将以含有不确定性的逻辑推断的方式影响观察者原有的信念（即观测者一开始认为袋子里是5黑5红的可能性更大，但是经过了上面的事实之后，修正了原有的信念，认为是10黑5红可能性更大）。上面的例子用贝叶斯概率论的语言来描述，就是观察者持有某个前置信念（prior belief），通过观测获得统计证据（evidence），通过满足一定条件的逻辑一致推断得出的关于该陈述的「合理性」，从而得出后置信念（posterior belief）来最好的表征观测后的知识状态（state of knowledge）。 看似贝叶斯框架比较的完美，而且可以克服一些频率派困难（比如投骰子次数不多，那么计算的频率显然与真实的分布想去甚远，但是贝叶斯的先验知识可以缓和这种极端情况）。但是贝叶斯的先验知识没有具体、规则化的获得方法，每个人的先验知识都可能是不一样的，而不良的先验概率甚至会使得最终的估计偏离真实的值。对此，贝叶斯的先验知识最好是客观计算出来的，抑或者拿不准时候用弱信息甚至无信息的先验假设来尽可能避免这类问题。 Linear Regression最原始的 Linear Regression 模型如下这就导出了我们原始的 least-squares 损失函数，但这是在我们对参数 w 没有加入任何先验分布的情况下。在数据维度很高的情况下，我们的模型参数很多，模型复杂度高，容易发生过拟合。比如我们常说的 “small n, large p problem”。（我们一般用 n 表示数据点的个数，用 p 表示变量的个数 ，即数据维度。当 的时候，不做任何其他假设或者限制的话，学习问题基本上是没法进行的。因为如果用上所有变量的话， p 越大，通常会导致模型越复杂，但是反过来 n 又很小，于是就会出现很严重的 overfitting 问题。 这个时候，我们可以对参数 w 引入先验分布，降低模型复杂度。 Ridge Regression我们对参数 $w$ 引入协方差为 $\alpha$ 的零均值高斯先验。 看我们得到的参数，在零附近是不是很密集，老实说 ridge regression 并不具有产生稀疏解的能力，也就是说参数并不会真出现很多零。假设我们的预测结果与两个特征相关，L2正则倾向于综合两者的影响，给影响大的特征赋予高的权重；而L1正则倾向于选择影响较大的参数，而舍弃掉影响较小的那个。实际应用中 L2正则表现往往会优于 L1正则，但 L1正则会大大降低我们的计算量。 Typically ridge or $l2$ penalties are much better for minimizing prediction error rather than ℓ1 penalties. The reason for this is that when two predictors are highly correlated, ℓ1 regularizer will simply pick one of the two predictors. In contrast, the ℓ2 regularizer will keep both of them and jointly shrink the corresponding coefficients a little bit. Thus, while the ℓ1 penalty can certainly reduce overfitting, you may also experience a loss in predictive power. 那现在我们知道了，对参数引入 高斯先验 等价于L2正则化。 LASSO Elastic Net 总结正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及 outliers 的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项 对应后验估计中的 先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。 参考 知乎 Charles Xiao 知乎 bsdelf 统计学之边角料——频率派和贝叶斯派 贝叶斯角度看L1，L2正则化]]></content>
      <categories>
        <category>algorithm</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图像的变换]]></title>
    <url>%2F2017%2F08-13-CV-ImageTransformation%2F</url>
    <content type="text"><![CDATA[等距变换等距变换 (Isometric Transformation) 保持欧氏距离不变。等距变换就是对图像的旋转和平移。 变换矩阵 H_E = \left( \begin{matrix} R & t\\ 0^T & 1 \end{matrix} \right) =\left( \begin{matrix} \epsilon \cos \theta & -\sin \theta & t_x \\ \epsilon \sin \theta & \cos \theta & t_y \\ 0 & 0 & 1 \end{matrix} \right)其中 $R$ 是单位旋转矩阵，$t$ 是平移向量，$\epsilon=\pm1$, 当$\epsilon=1$时保向，变换由旋转和平移组成，当$\epsilon=-1$时逆向，变换还包含一个镜像。 不变量：长度，角度，面积 自由度：3个自由度（1个旋转角+2个平移），至少需要2组点共4个方程求解。 相似变换相似变换（Similarity Transformation）是在等距变换的基础上加上一个缩放。等距变换就是对图像的旋转+平移+缩放。 变换矩阵 H_S = \left( \begin{matrix} s\cdot R & t\\ 0^T & 1 \end{matrix} \right) =\left( \begin{matrix} s \cos \theta & -s\sin \theta & t_x \\ s \sin \theta & s\cos \theta & t_y \\ 0 & 0 & 1 \end{matrix} \right)其中 $R$ 是单位旋转矩阵，$t$ 是平移向量, $s$ 是缩放因子。 不变量：角度，长度的比例，面积的比例，平行线关系 自由度：4个自由度（1个旋转角+2个平移+1个缩放尺度），至少需要2组点4个方程求解. 仿射变换仿射变换（Affine Transformation）仿射变换就是对图像的旋转+平移+缩放+切变（shear），相比前两种变换图像的形状发生了改变，但是原图中的平行线仍然保持平行。 变换矩阵 H_A = \left( \begin{matrix} A & t\\ 0 & 1 \end{matrix} \right) =\left( \begin{matrix} a_{11} & a_{12} & t_x \\ a_{21} & a_{22} & t_y \\ 0 & 0 & 1 \end{matrix} \right)其中 $A$ 是仿射矩阵，$t$ 是平移向量。 仿射变换的 $A$ 矩阵可以做SVD分解，即 A = UDV^T = (UV^T)(VDV^T) = R(\theta)R(-\phi)DR(\phi), \quad D=diag(\lambda_1, \lambda_2)所以仿射变换 $A$ 可以看成先旋转 $\phi$，再沿着 $x, y$ 方向按照比例 $\lambda_1, \lambda_2$ 进行缩放，然后再旋转 $-\phi$，最后旋转 $\theta$。 不变量：平行线关系，平行线长度的比例，面积的比例 自由度：6个自由度（2个旋转角+2个平移+2个缩放因子），至少需要3组点6个方程求解. 投影变换投影变换（Projective Transformation）又叫2D homographies，是把点从一个平面映射到另一个平面，如世界坐标系下的平面到相机的成像平面。投影变换就是对图像的旋转+平移+缩放+切变+射影，相比前三种变换图像的形变更为自由，原图中的平行线经过变换之后已经不在平行，而可能相交于一点，射影变换就是把理想点（平行直线在无穷远处相交）变换到图像上。 变换矩阵 H_P = \left( \begin{matrix} A & t\\ v^T & c \end{matrix} \right) =\left( \begin{matrix} h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & h_{33} \end{matrix} \right)其中 $A$ 是仿射矩阵，$t$ 是平移向量，$v=(v_1, v_2)^T$，$c$ 是一个常数。 不变量：长度的交比（比例的比例） 自由度：8个自由度（2个旋转角+2个平移+2个缩放因子+2个无穷远的线），至少需要4组点8个方程求解. 对于理想点 $(x_1,x_2,0)^T$ 的映射: H_P = \left( \begin{matrix} A & t\\ v^T & c \end{matrix} \right) \left( \begin{matrix} x_1 \\ x_2 \\ 0 \end{matrix} \right) =\left( \begin{matrix} A(x_1,x_2)^T \\ v_1x_1+v_2x_2 \end{matrix} \right)所以对于仿射变换，$v=0$，所有的理想点仍然被映射到理想点，对于投影变换，$v\neq 0$，一些理想点被映射到无穷远处。 参考 图像的等距变换，相似变换，仿射变换，射影变换及其matlab实现 Geometrical Image Analysis, Spring 08 2D projective transformations]]></content>
      <categories>
        <category>algorithm</category>
        <category>cv/cg</category>
      </categories>
      <tags>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MeshViewer框架搭建]]></title>
    <url>%2F2017%2F01-10-Qt-RedesignMeshviewerFrame%2F</url>
    <content type="text"><![CDATA[在 VS 中新建 Qt 项目这个就不废话了。 添加菜单栏这个很简单，在 VS 中双击 mainwindow.ui，直接在 Qt Designer 中添加就行。这里就添加一个 File 菜单，下面再添加两个 Action,分别为 Open 和 Save。其他的类似。 添加工具栏在右侧底部的“动作编辑器”中，选中一个 Action，拖放到窗口的工具栏上即可。在“动作编辑器”中还能设置快捷方式。 在“资源浏览器”中，点击上面的铅笔标志，添加应用图标的资源文件。 在“动作编辑器”中双击 Action，如 Open，可以添加应用图标，进行其他一些设置。 添加 Render Group Box首先在 Qt Designer 中拖入几个 Check Box，将对象名作相应修改，如 Point，Edge, Face 等。 然后选中这三个 Check Box，右击，从“布局”中选中“垂直布局”。 再添加一个 Group Box，将对象名改为 Render，结果如下： 然后将 Render CheckBox 整个布局拖到 Group Box 中，使其成为 Group Box 的子对象。你还可以在 Group Box 中添加其他一些 Widgets。这里添加了一个 PushBottom，对象名为 OK。 此时右侧的对象查看器中的结果是这样的，注意其中的对象包含关系。 最后还要做一个设置，选中 Group Box，右击，从“大小限定”中点击“设置最小宽度”和“设置最小高度”。 然后在 MainWindow 的构造函数中添加一些代码，具体如下。 12345678910111213141516MainWindow::MainWindow(QWidget *parent) : QMainWindow(parent)&#123; ui.setupUi(this); renderingwidget_ = new RenderingWidget(this); QVBoxLayout *layout_left = new QVBoxLayout; layout_left-&gt;addWidget(ui.groupBox_render); layout_left-&gt;addStretch(4); QHBoxLayout *layout_main = new QHBoxLayout; layout_main-&gt;addLayout(layout_left); layout_main-&gt;addWidget(renderingwidget_); layout_main-&gt;setStretch(1, 1); this-&gt;centralWidget()-&gt;setLayout(layout_main);&#125; 预览效果这是目前的预览效果。]]></content>
      <categories>
        <category>coding</category>
        <category>qt</category>
      </categories>
      <tags>
        <tag>qt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Simple Tutorial]]></title>
    <url>%2F2017%2F01-05-Git-SimpleTutorial%2F</url>
    <content type="text"><![CDATA[转载自https://try.github.io。 Initializing1git init To initialize a Git repository locally, and local directory now has an empty repository in /.git/. The repository is a hidden directory where Git operates. Checking the Status 1git status To see what the current state of our project Adding Changes 1git add filename To add filename to the staging area Adding All Changes 1git add &apos;*.txt&apos; To add all the new files using a wildcard with git add. Committing 1git commit -m &quot;Add cute octocat story&quot; The files in the Staging Area are not in our repository yet. We could add or remove files from the stage before we store them in the repository. To store staged changes, run the commit command with a message describing what we’ve changed. Committing All Changes 1git commit -m &apos;Add all the txt files&apos; History 1git log To browse all the commits to see what we changed. Git’s log remembers all the changes we’ve committed so far, in the order we committed them. Remote Repositories 1git remote add origin https://github.com/try-git/try_git.git To push our local repo to the GitHub server. We need first to create a new empty GitHub repository and then add a remote repository. This command takes a remote name and a repository URL, which in your case is https://github.com/try-git/try_git.git. Pushing Remotely 1git push -u origin master To push our local changes to our origin repo (on GitHub). The push command tells Git where to put our commits when we’re ready. The name of our remote is origin and the default local branch name is master. The -u tells Git to remember the parameters, so that next time we can simply run git push and Git will know what to do. Pulling Remotely 1git pull origin master To pull down any new changes on our GitHub repository. Differences 1git diff HEAD To take a look at what is different from our last commit by using the git diff command. In this case we want the diff of our most recent commit, which we can refer to using the HEAD pointer. Staged Differences 1git add filename To look at changes within files that have already been staged. Remember, staged files are files we have told git that are ready to be committed. Staged Differences (cont’d) 1git diff --staged To see the changes you just staged, run git diff with the —staged option. Resetting the Stage 1git reset filename To remove(unstage) a file, use the git reset command. Notice that the file is still there. It’s just not staged anymore. Undo 1git checkout -- filename To change things back to how they were at the last commit by using the command: git checkout — . Branching Out 1git branch clean_up To create a branch called clean_up. When developers are working on a feature or bug they’ll often create a copy (aka. branch) of their code they can make separate commits to. Then when they’re done they can merge this branch back into their main master branch. Switching Branches 1git checkout clean_up To switch branches using the git checkout command. Now if you type git branch you’ll see two local branches: a main branch named master and your new branch named clean_up. Removing All The Things 1git rm &apos;*.txt&apos; To remove all the txt file in the clean_up branch, use the git rm command which will not only remove the actual files from disk, but will also stage the removal of the files for us. Commiting Branch Changes 1git commit -m &quot;Remove all the txt file&quot; To commit changes. Switching Back to master 1git checkout master Preparing to Merge 1git merge clean_up To merge your changes from the clean_up branch into the master branch. Keeping Things Clean 1git branch -d clean_up To delete a branch, use git branch -d command. The Final Push 1git push To push everything you’ve been working on to your remote repository.]]></content>
      <categories>
        <category>tools</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
</search>
